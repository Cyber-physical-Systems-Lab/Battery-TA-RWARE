{"custom_metrics": {}, "episode_media": {}, "info": {"learner": {"agent_4": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 10.506382851302623, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 3.9650884374976156, "policy_loss": 0.2842683002818376, "vf_loss": 3.4123239412903787, "vf_explained_var": 0.09894251748919487, "kl": 1.3424810335040092, "entropy": 5.789449524879456, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_19": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 26.477173259854318, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 1.6279948584735393, "policy_loss": 0.31833993196487426, "vf_loss": 0.7201287873787805, "vf_explained_var": 0.7926087744534016, "kl": 2.9476307034492493, "entropy": 4.873362191021442, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 18.39516144245863, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 1.2381837228313088, "policy_loss": 0.3039017401635647, "vf_loss": 0.6493724838132039, "vf_explained_var": 0.8869567088782787, "kl": 1.4245474882423879, "entropy": 5.548230549693107, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_16": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 23.924620342254638, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 3.4515472248196604, "policy_loss": 0.359045719448477, "vf_loss": 2.4779706902801992, "vf_explained_var": 0.3047569639980793, "kl": 3.0726540848612784, "entropy": 4.808916117250919, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_9": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 9.694102062284946, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 0.8463362393900752, "policy_loss": 0.2770394003484398, "vf_loss": 0.3015657209558412, "vf_explained_var": 0.9143679536879062, "kl": 1.338655550032854, "entropy": 5.780524590611458, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_5": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 12.972044651210307, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 0.86528554931283, "policy_loss": 0.31527641392312944, "vf_loss": 0.3072131859720685, "vf_explained_var": 0.8677082970738411, "kl": 1.2139797199517488, "entropy": 5.754746770858764, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_7": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 11.224014928936958, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 0.9609550612047315, "policy_loss": 0.2845067332498729, "vf_loss": 0.43446472896030175, "vf_explained_var": 0.9064240284264088, "kl": 1.209917939081788, "entropy": 5.7242969363927845, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_14": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 30.50522391498089, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 1.6365813303738832, "policy_loss": 0.34017860554158685, "vf_loss": 0.8512148499023169, "vf_explained_var": 0.7012599729001522, "kl": 2.2259393192827703, "entropy": 5.161363336443901, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_18": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 32.46109904646873, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 1.723854587972164, "policy_loss": 0.34750501248054205, "vf_loss": 0.8636915173381567, "vf_explained_var": 0.8307105727493763, "kl": 2.563290349394083, "entropy": 5.003801497817039, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_12": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 13.30415912270546, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 0.8840165508911013, "policy_loss": 0.30045431832550096, "vf_loss": 0.3298794470843859, "vf_explained_var": 0.905810696259141, "kl": 1.2684139087796211, "entropy": 5.73697544336319, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_11": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 18.635665926337243, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 1.062852138094604, "policy_loss": 0.2866093228105456, "vf_loss": 0.5098803882137872, "vf_explained_var": 0.8364024121314287, "kl": 1.331812146306038, "entropy": 5.679472807049751, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_13": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 11.17391046434641, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 0.7391427090391517, "policy_loss": 0.2761323875747621, "vf_loss": 0.23450811790535225, "vf_explained_var": 0.8681016754359007, "kl": 1.142510998249054, "entropy": 5.713748967647552, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 11.539571069180965, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 1.0823415093123914, "policy_loss": 0.26853539189323783, "vf_loss": 0.5269897370948456, "vf_explained_var": 0.8844233486801386, "kl": 1.4340818881988526, "entropy": 5.713370382785797, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_20": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 19.535321739315986, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 2.867488693445921, "policy_loss": 0.3713766150176525, "vf_loss": 1.952372982352972, "vf_explained_var": 0.38406247869133947, "kl": 2.7186954118311406, "entropy": 5.011374570429325, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 17.820411470532417, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 0.9030882572755218, "policy_loss": 0.2846004114020616, "vf_loss": 0.27314124871045353, "vf_explained_var": 0.7052024316042662, "kl": 1.7267329409718513, "entropy": 5.733007168769836, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_10": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 15.414172351360321, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 0.9062580052763224, "policy_loss": 0.25716083189472555, "vf_loss": 0.42703318869462237, "vf_explained_var": 0.827220506593585, "kl": 1.1103199295699597, "entropy": 5.775391471385956, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 14.752028527855874, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 2.5048976920545103, "policy_loss": 0.31634885161183773, "vf_loss": 1.9173976004123687, "vf_explained_var": 0.08382267020642757, "kl": 1.3557561703026295, "entropy": 5.545526054501534, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_15": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 23.037461900711058, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 2.4930137518793343, "policy_loss": 0.3523242357186973, "vf_loss": 1.5781660623382776, "vf_explained_var": 0.6253017317503691, "kl": 2.8126171991229056, "entropy": 5.149953770637512, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_17": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 26.548891896009444, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 1.6872027337551116, "policy_loss": 0.3927196251228452, "vf_loss": 0.6593107575550675, "vf_explained_var": 0.6246158007532359, "kl": 3.1758617132902147, "entropy": 4.860410583019257, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_6": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 12.463435374200344, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 0.7921874340623617, "policy_loss": 0.276343695493415, "vf_loss": 0.24505254438845442, "vf_explained_var": 0.9103069819509984, "kl": 1.3539559625089168, "entropy": 5.699210879206658, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_8": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 19.74282488524914, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 5.000000000000001e-05, "total_loss": 1.4773888951167464, "policy_loss": 0.3123434400651604, "vf_loss": 0.8997351786121726, "vf_explained_var": 0.8230850953608752, "kl": 1.326551329344511, "entropy": 5.700825411081314, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 80.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}}, "num_env_steps_sampled": 4000, "num_env_steps_trained": 4000, "num_agent_steps_sampled": 84000, "num_agent_steps_trained": 84000}, "env_runners": {"episode_reward_max": -47.65000000000047, "episode_reward_min": -61.81000000000211, "episode_reward_mean": -55.29166666666787, "episode_len_mean": 500.0, "episode_media": {}, "episodes_timesteps_total": 3000, "policy_reward_min": {"agent_0": -5.240000000000027, "agent_1": -3.5500000000000087, "agent_2": -5.250000000000027, "agent_3": -5.300000000000027, "agent_4": -5.340000000000028, "agent_5": -4.9400000000000235, "agent_6": -5.310000000000027, "agent_7": -5.220000000000026, "agent_8": -5.040000000000025, "agent_9": -0.5000000000000003, "agent_10": -5.150000000000026, "agent_11": -5.150000000000026, "agent_12": -5.240000000000027, "agent_13": -5.270000000000027, "agent_14": -4.690000000000021, "agent_15": -5.140000000000025, "agent_16": -5.220000000000026, "agent_17": -4.790000000000022, "agent_18": -5.290000000000028, "agent_19": -5.090000000000026, "agent_20": -4.9000000000000234}, "policy_reward_max": {"agent_0": -0.5000000000000003, "agent_1": -0.5000000000000003, "agent_2": -0.5000000000000003, "agent_3": -0.5000000000000003, "agent_4": -0.5000000000000003, "agent_5": -0.5000000000000003, "agent_6": -0.5000000000000003, "agent_7": -0.5000000000000003, "agent_8": -0.5000000000000003, "agent_9": -0.5000000000000003, "agent_10": -0.5000000000000003, "agent_11": -0.5000000000000003, "agent_12": -0.5000000000000003, "agent_13": -0.5000000000000003, "agent_14": -4.000000000000013, "agent_15": -3.950000000000013, "agent_16": -4.140000000000015, "agent_17": -3.9800000000000133, "agent_18": -3.9000000000000123, "agent_19": -3.75000000000001, "agent_20": -3.6700000000000097}, "policy_reward_mean": {"agent_0": -2.0516666666666756, "agent_1": -1.008333333333335, "agent_2": -1.2916666666666712, "agent_3": -2.058333333333342, "agent_4": -1.3066666666666715, "agent_5": -1.240000000000004, "agent_6": -1.956666666666674, "agent_7": -3.01833333333334, "agent_8": -1.256666666666671, "agent_9": -0.5000000000000003, "agent_10": -1.973333333333341, "agent_11": -1.2750000000000046, "agent_12": -2.0550000000000086, "agent_13": -2.6183333333333434, "agent_14": -4.460000000000018, "agent_15": -4.646666666666687, "agent_16": -4.765000000000022, "agent_17": -4.5033333333333525, "agent_18": -4.766666666666689, "agent_19": -4.301666666666684, "agent_20": -4.238333333333349}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-53.290000000000305, -55.12000000000127, -61.81000000000211, -47.65000000000047, -57.68000000000135, -56.200000000001694], "episode_lengths": [500, 500, 500, 500, 500, 500], "policy_agent_0_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.070000000000025, -5.240000000000027], "policy_agent_1_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.5500000000000087], "policy_agent_2_reward": [-5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_3_reward": [-0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.050000000000025], "policy_agent_4_reward": [-0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_5_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003], "policy_agent_6_reward": [-5.310000000000027, -0.5000000000000003, -0.5000000000000003, -4.430000000000018, -0.5000000000000003, -0.5000000000000003], "policy_agent_7_reward": [-0.5000000000000003, -2.349999999999995, -5.220000000000026, -2.2499999999999942, -5.040000000000025, -2.75], "policy_agent_8_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.040000000000025, -0.5000000000000003], "policy_agent_9_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_10_reward": [-4.690000000000021, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003], "policy_agent_11_reward": [-0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_12_reward": [-0.5000000000000003, -5.240000000000027, -5.090000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_13_reward": [-0.5000000000000003, -5.220000000000026, -5.270000000000027, -0.5000000000000003, -0.9600000000000007, -3.2600000000000056], "policy_agent_14_reward": [-4.680000000000021, -4.600000000000021, -4.260000000000016, -4.000000000000013, -4.690000000000021, -4.530000000000019], "policy_agent_15_reward": [-5.140000000000025, -4.400000000000018, -4.830000000000022, -4.520000000000019, -5.040000000000025, -3.950000000000013], "policy_agent_16_reward": [-4.950000000000024, -5.150000000000026, -4.950000000000024, -4.140000000000015, -5.220000000000026, -4.180000000000016], "policy_agent_17_reward": [-4.780000000000021, -4.790000000000022, -3.9800000000000133, -4.3800000000000185, -4.340000000000017, -4.750000000000021], "policy_agent_18_reward": [-5.280000000000027, -4.150000000000015, -3.9000000000000123, -5.290000000000028, -4.800000000000022, -5.180000000000027], "policy_agent_19_reward": [-3.9800000000000133, -4.020000000000014, -5.090000000000026, -3.75000000000001, -4.370000000000019, -4.60000000000002], "policy_agent_20_reward": [-3.7300000000000106, -4.9000000000000234, -4.2300000000000155, -4.240000000000016, -3.6700000000000097, -4.660000000000021]}, "sampler_perf": {"mean_raw_obs_processing_ms": 1.885068176934956, "mean_inference_ms": 9.037369602694682, "mean_action_processing_ms": 0.7616480247107239, "mean_env_wait_ms": 2.7516258713049813, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"ObsPreprocessorConnector_ms": 0.0025202357579791356, "StateBufferConnector_ms": 0.0013482002984909784, "ViewRequirementAgentConnector_ms": 0.03008880312480624}, "num_episodes": 6, "episode_return_max": -47.65000000000047, "episode_return_min": -61.81000000000211, "episode_return_mean": -55.29166666666787, "episodes_this_iter": 6}, "num_healthy_workers": 3, "actor_manager_num_outstanding_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 84000, "num_agent_steps_trained": 84000, "num_env_steps_sampled": 4000, "num_env_steps_trained": 4000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 134.161603371492, "num_env_steps_trained_throughput_per_sec": 134.161603371492, "timesteps_total": 4000, "num_env_steps_sampled_lifetime": 4000, "num_agent_steps_sampled_lifetime": 84000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 84000, "timers": {"training_iteration_time_ms": 29814.796, "restore_workers_time_ms": 0.013, "training_step_time_ms": 29814.757, "sample_time_ms": 19727.818, "learn_time_ms": 10066.357, "learn_throughput": 397.363, "synch_weights_time_ms": 18.461}, "counters": {"num_env_steps_sampled": 4000, "num_env_steps_trained": 4000, "num_agent_steps_sampled": 84000, "num_agent_steps_trained": 84000}, "done": false, "training_iteration": 1, "trial_id": "cfc88_00000", "date": "2025-10-21_11-18-26", "timestamp": 1761038306, "time_this_iter_s": 29.82740616798401, "time_total_s": 29.82740616798401, "pid": 3279004, "hostname": "xuezhi-Precision-3660", "node_ip": "130.238.16.41", "config": {"exploration_config": {"type": "StochasticSampling"}, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "placement_strategy": "PACK", "num_gpus": 1, "_fake_gpus": false, "num_cpus_for_main_process": 1, "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "torch_ddp_kwargs": {}, "torch_skip_nan_gradients": false, "env": "<class 'train_utils.WarehouseMultiAgentEnv'>", "env_config": {"env_id": "tarware-extralarge-14agvs-7pickers-partialobs-chg-v1"}, "observation_space": null, "action_space": null, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "_is_atari": null, "disable_env_checking": false, "render_env": true, "action_mask_key": "action_mask", "env_runner_cls": null, "num_env_runners": 3, "create_local_env_runner": true, "num_envs_per_env_runner": 1, "gym_env_vectorize_mode": "SYNC", "num_cpus_per_env_runner": 1, "num_gpus_per_env_runner": 0, "custom_resources_per_env_runner": {}, "validate_env_runners_after_construction": true, "episodes_to_numpy": true, "max_requests_in_flight_per_env_runner": 1, "sample_timeout_s": 60.0, "_env_to_module_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "_module_to_env_connector": null, "add_default_connectors_to_module_to_env_pipeline": true, "merge_env_runner_states": "training_only", "broadcast_env_runner_states": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "compress_observations": false, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "sampler_perf_stats_ema_coef": null, "_is_online": true, "num_learners": 0, "num_gpus_per_learner": 1, "num_cpus_per_learner": "auto", "num_aggregator_actors_per_learner": 0, "max_requests_in_flight_per_aggregator_actor": 3, "local_gpu_idx": 0, "max_requests_in_flight_per_learner": 3, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "_train_batch_size_per_learner": null, "train_batch_size": 4000, "num_epochs": 10, "minibatch_size": 256, "shuffle_batch_per_epoch": true, "model": {"fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "log_std_clip_param": 20.0, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1, "_disable_preprocessor_api": false, "_disable_action_flattening": false}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "learner_config_dict": {}, "optimizer": {}, "_learner_class": null, "callbacks_on_algorithm_init": null, "callbacks_on_env_runners_recreated": null, "callbacks_on_offline_eval_runners_recreated": null, "callbacks_on_checkpoint_loaded": null, "callbacks_on_environment_created": null, "callbacks_on_episode_created": null, "callbacks_on_episode_start": null, "callbacks_on_episode_step": null, "callbacks_on_episode_end": null, "callbacks_on_evaluate_start": null, "callbacks_on_evaluate_end": null, "callbacks_on_evaluate_offline_start": null, "callbacks_on_evaluate_offline_end": null, "callbacks_on_sample_end": null, "callbacks_on_train_result": null, "explore": true, "enable_rl_module_and_learner": false, "enable_env_runner_and_connector_v2": false, "_prior_exploration_config": null, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function train.<locals>.<lambda> at 0x75979fd5beb0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "offline_data_class": null, "input_read_method": "read_parquet", "input_read_method_kwargs": {}, "input_read_schema": {}, "input_read_episodes": false, "input_read_sample_batches": false, "input_read_batch_size": null, "input_filesystem": null, "input_filesystem_kwargs": {}, "input_compress_columns": ["obs", "new_obs"], "input_spaces_jsonable": true, "materialize_data": false, "materialize_mapped_data": true, "map_batches_kwargs": {}, "iter_batches_kwargs": {}, "ignore_final_observation": false, "prelearner_class": null, "prelearner_buffer_class": null, "prelearner_buffer_kwargs": {}, "prelearner_module_synch_period": 10, "dataset_num_iters_per_learner": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "output_max_rows_per_file": null, "output_write_remaining_data": false, "output_write_method": "write_parquet", "output_write_method_kwargs": {}, "output_filesystem": null, "output_filesystem_kwargs": {}, "output_write_episodes": true, "offline_sampling": false, "evaluation_interval": 10, "evaluation_duration": 5, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 120.0, "evaluation_auto_duration_min_env_steps_per_sample": 100, "evaluation_auto_duration_max_env_steps_per_sample": 2000, "evaluation_parallel_to_training": false, "evaluation_force_reset_envs_before_iteration": true, "evaluation_config": {"explore": false}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_env_runners": 0, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 10.0, "offline_evaluation_interval": null, "num_offline_eval_runners": 0, "offline_evaluation_type": null, "offline_eval_runner_class": null, "offline_loss_for_module_fn": null, "offline_evaluation_duration": 1, "offline_evaluation_parallel_to_training": false, "offline_evaluation_timeout_s": 120.0, "num_cpus_per_offline_eval_runner": 1, "num_gpus_per_offline_eval_runner": 0, "custom_resources_per_offline_eval_runner": {}, "restart_failed_offline_eval_runners": true, "ignore_offline_eval_runner_failures": false, "max_num_offline_eval_runner_restarts": 1000, "offline_eval_runner_restore_timeout_s": 1800.0, "max_requests_in_flight_per_offline_eval_runner": 1, "validate_offline_eval_runners_after_construction": true, "offline_eval_runner_health_probe_timeout_s": 30.0, "offline_eval_rl_module_inference_only": false, "broadcast_offline_eval_runner_states": false, "offline_eval_batch_size_per_runner": 256, "dataset_num_iters_per_eval_runner": 1, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "log_gradients": false, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "restart_failed_env_runners": true, "ignore_env_runner_failures": false, "max_num_env_runner_restarts": 1000, "delay_between_env_runner_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_env_runner_failures_tolerance": 100, "env_runner_health_probe_timeout_s": 30.0, "env_runner_restore_timeout_s": 1800.0, "_model_config": {}, "_rl_module_spec": null, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "_validate_config": true, "_use_msgpack_checkpoints": false, "_torch_grad_scaler_class": null, "_torch_lr_scheduler_classes": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "_dont_auto_sync_env_runner_states": false, "env_task_fn": -1, "enable_connectors": -1, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "enable_async_evaluation": -1, "custom_async_evaluation_function": -1, "_enable_rl_module_api": -1, "auto_wrap_old_gym_envs": -1, "always_attach_evaluation_results": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "clip_param": 0.2, "vf_clip_param": 10.0, "entropy_coeff_schedule": null, "lr_schedule": null, "sgd_minibatch_size": -1, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 0.95, "input": "sampler", "policies": {"agent_0": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_1": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_2": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_3": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_4": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_5": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_6": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_7": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_8": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_9": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_10": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_11": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_12": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_13": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_14": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_15": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_16": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_17": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_18": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_19": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_20": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}]}, "callbacks": "<class 'ray.rllib.callbacks.callbacks.RLlibCallback'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch"}, "time_since_restore": 29.82740616798401, "iterations_since_restore": 1, "perf": {"cpu_util_percent": 8.633333333333333, "ram_util_percent": 26.27380952380953, "gpu_util_percent0": 0.17166666666666663, "vram_util_percent0": 0.08365552376299812}}
{"custom_metrics": {}, "episode_media": {}, "info": {"learner": {"agent_4": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.909130147099495, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.016979002981679515, "policy_loss": -0.04630876751616597, "vf_loss": 0.026519792480394246, "vf_explained_var": 0.9327010951936245, "kl": 0.009366570636796967, "entropy": 5.908527258038521, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_19": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 9.141758102178574, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.050015307088324336, "policy_loss": -0.04756913199962583, "vf_loss": 0.09244133365573362, "vf_explained_var": 0.9139888614416123, "kl": 0.017143685689279663, "entropy": 5.40358584523201, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.622940254211426, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.0006329538591671735, "policy_loss": -0.04560225273307879, "vf_loss": 0.04274510479299352, "vf_explained_var": 0.9422575145959854, "kl": 0.011633669355628883, "entropy": 5.760531812906265, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_16": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 9.031878976523876, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.05654986468434799, "policy_loss": -0.04045500239299145, "vf_loss": 0.0920280703343451, "vf_explained_var": 0.9411739550530911, "kl": 0.01658931830700112, "entropy": 5.134206223487854, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_9": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.928624233603477, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.01809791368432343, "policy_loss": -0.0484969496174017, "vf_loss": 0.027555052257957867, "vf_explained_var": 0.9028727728873491, "kl": 0.009479942717259377, "entropy": 5.875709342956543, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_5": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.667160151898861, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.012745950923999771, "policy_loss": -0.04809235523425741, "vf_loss": 0.03219178851577453, "vf_explained_var": 0.9232967037707567, "kl": 0.010515386888474698, "entropy": 5.834368014335633, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_7": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.409034617245197, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.008179181166633498, "policy_loss": -0.047874951150151904, "vf_loss": 0.03623034308548086, "vf_explained_var": 0.9619458045810461, "kl": 0.011551423603923139, "entropy": 5.830965587496758, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_14": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 9.597347916662693, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.04380243107152637, "policy_loss": -0.04534043233961711, "vf_loss": 0.08473036913201212, "vf_explained_var": 0.8844053272157908, "kl": 0.014708313037590414, "entropy": 5.433905383944511, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_18": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 8.865763902664185, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.046351074235280974, "policy_loss": -0.04746562764630653, "vf_loss": 0.08892757012508809, "vf_explained_var": 0.8971827991306782, "kl": 0.016297107265040724, "entropy": 5.443646404147148, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_12": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.417821702361107, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.005680157261667773, "policy_loss": -0.046269027749076486, "vf_loss": 0.03740946234902367, "vf_explained_var": 0.9476967711001635, "kl": 0.010598025385623554, "entropy": 5.8967968910932544, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_11": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 7.225510402023792, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.027455013601866085, "policy_loss": -0.042650094471173364, "vf_loss": 0.0664771513431333, "vf_explained_var": 0.9381018001586199, "kl": 0.012093188451979237, "entropy": 5.830682784318924, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_13": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.528804391622543, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.02021605368499877, "policy_loss": -0.049471611547051, "vf_loss": 0.026675333041930573, "vf_explained_var": 0.9031124468892813, "kl": 0.00860074714255461, "entropy": 5.886542427539825, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.379590502381324, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.00048764277598820625, "policy_loss": -0.04358983943529893, "vf_loss": 0.040725938917603344, "vf_explained_var": 0.9409364633262157, "kl": 0.0111718134447155, "entropy": 5.883359068632126, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_20": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 8.414472645521164, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.03728176393196918, "policy_loss": -0.04703901723842137, "vf_loss": 0.08006414646515622, "vf_explained_var": 0.9450029443949461, "kl": 0.014188786606428038, "entropy": 5.489639553427696, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.721385069191456, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.01311755525530316, "policy_loss": -0.04908314282656647, "vf_loss": 0.03300143691012636, "vf_explained_var": 0.8944685854017734, "kl": 0.009880498018156852, "entropy": 5.8364703297615055, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_10": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.222324807941914, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.004132482739078114, "policy_loss": -0.04666927382932044, "vf_loss": 0.03952040181029588, "vf_explained_var": 0.9219110306352377, "kl": 0.0100546274270697, "entropy": 5.898553511500358, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.339191564917565, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.0023278033069800584, "policy_loss": -0.045680419556447305, "vf_loss": 0.04484339708578773, "vf_explained_var": 0.9443200215697288, "kl": 0.010549418092823903, "entropy": 5.779769298434258, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_15": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 11.521334969997406, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.08961420122359413, "policy_loss": -0.043640212246100415, "vf_loss": 0.1275790423853323, "vf_explained_var": 0.9362957011908293, "kl": 0.018917895084011187, "entropy": 5.459028378129005, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_17": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 11.75059278011322, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.07076258170418441, "policy_loss": -0.042070050424081275, "vf_loss": 0.10752514596097171, "vf_explained_var": 0.8726330514997244, "kl": 0.01769162177275606, "entropy": 5.273749259114266, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_6": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.241212785243988, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.009798281644179951, "policy_loss": -0.05041577887313906, "vf_loss": 0.037440441659418865, "vf_explained_var": 0.9222481984645128, "kl": 0.010590184068649774, "entropy": 5.841851428151131, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_8": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.244158181548118, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.0070841055945493284, "policy_loss": -0.047021409396256784, "vf_loss": 0.037226357136387375, "vf_explained_var": 0.9253244549036026, "kl": 0.009036487657055886, "entropy": 5.864993420243263, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}}, "num_env_steps_sampled": 8000, "num_env_steps_trained": 8000, "num_agent_steps_sampled": 168000, "num_agent_steps_trained": 168000}, "env_runners": {"episode_reward_max": -47.65000000000047, "episode_reward_min": -73.5400000000017, "episode_reward_mean": -57.28600000000135, "episode_len_mean": 500.0, "episode_media": {}, "episodes_timesteps_total": 7500, "policy_reward_min": {"agent_0": -5.240000000000027, "agent_1": -5.360000000000028, "agent_2": -5.300000000000027, "agent_3": -5.340000000000028, "agent_4": -5.340000000000028, "agent_5": -5.350000000000028, "agent_6": -5.310000000000027, "agent_7": -5.340000000000028, "agent_8": -5.280000000000027, "agent_9": -4.9400000000000235, "agent_10": -5.350000000000028, "agent_11": -5.150000000000026, "agent_12": -5.240000000000027, "agent_13": -5.310000000000027, "agent_14": -5.240000000000027, "agent_15": -5.140000000000025, "agent_16": -5.280000000000027, "agent_17": -5.340000000000028, "agent_18": -5.290000000000028, "agent_19": -5.110000000000025, "agent_20": -5.090000000000026}, "policy_reward_max": {"agent_0": -0.5000000000000003, "agent_1": -0.5000000000000003, "agent_2": -0.5000000000000003, "agent_3": -0.5000000000000003, "agent_4": -0.5000000000000003, "agent_5": -0.5000000000000003, "agent_6": -0.5000000000000003, "agent_7": -0.5000000000000003, "agent_8": -0.5000000000000003, "agent_9": -0.5000000000000003, "agent_10": -0.5000000000000003, "agent_11": -0.5000000000000003, "agent_12": -0.5000000000000003, "agent_13": -0.5000000000000003, "agent_14": -3.780000000000011, "agent_15": -3.770000000000011, "agent_16": -4.120000000000014, "agent_17": -3.8100000000000116, "agent_18": -3.5300000000000082, "agent_19": -3.75000000000001, "agent_20": -3.6700000000000097}, "policy_reward_mean": {"agent_0": -1.4280000000000053, "agent_1": -1.3473333333333377, "agent_2": -2.153333333333339, "agent_3": -2.722000000000013, "agent_4": -1.139333333333337, "agent_5": -2.0413333333333417, "agent_6": -1.7040000000000064, "agent_7": -2.4680000000000084, "agent_8": -1.3906666666666716, "agent_9": -0.7960000000000017, "agent_10": -2.1240000000000077, "agent_11": -1.484666666666672, "agent_12": -1.8126666666666726, "agent_13": -2.9260000000000135, "agent_14": -4.645333333333353, "agent_15": -4.450666666666685, "agent_16": -4.731333333333355, "agent_17": -4.5193333333333525, "agent_18": -4.630666666666688, "agent_19": -4.404000000000017, "agent_20": -4.367333333333351}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-53.290000000000305, -55.12000000000127, -61.81000000000211, -47.65000000000047, -57.68000000000135, -56.200000000001694, -52.780000000000356, -57.85000000000114, -54.040000000000376, -64.27000000000298, -49.57000000000024, -73.5400000000017, -62.680000000002885, -60.280000000003, -52.53000000000038], "episode_lengths": [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500], "policy_agent_0_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.070000000000025, -5.240000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.110000000000025, -0.5000000000000003, -0.5000000000000003], "policy_agent_1_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.5500000000000087, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -5.360000000000028], "policy_agent_2_reward": [-5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -3.500000000000008, -4.840000000000023, -2.129999999999993, -2.149999999999986, -5.130000000000026], "policy_agent_3_reward": [-0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.050000000000025, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -5.160000000000026, -0.5000000000000003, -5.330000000000028, -5.240000000000027, -0.7300000000000005, -5.180000000000026], "policy_agent_4_reward": [-0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_5_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5000000000000003, -4.870000000000023, -5.350000000000028, -0.5000000000000003], "policy_agent_6_reward": [-5.310000000000027, -0.5000000000000003, -0.5000000000000003, -4.430000000000018, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_7_reward": [-0.5000000000000003, -2.349999999999995, -5.220000000000026, -2.2499999999999942, -5.040000000000025, -2.75, -5.300000000000027, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_8_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.040000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -4.54000000000002, -0.5000000000000003], "policy_agent_9_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_10_reward": [-4.690000000000021, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.120000000000026, -4.930000000000024, -5.350000000000028, -0.5000000000000003, -2.119999999999993, -0.5000000000000003], "policy_agent_11_reward": [-0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.3699999999999974, -0.5000000000000003, -5.150000000000026, -0.5000000000000003], "policy_agent_12_reward": [-0.5000000000000003, -5.240000000000027, -5.090000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.170000000000027, -0.5000000000000003, -0.5000000000000003, -2.139999999999993, -0.5000000000000003, -0.5000000000000003, -4.550000000000019, -0.5000000000000003, -0.5000000000000003], "policy_agent_13_reward": [-0.5000000000000003, -5.220000000000026, -5.270000000000027, -0.5000000000000003, -0.9600000000000007, -3.2600000000000056, -5.110000000000025, -5.260000000000027, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -5.280000000000027, -5.220000000000026, -0.5000000000000003, -0.5000000000000003], "policy_agent_14_reward": [-4.680000000000021, -4.600000000000021, -4.260000000000016, -4.000000000000013, -4.690000000000021, -4.530000000000019, -5.240000000000027, -4.8600000000000225, -5.150000000000026, -5.220000000000026, -3.780000000000011, -4.090000000000014, -5.000000000000024, -4.970000000000024, -4.61000000000002], "policy_agent_15_reward": [-5.140000000000025, -4.400000000000018, -4.830000000000022, -4.520000000000019, -5.040000000000025, -3.950000000000013, -4.2300000000000155, -4.700000000000021, -3.770000000000011, -3.970000000000013, -4.950000000000024, -4.110000000000015, -4.560000000000019, -4.470000000000018, -4.120000000000014], "policy_agent_16_reward": [-4.950000000000024, -5.150000000000026, -4.950000000000024, -4.140000000000015, -5.220000000000026, -4.180000000000016, -5.280000000000027, -4.750000000000021, -5.140000000000026, -4.530000000000019, -4.410000000000018, -4.720000000000021, -4.800000000000023, -4.120000000000014, -4.63000000000002], "policy_agent_17_reward": [-4.780000000000021, -4.790000000000022, -3.9800000000000133, -4.3800000000000185, -4.340000000000017, -4.750000000000021, -3.8100000000000116, -4.54000000000002, -5.190000000000026, -5.100000000000025, -4.140000000000015, -4.140000000000015, -4.260000000000016, -4.250000000000016, -5.340000000000028], "policy_agent_18_reward": [-5.280000000000027, -4.150000000000015, -3.9000000000000123, -5.290000000000028, -4.800000000000022, -5.180000000000027, -3.5300000000000082, -3.780000000000011, -4.720000000000021, -4.720000000000021, -4.460000000000019, -5.290000000000028, -4.770000000000022, -4.990000000000024, -4.60000000000002], "policy_agent_19_reward": [-3.9800000000000133, -4.020000000000014, -5.090000000000026, -3.75000000000001, -4.370000000000019, -4.60000000000002, -5.110000000000025, -4.9400000000000235, -4.560000000000019, -4.220000000000016, -4.0700000000000145, -5.040000000000025, -3.8100000000000116, -4.5400000000000205, -3.9600000000000133], "policy_agent_20_reward": [-3.7300000000000106, -4.9000000000000234, -4.2300000000000155, -4.240000000000016, -3.6700000000000097, -4.660000000000021, -4.500000000000019, -4.350000000000017, -4.460000000000019, -4.570000000000021, -4.550000000000019, -5.090000000000026, -4.360000000000017, -4.100000000000014, -4.100000000000015]}, "sampler_perf": {"mean_raw_obs_processing_ms": 1.907453247893545, "mean_inference_ms": 9.072667883345964, "mean_action_processing_ms": 0.7360830577325195, "mean_env_wait_ms": 2.751247410341937, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"ObsPreprocessorConnector_ms": 0.0025690169561476935, "StateBufferConnector_ms": 0.0014047017173161582, "ViewRequirementAgentConnector_ms": 0.03155617486862909}, "num_episodes": 9, "episode_return_max": -47.65000000000047, "episode_return_min": -73.5400000000017, "episode_return_mean": -57.28600000000135, "episodes_this_iter": 9}, "num_healthy_workers": 3, "actor_manager_num_outstanding_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 168000, "num_agent_steps_trained": 168000, "num_env_steps_sampled": 8000, "num_env_steps_trained": 8000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 134.2917119899695, "num_env_steps_trained_throughput_per_sec": 134.2917119899695, "timesteps_total": 8000, "num_env_steps_sampled_lifetime": 8000, "num_agent_steps_sampled_lifetime": 168000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 168000, "timers": {"training_iteration_time_ms": 29800.355, "restore_workers_time_ms": 0.015, "training_step_time_ms": 29800.306, "sample_time_ms": 19716.949, "learn_time_ms": 10063.24, "learn_throughput": 397.486, "synch_weights_time_ms": 17.526}, "counters": {"num_env_steps_sampled": 8000, "num_env_steps_trained": 8000, "num_agent_steps_sampled": 168000, "num_agent_steps_trained": 168000}, "done": false, "training_iteration": 2, "trial_id": "cfc88_00000", "date": "2025-10-21_11-18-56", "timestamp": 1761038336, "time_this_iter_s": 29.795695781707764, "time_total_s": 59.62310194969177, "pid": 3279004, "hostname": "xuezhi-Precision-3660", "node_ip": "130.238.16.41", "config": {"exploration_config": {"type": "StochasticSampling"}, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "placement_strategy": "PACK", "num_gpus": 1, "_fake_gpus": false, "num_cpus_for_main_process": 1, "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "torch_ddp_kwargs": {}, "torch_skip_nan_gradients": false, "env": "<class 'train_utils.WarehouseMultiAgentEnv'>", "env_config": {"env_id": "tarware-extralarge-14agvs-7pickers-partialobs-chg-v1"}, "observation_space": null, "action_space": null, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "_is_atari": null, "disable_env_checking": false, "render_env": true, "action_mask_key": "action_mask", "env_runner_cls": null, "num_env_runners": 3, "create_local_env_runner": true, "num_envs_per_env_runner": 1, "gym_env_vectorize_mode": "SYNC", "num_cpus_per_env_runner": 1, "num_gpus_per_env_runner": 0, "custom_resources_per_env_runner": {}, "validate_env_runners_after_construction": true, "episodes_to_numpy": true, "max_requests_in_flight_per_env_runner": 1, "sample_timeout_s": 60.0, "_env_to_module_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "_module_to_env_connector": null, "add_default_connectors_to_module_to_env_pipeline": true, "merge_env_runner_states": "training_only", "broadcast_env_runner_states": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "compress_observations": false, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "sampler_perf_stats_ema_coef": null, "_is_online": true, "num_learners": 0, "num_gpus_per_learner": 1, "num_cpus_per_learner": "auto", "num_aggregator_actors_per_learner": 0, "max_requests_in_flight_per_aggregator_actor": 3, "local_gpu_idx": 0, "max_requests_in_flight_per_learner": 3, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "_train_batch_size_per_learner": null, "train_batch_size": 4000, "num_epochs": 10, "minibatch_size": 256, "shuffle_batch_per_epoch": true, "model": {"fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "log_std_clip_param": 20.0, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1, "_disable_preprocessor_api": false, "_disable_action_flattening": false}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "learner_config_dict": {}, "optimizer": {}, "_learner_class": null, "callbacks_on_algorithm_init": null, "callbacks_on_env_runners_recreated": null, "callbacks_on_offline_eval_runners_recreated": null, "callbacks_on_checkpoint_loaded": null, "callbacks_on_environment_created": null, "callbacks_on_episode_created": null, "callbacks_on_episode_start": null, "callbacks_on_episode_step": null, "callbacks_on_episode_end": null, "callbacks_on_evaluate_start": null, "callbacks_on_evaluate_end": null, "callbacks_on_evaluate_offline_start": null, "callbacks_on_evaluate_offline_end": null, "callbacks_on_sample_end": null, "callbacks_on_train_result": null, "explore": true, "enable_rl_module_and_learner": false, "enable_env_runner_and_connector_v2": false, "_prior_exploration_config": null, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function train.<locals>.<lambda> at 0x75979fd64ca0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "offline_data_class": null, "input_read_method": "read_parquet", "input_read_method_kwargs": {}, "input_read_schema": {}, "input_read_episodes": false, "input_read_sample_batches": false, "input_read_batch_size": null, "input_filesystem": null, "input_filesystem_kwargs": {}, "input_compress_columns": ["obs", "new_obs"], "input_spaces_jsonable": true, "materialize_data": false, "materialize_mapped_data": true, "map_batches_kwargs": {}, "iter_batches_kwargs": {}, "ignore_final_observation": false, "prelearner_class": null, "prelearner_buffer_class": null, "prelearner_buffer_kwargs": {}, "prelearner_module_synch_period": 10, "dataset_num_iters_per_learner": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "output_max_rows_per_file": null, "output_write_remaining_data": false, "output_write_method": "write_parquet", "output_write_method_kwargs": {}, "output_filesystem": null, "output_filesystem_kwargs": {}, "output_write_episodes": true, "offline_sampling": false, "evaluation_interval": 10, "evaluation_duration": 5, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 120.0, "evaluation_auto_duration_min_env_steps_per_sample": 100, "evaluation_auto_duration_max_env_steps_per_sample": 2000, "evaluation_parallel_to_training": false, "evaluation_force_reset_envs_before_iteration": true, "evaluation_config": {"explore": false}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_env_runners": 0, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 10.0, "offline_evaluation_interval": null, "num_offline_eval_runners": 0, "offline_evaluation_type": null, "offline_eval_runner_class": null, "offline_loss_for_module_fn": null, "offline_evaluation_duration": 1, "offline_evaluation_parallel_to_training": false, "offline_evaluation_timeout_s": 120.0, "num_cpus_per_offline_eval_runner": 1, "num_gpus_per_offline_eval_runner": 0, "custom_resources_per_offline_eval_runner": {}, "restart_failed_offline_eval_runners": true, "ignore_offline_eval_runner_failures": false, "max_num_offline_eval_runner_restarts": 1000, "offline_eval_runner_restore_timeout_s": 1800.0, "max_requests_in_flight_per_offline_eval_runner": 1, "validate_offline_eval_runners_after_construction": true, "offline_eval_runner_health_probe_timeout_s": 30.0, "offline_eval_rl_module_inference_only": false, "broadcast_offline_eval_runner_states": false, "offline_eval_batch_size_per_runner": 256, "dataset_num_iters_per_eval_runner": 1, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "log_gradients": false, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "restart_failed_env_runners": true, "ignore_env_runner_failures": false, "max_num_env_runner_restarts": 1000, "delay_between_env_runner_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_env_runner_failures_tolerance": 100, "env_runner_health_probe_timeout_s": 30.0, "env_runner_restore_timeout_s": 1800.0, "_model_config": {}, "_rl_module_spec": null, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "_validate_config": true, "_use_msgpack_checkpoints": false, "_torch_grad_scaler_class": null, "_torch_lr_scheduler_classes": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "_dont_auto_sync_env_runner_states": false, "env_task_fn": -1, "enable_connectors": -1, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "enable_async_evaluation": -1, "custom_async_evaluation_function": -1, "_enable_rl_module_api": -1, "auto_wrap_old_gym_envs": -1, "always_attach_evaluation_results": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "clip_param": 0.2, "vf_clip_param": 10.0, "entropy_coeff_schedule": null, "lr_schedule": null, "sgd_minibatch_size": -1, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 0.95, "input": "sampler", "policies": {"agent_0": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_1": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_2": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_3": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_4": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_5": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_6": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_7": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_8": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_9": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_10": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_11": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_12": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_13": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_14": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_15": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_16": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_17": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_18": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_19": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_20": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}]}, "callbacks": "<class 'ray.rllib.callbacks.callbacks.RLlibCallback'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch"}, "time_since_restore": 59.62310194969177, "iterations_since_restore": 2, "perf": {"cpu_util_percent": 9.226829268292683, "ram_util_percent": 27.821951219512197, "gpu_util_percent0": 0.18707317073170734, "vram_util_percent0": 0.09591788101564454}}
{"custom_metrics": {}, "episode_media": {}, "info": {"learner": {"agent_4": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.350637106597423, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.027584677598497365, "policy_loss": -0.05380636350018904, "vf_loss": 0.023669149860506878, "vf_explained_var": 0.9244089983403683, "kl": 0.008508452125841792, "entropy": 5.892447164654731, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_19": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 8.212970599532127, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.02830151858215686, "policy_loss": -0.04964352739043534, "vf_loss": 0.07334012913051993, "vf_explained_var": 0.9482061035931111, "kl": 0.01534972080585587, "entropy": 5.3755299866199495, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.99754648655653, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.01576282390888082, "policy_loss": -0.04799501278612297, "vf_loss": 0.02955251226085238, "vf_explained_var": 0.9599227514117956, "kl": 0.008932256634480298, "entropy": 5.730618041753769, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_16": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 8.9202400714159, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.03432975523173809, "policy_loss": -0.04264968045754358, "vf_loss": 0.07301888077054172, "vf_explained_var": 0.9299347296357154, "kl": 0.013201853676469, "entropy": 5.1498523384332655, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_9": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.03537827283144, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.025538887931907082, "policy_loss": -0.051256504043703896, "vf_loss": 0.02314433444989845, "vf_explained_var": 0.9594564281404019, "kl": 0.008577603035749304, "entropy": 5.884896540641785, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_5": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.2515733420848845, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.025432370666385394, "policy_loss": -0.05211241418728605, "vf_loss": 0.023765907989582048, "vf_explained_var": 0.9248537439852953, "kl": 0.00971377804841278, "entropy": 5.787606817483902, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_7": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.819451606273651, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.02288883431465365, "policy_loss": -0.05448695514060091, "vf_loss": 0.028680817264830693, "vf_explained_var": 0.957449097558856, "kl": 0.00972434417852903, "entropy": 5.8437325835227965, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_14": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 8.96919917166233, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.032435651219566354, "policy_loss": -0.04964001620537602, "vf_loss": 0.0784339454723522, "vf_explained_var": 0.8977758891880512, "kl": 0.012139076717971875, "entropy": 5.452109816670418, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_18": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 9.97269066274166, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.030639317746681625, "policy_loss": -0.04589509833022021, "vf_loss": 0.07222579473163933, "vf_explained_var": 0.9345384731888771, "kl": 0.014362068458900878, "entropy": 5.4305765390396115, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_12": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.270170100033283, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.013693211718054954, "policy_loss": -0.04947968231863342, "vf_loss": 0.032959872973151505, "vf_explained_var": 0.9470054868608713, "kl": 0.009421986874625875, "entropy": 5.885759088397026, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_11": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.286587208509445, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.003160198376281187, "policy_loss": -0.047187955694971605, "vf_loss": 0.047341562568908556, "vf_explained_var": 0.9549512371420861, "kl": 0.01002196943188476, "entropy": 5.833373922109604, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_13": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.038217054307461, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.02819715071382234, "policy_loss": -0.051818448439007625, "vf_loss": 0.02090209210291505, "vf_explained_var": 0.9558411635458469, "kl": 0.009064019767697573, "entropy": 5.851919248700142, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.024792556464672, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.018157124254503287, "policy_loss": -0.04919642081076745, "vf_loss": 0.02849906335468404, "vf_explained_var": 0.9330974031239748, "kl": 0.008467444697535919, "entropy": 5.8874386757612225, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_20": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.831731328368187, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.008350158308167011, "policy_loss": -0.04832874441344757, "vf_loss": 0.052933902689255775, "vf_explained_var": 0.8806165970861912, "kl": 0.012483335221073987, "entropy": 5.474969837069511, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.056102959811687, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.024109951104037464, "policy_loss": -0.052766839804826306, "vf_loss": 0.026044453511713073, "vf_explained_var": 0.9093841176480055, "kl": 0.008708118424878796, "entropy": 5.841679164767266, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_10": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.065691554546357, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.015028353202797006, "policy_loss": -0.049859072011895475, "vf_loss": 0.03237888704170473, "vf_explained_var": 0.9210980348289013, "kl": 0.008172773359556412, "entropy": 5.874786052107811, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.342705552279949, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.008198350136808585, "policy_loss": -0.04776053399546072, "vf_loss": 0.03644334844429977, "vf_explained_var": 0.9266115874052048, "kl": 0.010396119427119616, "entropy": 5.769637057185173, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_15": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 8.878148230910302, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.04059645563538652, "policy_loss": -0.04614102634950541, "vf_loss": 0.08223474932601675, "vf_explained_var": 0.9396671153604984, "kl": 0.01500911011187327, "entropy": 5.425288224220276, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_17": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 11.334854735434055, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.029128080107329878, "policy_loss": -0.046745431632734834, "vf_loss": 0.07176599823869764, "vf_explained_var": 0.9304987214505672, "kl": 0.013691708225158061, "entropy": 5.195937529206276, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_6": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.621358139812946, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.016331137869565283, "policy_loss": -0.05276543879590463, "vf_loss": 0.03324306798167527, "vf_explained_var": 0.9304069377481937, "kl": 0.010637444470041047, "entropy": 5.818560013175011, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_8": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.9558788254857062, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.019285168346686987, "policy_loss": -0.04736167467781342, "vf_loss": 0.025724310101941227, "vf_explained_var": 0.9604801170527935, "kl": 0.007840650561244684, "entropy": 5.859850308299064, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 400.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}}, "num_env_steps_sampled": 12000, "num_env_steps_trained": 12000, "num_agent_steps_sampled": 252000, "num_agent_steps_trained": 252000}, "env_runners": {"episode_reward_max": -43.9300000000001, "episode_reward_min": -73.5400000000017, "episode_reward_mean": -57.02727272727422, "episode_len_mean": 500.0, "episode_media": {}, "episodes_timesteps_total": 11000, "policy_reward_min": {"agent_0": -5.240000000000027, "agent_1": -5.360000000000028, "agent_2": -5.300000000000027, "agent_3": -5.350000000000028, "agent_4": -5.340000000000028, "agent_5": -5.350000000000028, "agent_6": -5.310000000000027, "agent_7": -5.340000000000028, "agent_8": -5.290000000000028, "agent_9": -4.9400000000000235, "agent_10": -5.350000000000028, "agent_11": -5.150000000000026, "agent_12": -5.280000000000027, "agent_13": -5.310000000000027, "agent_14": -5.240000000000027, "agent_15": -5.140000000000025, "agent_16": -5.280000000000027, "agent_17": -5.3700000000000285, "agent_18": -5.290000000000028, "agent_19": -5.110000000000025, "agent_20": -5.330000000000028}, "policy_reward_max": {"agent_0": -0.5000000000000003, "agent_1": -0.5000000000000003, "agent_2": -0.5000000000000003, "agent_3": -0.5000000000000003, "agent_4": -0.5000000000000003, "agent_5": -0.5000000000000003, "agent_6": -0.5000000000000003, "agent_7": -0.5000000000000003, "agent_8": -0.5000000000000003, "agent_9": -0.5000000000000003, "agent_10": -0.5000000000000003, "agent_11": -0.5000000000000003, "agent_12": -0.5000000000000003, "agent_13": -0.5000000000000003, "agent_14": -3.680000000000009, "agent_15": -3.770000000000011, "agent_16": -4.120000000000014, "agent_17": -3.8100000000000116, "agent_18": -3.5300000000000082, "agent_19": -3.75000000000001, "agent_20": -3.6700000000000097}, "policy_reward_mean": {"agent_0": -1.6554545454545513, "agent_1": -1.2786363636363678, "agent_2": -2.1013636363636423, "agent_3": -2.8654545454545586, "agent_4": -1.0863636363636398, "agent_5": -1.8550000000000069, "agent_6": -2.021363636363644, "agent_7": -1.955454545454551, "agent_8": -1.6509090909090967, "agent_9": -0.7018181818181831, "agent_10": -2.5850000000000093, "agent_11": -1.1713636363636402, "agent_12": -1.6959090909090966, "agent_13": -2.7750000000000123, "agent_14": -4.583181818181837, "agent_15": -4.402272727272744, "agent_16": -4.683636363636383, "agent_17": -4.564090909090929, "agent_18": -4.54590909090911, "agent_19": -4.360000000000017, "agent_20": -4.489090909090928}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-53.290000000000305, -55.12000000000127, -61.81000000000211, -47.65000000000047, -57.68000000000135, -56.200000000001694, -52.780000000000356, -57.85000000000114, -54.040000000000376, -64.27000000000298, -49.57000000000024, -73.5400000000017, -62.680000000002885, -60.280000000003, -52.53000000000038, -58.05000000000256, -43.9300000000001, -59.7700000000019, -61.3400000000021, -52.9100000000017, -56.880000000001324, -62.430000000003005], "episode_lengths": [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500], "policy_agent_0_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.070000000000025, -5.240000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.110000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -2.5999999999999983, -5.210000000000027, -0.5000000000000003], "policy_agent_1_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.5500000000000087, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -5.360000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.920000000000023, -0.5000000000000003], "policy_agent_2_reward": [-5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -3.500000000000008, -4.840000000000023, -2.129999999999993, -2.149999999999986, -5.130000000000026, -2.429999999999996, -0.5000000000000003, -0.5000000000000003, -4.67000000000002, -0.5000000000000003, -0.5000000000000003, -4.830000000000022], "policy_agent_3_reward": [-0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.050000000000025, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -5.160000000000026, -0.5000000000000003, -5.330000000000028, -5.240000000000027, -0.7300000000000005, -5.180000000000026, -4.590000000000019, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -5.190000000000026, -0.8900000000000007, -5.350000000000028], "policy_agent_4_reward": [-0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.8100000000000116, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_5_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5000000000000003, -4.870000000000023, -5.350000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.589999999999998], "policy_agent_6_reward": [-5.310000000000027, -0.5000000000000003, -0.5000000000000003, -4.430000000000018, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -4.930000000000024, -5.030000000000024, -0.5000000000000003, -2.429999999999996, -0.5000000000000003, -0.5000000000000003], "policy_agent_7_reward": [-0.5000000000000003, -2.349999999999995, -5.220000000000026, -2.2499999999999942, -5.040000000000025, -2.75, -5.300000000000027, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.0000000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_8_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.040000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -4.54000000000002, -0.5000000000000003, -3.0900000000000034, -0.5000000000000003, -0.5000000000000003, -5.080000000000025, -0.5000000000000003, -5.290000000000028, -0.5000000000000003], "policy_agent_9_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_10_reward": [-4.690000000000021, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.120000000000026, -4.930000000000024, -5.350000000000028, -0.5000000000000003, -2.119999999999993, -0.5000000000000003, -4.920000000000023, -2.6199999999999983, -0.5000000000000003, -3.4700000000000077, -4.560000000000019, -4.910000000000023, -4.030000000000014], "policy_agent_11_reward": [-0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.3699999999999974, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_12_reward": [-0.5000000000000003, -5.240000000000027, -5.090000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.170000000000027, -0.5000000000000003, -0.5000000000000003, -2.139999999999993, -0.5000000000000003, -0.5000000000000003, -4.550000000000019, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.3399999999999954, -0.5000000000000003, -5.280000000000027], "policy_agent_13_reward": [-0.5000000000000003, -5.220000000000026, -5.270000000000027, -0.5000000000000003, -0.9600000000000007, -3.2600000000000056, -5.110000000000025, -5.260000000000027, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -5.280000000000027, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.210000000000027, -4.920000000000023, -0.5000000000000003, -0.5000000000000003, -5.030000000000024], "policy_agent_14_reward": [-4.680000000000021, -4.600000000000021, -4.260000000000016, -4.000000000000013, -4.690000000000021, -4.530000000000019, -5.240000000000027, -4.8600000000000225, -5.150000000000026, -5.220000000000026, -3.780000000000011, -4.090000000000014, -5.000000000000024, -4.970000000000024, -4.61000000000002, -5.000000000000024, -4.160000000000014, -4.8200000000000225, -4.710000000000021, -4.420000000000018, -4.360000000000017, -3.680000000000009], "policy_agent_15_reward": [-5.140000000000025, -4.400000000000018, -4.830000000000022, -4.520000000000019, -5.040000000000025, -3.950000000000013, -4.2300000000000155, -4.700000000000021, -3.770000000000011, -3.970000000000013, -4.950000000000024, -4.110000000000015, -4.560000000000019, -4.470000000000018, -4.120000000000014, -4.300000000000017, -3.8200000000000114, -4.240000000000015, -4.520000000000019, -4.480000000000018, -4.700000000000021, -4.030000000000014], "policy_agent_16_reward": [-4.950000000000024, -5.150000000000026, -4.950000000000024, -4.140000000000015, -5.220000000000026, -4.180000000000016, -5.280000000000027, -4.750000000000021, -5.140000000000026, -4.530000000000019, -4.410000000000018, -4.720000000000021, -4.800000000000023, -4.120000000000014, -4.63000000000002, -4.67000000000002, -5.100000000000025, -4.470000000000018, -4.700000000000021, -4.240000000000016, -4.450000000000019, -4.440000000000018], "policy_agent_17_reward": [-4.780000000000021, -4.790000000000022, -3.9800000000000133, -4.3800000000000185, -4.340000000000017, -4.750000000000021, -3.8100000000000116, -4.54000000000002, -5.190000000000026, -5.100000000000025, -4.140000000000015, -4.140000000000015, -4.260000000000016, -4.250000000000016, -5.340000000000028, -4.330000000000017, -4.950000000000024, -4.61000000000002, -5.3700000000000285, -4.490000000000019, -4.140000000000015, -4.730000000000022], "policy_agent_18_reward": [-5.280000000000027, -4.150000000000015, -3.9000000000000123, -5.290000000000028, -4.800000000000022, -5.180000000000027, -3.5300000000000082, -3.780000000000011, -4.720000000000021, -4.720000000000021, -4.460000000000019, -5.290000000000028, -4.770000000000022, -4.990000000000024, -4.60000000000002, -4.550000000000019, -4.430000000000018, -3.5700000000000087, -4.990000000000024, -3.5700000000000074, -4.290000000000017, -5.150000000000026], "policy_agent_19_reward": [-3.9800000000000133, -4.020000000000014, -5.090000000000026, -3.75000000000001, -4.370000000000019, -4.60000000000002, -5.110000000000025, -4.9400000000000235, -4.560000000000019, -4.220000000000016, -4.0700000000000145, -5.040000000000025, -3.8100000000000116, -4.5400000000000205, -3.9600000000000133, -4.130000000000014, -3.8300000000000116, -4.530000000000019, -4.170000000000015, -4.760000000000022, -4.410000000000018, -4.030000000000014], "policy_agent_20_reward": [-3.7300000000000106, -4.9000000000000234, -4.2300000000000155, -4.240000000000016, -3.6700000000000097, -4.660000000000021, -4.500000000000019, -4.350000000000017, -4.460000000000019, -4.570000000000021, -4.550000000000019, -5.090000000000026, -4.360000000000017, -4.100000000000014, -4.100000000000015, -4.020000000000013, -4.090000000000014, -4.690000000000021, -5.050000000000025, -5.330000000000028, -4.810000000000022, -5.260000000000027]}, "sampler_perf": {"mean_raw_obs_processing_ms": 1.9263638091066455, "mean_inference_ms": 9.137880193382811, "mean_action_processing_ms": 0.7304238383539988, "mean_env_wait_ms": 2.7690390566560086, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"ObsPreprocessorConnector_ms": 0.002632770703468488, "StateBufferConnector_ms": 0.001460236388367492, "ViewRequirementAgentConnector_ms": 0.03273461288187927}, "num_episodes": 7, "episode_return_max": -43.9300000000001, "episode_return_min": -73.5400000000017, "episode_return_mean": -57.02727272727422, "episodes_this_iter": 7}, "num_healthy_workers": 3, "actor_manager_num_outstanding_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 252000, "num_agent_steps_trained": 252000, "num_env_steps_sampled": 12000, "num_env_steps_trained": 12000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 128.4654549873967, "num_env_steps_trained_throughput_per_sec": 128.4654549873967, "timesteps_total": 12000, "num_env_steps_sampled_lifetime": 12000, "num_agent_steps_sampled_lifetime": 252000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 252000, "timers": {"training_iteration_time_ms": 30245.831, "restore_workers_time_ms": 0.014, "training_step_time_ms": 30245.786, "sample_time_ms": 20129.722, "learn_time_ms": 10094.361, "learn_throughput": 396.261, "synch_weights_time_ms": 19.597}, "counters": {"num_env_steps_sampled": 12000, "num_env_steps_trained": 12000, "num_agent_steps_sampled": 252000, "num_agent_steps_trained": 252000}, "done": false, "training_iteration": 3, "trial_id": "cfc88_00000", "date": "2025-10-21_11-19-27", "timestamp": 1761038367, "time_this_iter_s": 31.150197744369507, "time_total_s": 90.77329969406128, "pid": 3279004, "hostname": "xuezhi-Precision-3660", "node_ip": "130.238.16.41", "config": {"exploration_config": {"type": "StochasticSampling"}, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "placement_strategy": "PACK", "num_gpus": 1, "_fake_gpus": false, "num_cpus_for_main_process": 1, "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "torch_ddp_kwargs": {}, "torch_skip_nan_gradients": false, "env": "<class 'train_utils.WarehouseMultiAgentEnv'>", "env_config": {"env_id": "tarware-extralarge-14agvs-7pickers-partialobs-chg-v1"}, "observation_space": null, "action_space": null, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "_is_atari": null, "disable_env_checking": false, "render_env": true, "action_mask_key": "action_mask", "env_runner_cls": null, "num_env_runners": 3, "create_local_env_runner": true, "num_envs_per_env_runner": 1, "gym_env_vectorize_mode": "SYNC", "num_cpus_per_env_runner": 1, "num_gpus_per_env_runner": 0, "custom_resources_per_env_runner": {}, "validate_env_runners_after_construction": true, "episodes_to_numpy": true, "max_requests_in_flight_per_env_runner": 1, "sample_timeout_s": 60.0, "_env_to_module_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "_module_to_env_connector": null, "add_default_connectors_to_module_to_env_pipeline": true, "merge_env_runner_states": "training_only", "broadcast_env_runner_states": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "compress_observations": false, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "sampler_perf_stats_ema_coef": null, "_is_online": true, "num_learners": 0, "num_gpus_per_learner": 1, "num_cpus_per_learner": "auto", "num_aggregator_actors_per_learner": 0, "max_requests_in_flight_per_aggregator_actor": 3, "local_gpu_idx": 0, "max_requests_in_flight_per_learner": 3, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "_train_batch_size_per_learner": null, "train_batch_size": 4000, "num_epochs": 10, "minibatch_size": 256, "shuffle_batch_per_epoch": true, "model": {"fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "log_std_clip_param": 20.0, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1, "_disable_preprocessor_api": false, "_disable_action_flattening": false}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "learner_config_dict": {}, "optimizer": {}, "_learner_class": null, "callbacks_on_algorithm_init": null, "callbacks_on_env_runners_recreated": null, "callbacks_on_offline_eval_runners_recreated": null, "callbacks_on_checkpoint_loaded": null, "callbacks_on_environment_created": null, "callbacks_on_episode_created": null, "callbacks_on_episode_start": null, "callbacks_on_episode_step": null, "callbacks_on_episode_end": null, "callbacks_on_evaluate_start": null, "callbacks_on_evaluate_end": null, "callbacks_on_evaluate_offline_start": null, "callbacks_on_evaluate_offline_end": null, "callbacks_on_sample_end": null, "callbacks_on_train_result": null, "explore": true, "enable_rl_module_and_learner": false, "enable_env_runner_and_connector_v2": false, "_prior_exploration_config": null, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function train.<locals>.<lambda> at 0x75979fd64f70>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "offline_data_class": null, "input_read_method": "read_parquet", "input_read_method_kwargs": {}, "input_read_schema": {}, "input_read_episodes": false, "input_read_sample_batches": false, "input_read_batch_size": null, "input_filesystem": null, "input_filesystem_kwargs": {}, "input_compress_columns": ["obs", "new_obs"], "input_spaces_jsonable": true, "materialize_data": false, "materialize_mapped_data": true, "map_batches_kwargs": {}, "iter_batches_kwargs": {}, "ignore_final_observation": false, "prelearner_class": null, "prelearner_buffer_class": null, "prelearner_buffer_kwargs": {}, "prelearner_module_synch_period": 10, "dataset_num_iters_per_learner": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "output_max_rows_per_file": null, "output_write_remaining_data": false, "output_write_method": "write_parquet", "output_write_method_kwargs": {}, "output_filesystem": null, "output_filesystem_kwargs": {}, "output_write_episodes": true, "offline_sampling": false, "evaluation_interval": 10, "evaluation_duration": 5, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 120.0, "evaluation_auto_duration_min_env_steps_per_sample": 100, "evaluation_auto_duration_max_env_steps_per_sample": 2000, "evaluation_parallel_to_training": false, "evaluation_force_reset_envs_before_iteration": true, "evaluation_config": {"explore": false}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_env_runners": 0, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 10.0, "offline_evaluation_interval": null, "num_offline_eval_runners": 0, "offline_evaluation_type": null, "offline_eval_runner_class": null, "offline_loss_for_module_fn": null, "offline_evaluation_duration": 1, "offline_evaluation_parallel_to_training": false, "offline_evaluation_timeout_s": 120.0, "num_cpus_per_offline_eval_runner": 1, "num_gpus_per_offline_eval_runner": 0, "custom_resources_per_offline_eval_runner": {}, "restart_failed_offline_eval_runners": true, "ignore_offline_eval_runner_failures": false, "max_num_offline_eval_runner_restarts": 1000, "offline_eval_runner_restore_timeout_s": 1800.0, "max_requests_in_flight_per_offline_eval_runner": 1, "validate_offline_eval_runners_after_construction": true, "offline_eval_runner_health_probe_timeout_s": 30.0, "offline_eval_rl_module_inference_only": false, "broadcast_offline_eval_runner_states": false, "offline_eval_batch_size_per_runner": 256, "dataset_num_iters_per_eval_runner": 1, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "log_gradients": false, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "restart_failed_env_runners": true, "ignore_env_runner_failures": false, "max_num_env_runner_restarts": 1000, "delay_between_env_runner_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_env_runner_failures_tolerance": 100, "env_runner_health_probe_timeout_s": 30.0, "env_runner_restore_timeout_s": 1800.0, "_model_config": {}, "_rl_module_spec": null, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "_validate_config": true, "_use_msgpack_checkpoints": false, "_torch_grad_scaler_class": null, "_torch_lr_scheduler_classes": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "_dont_auto_sync_env_runner_states": false, "env_task_fn": -1, "enable_connectors": -1, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "enable_async_evaluation": -1, "custom_async_evaluation_function": -1, "_enable_rl_module_api": -1, "auto_wrap_old_gym_envs": -1, "always_attach_evaluation_results": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "clip_param": 0.2, "vf_clip_param": 10.0, "entropy_coeff_schedule": null, "lr_schedule": null, "sgd_minibatch_size": -1, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 0.95, "input": "sampler", "policies": {"agent_0": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_1": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_2": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_3": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_4": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_5": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_6": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_7": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_8": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_9": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_10": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_11": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_12": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_13": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_14": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_15": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_16": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_17": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_18": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_19": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_20": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}]}, "callbacks": "<class 'ray.rllib.callbacks.callbacks.RLlibCallback'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch"}, "time_since_restore": 90.77329969406128, "iterations_since_restore": 3, "perf": {"cpu_util_percent": 9.232558139534886, "ram_util_percent": 27.93720930232558, "gpu_util_percent0": 0.19116279069767445, "vram_util_percent0": 0.0969654968700651}}
{"custom_metrics": {}, "episode_media": {}, "info": {"learner": {"agent_4": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.15456650108099, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03448364512732951, "policy_loss": -0.0551377671501541, "vf_loss": 0.01790279354027007, "vf_explained_var": 0.9597514927387237, "kl": 0.009171095463758116, "entropy": 5.859853371977806, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_19": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.798859730362892, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.002711026660108473, "policy_loss": -0.05495727899542544, "vf_loss": 0.04856510476092808, "vf_explained_var": 0.9435168739408255, "kl": 0.012270492874906929, "entropy": 5.304979345202446, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.759795232117176, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.02296120955288643, "policy_loss": -0.0502765247336356, "vf_loss": 0.02430941546917893, "vf_explained_var": 0.9456363979727029, "kl": 0.010019663996974576, "entropy": 5.72955601811409, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_16": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 8.865844745934009, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.012230429593182634, "policy_loss": -0.04797597194119589, "vf_loss": 0.055830958217848094, "vf_explained_var": 0.9466551717370748, "kl": 0.014584810031629081, "entropy": 5.166791120171547, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_9": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.771043555438519, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03274969740450615, "policy_loss": -0.055751497123856096, "vf_loss": 0.020369486836716532, "vf_explained_var": 0.9195159293711186, "kl": 0.008774376822377875, "entropy": 5.83688805103302, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_5": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.301132518053055, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.02288387437874917, "policy_loss": -0.048920295585412533, "vf_loss": 0.02306422081310302, "vf_explained_var": 0.9322266533970833, "kl": 0.009907335697511722, "entropy": 5.762297061085701, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_7": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.687741780281067, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.02873310131690232, "policy_loss": -0.0560130306854262, "vf_loss": 0.024134182260604577, "vf_explained_var": 0.9302608519792557, "kl": 0.010485824690768397, "entropy": 5.798886296153069, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_14": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 7.496793313324451, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.016102941596182065, "policy_loss": -0.04712213594757486, "vf_loss": 0.059334834944456814, "vf_explained_var": 0.8630283586680889, "kl": 0.012967478072914674, "entropy": 5.266721498966217, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_18": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 8.164631550014018, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.016000589850591496, "policy_loss": -0.05051377944037085, "vf_loss": 0.062233011645730585, "vf_explained_var": 0.8755973089486361, "kl": 0.014271192068365884, "entropy": 5.381966027617454, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_12": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.514397576451302, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.026580656104488298, "policy_loss": -0.052963942963106095, "vf_loss": 0.023810037187649867, "vf_explained_var": 0.9058074660599231, "kl": 0.008577497861703226, "entropy": 5.86674635708332, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_11": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.073015482723713, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.001293799828272313, "policy_loss": -0.04756667892215773, "vf_loss": 0.04311947250389494, "vf_explained_var": 0.9045791506767273, "kl": 0.01051135339879083, "entropy": 5.78701191842556, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_13": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.143818517029286, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03086948553827824, "policy_loss": -0.052923242989345455, "vf_loss": 0.019273187054204756, "vf_explained_var": 0.9513990897685289, "kl": 0.009268565980710264, "entropy": 5.8493777215480804, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.3485170096158985, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.024657081186887808, "policy_loss": -0.04882458029605914, "vf_loss": 0.021595164624159224, "vf_explained_var": 0.9125660754740238, "kl": 0.008574452335776694, "entropy": 5.846438232064247, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_20": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.648689934611321, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.0014364159986143931, "policy_loss": -0.05121187759214081, "vf_loss": 0.04618918079067953, "vf_explained_var": 0.9312372583895921, "kl": 0.011954268882467206, "entropy": 5.435810551047325, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.905933880805969, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.027754509018268435, "policy_loss": -0.05073200735496357, "vf_loss": 0.020464992173947393, "vf_explained_var": 0.9127815067768097, "kl": 0.00837501944827177, "entropy": 5.841763985157013, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_10": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.110134544968605, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03186113139090594, "policy_loss": -0.05414785122848116, "vf_loss": 0.020028609308064917, "vf_explained_var": 0.8851782154291868, "kl": 0.007527034376314445, "entropy": 5.863912513852119, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.073477537930012, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.019991456060961355, "policy_loss": -0.05093548159929924, "vf_loss": 0.028088978893356396, "vf_explained_var": 0.939642782881856, "kl": 0.009516822299399253, "entropy": 5.788465195894242, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_15": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 8.020292769372464, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.016144668622291646, "policy_loss": -0.04605649961158633, "vf_loss": 0.05849886068608612, "vf_explained_var": 0.927883968129754, "kl": 0.01234102625835476, "entropy": 5.358794650435447, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_17": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 8.733315262198449, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.010563492854998913, "policy_loss": -0.05099067089613527, "vf_loss": 0.057401667267549784, "vf_explained_var": 0.9163140166550875, "kl": 0.01384165490115869, "entropy": 5.069643950462341, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_6": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.1927374571561815, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.02981048528163228, "policy_loss": -0.05594988670782186, "vf_loss": 0.023145672224927694, "vf_explained_var": 0.9213165428489447, "kl": 0.009979093700860791, "entropy": 5.820306959748268, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_8": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.1111243784427645, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.028485905915295005, "policy_loss": -0.05177203854545951, "vf_loss": 0.021054549267864785, "vf_explained_var": 0.8960204903036356, "kl": 0.007438609061089463, "entropy": 5.832919633388519, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 560.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}}, "num_env_steps_sampled": 16000, "num_env_steps_trained": 16000, "num_agent_steps_sampled": 336000, "num_agent_steps_trained": 336000}, "env_runners": {"episode_reward_max": -43.9300000000001, "episode_reward_min": -73.5400000000017, "episode_reward_mean": -57.61166666666822, "episode_len_mean": 500.0, "episode_media": {}, "episodes_timesteps_total": 15000, "policy_reward_min": {"agent_0": -5.280000000000027, "agent_1": -5.360000000000028, "agent_2": -5.300000000000027, "agent_3": -5.350000000000028, "agent_4": -5.340000000000028, "agent_5": -5.350000000000028, "agent_6": -5.330000000000028, "agent_7": -5.340000000000028, "agent_8": -5.300000000000027, "agent_9": -5.180000000000026, "agent_10": -5.350000000000028, "agent_11": -5.340000000000028, "agent_12": -5.380000000000028, "agent_13": -5.310000000000027, "agent_14": -5.240000000000027, "agent_15": -5.140000000000025, "agent_16": -5.280000000000027, "agent_17": -5.3700000000000285, "agent_18": -5.290000000000028, "agent_19": -5.340000000000028, "agent_20": -5.330000000000028}, "policy_reward_max": {"agent_0": -0.5000000000000003, "agent_1": -0.5000000000000003, "agent_2": -0.5000000000000003, "agent_3": -0.5000000000000003, "agent_4": -0.5000000000000003, "agent_5": -0.5000000000000003, "agent_6": -0.5000000000000003, "agent_7": -0.5000000000000003, "agent_8": -0.5000000000000003, "agent_9": -0.5000000000000003, "agent_10": -0.5000000000000003, "agent_11": -0.5000000000000003, "agent_12": -0.5000000000000003, "agent_13": -0.5000000000000003, "agent_14": -3.590000000000009, "agent_15": -3.770000000000011, "agent_16": -4.010000000000013, "agent_17": -3.6300000000000097, "agent_18": -3.5300000000000082, "agent_19": -3.7000000000000104, "agent_20": -3.6700000000000097}, "policy_reward_mean": {"agent_0": -1.8160000000000072, "agent_1": -1.3756666666666715, "agent_2": -2.4110000000000085, "agent_3": -2.947000000000013, "agent_4": -0.9300000000000025, "agent_5": -2.299333333333342, "agent_6": -2.0790000000000073, "agent_7": -1.7246666666666717, "agent_8": -1.7373333333333396, "agent_9": -0.9910000000000022, "agent_10": -2.336333333333342, "agent_11": -1.6303333333333396, "agent_12": -1.6996666666666724, "agent_13": -2.1683333333333423, "agent_14": -4.481000000000019, "agent_15": -4.416000000000017, "agent_16": -4.666000000000021, "agent_17": -4.497000000000018, "agent_18": -4.518666666666686, "agent_19": -4.426000000000019, "agent_20": -4.461333333333352}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-53.290000000000305, -55.12000000000127, -61.81000000000211, -47.65000000000047, -57.68000000000135, -56.200000000001694, -52.780000000000356, -57.85000000000114, -54.040000000000376, -64.27000000000298, -49.57000000000024, -73.5400000000017, -62.680000000002885, -60.280000000003, -52.53000000000038, -58.05000000000256, -43.9300000000001, -59.7700000000019, -61.3400000000021, -52.9100000000017, -56.880000000001324, -62.430000000003005, -67.53000000000264, -58.4000000000019, -63.640000000003184, -48.380000000000244, -59.750000000001855, -54.76000000000095, -48.56000000000037, -72.73000000000226], "episode_lengths": [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500], "policy_agent_0_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.070000000000025, -5.240000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.110000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -2.5999999999999983, -5.210000000000027, -0.5000000000000003, -5.070000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.280000000000027, -5.210000000000027], "policy_agent_1_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.5500000000000087, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -5.360000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.920000000000023, -0.5000000000000003, -5.360000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.780000000000022], "policy_agent_2_reward": [-5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -3.500000000000008, -4.840000000000023, -2.129999999999993, -2.149999999999986, -5.130000000000026, -2.429999999999996, -0.5000000000000003, -0.5000000000000003, -4.67000000000002, -0.5000000000000003, -0.5000000000000003, -4.830000000000022, -5.060000000000025, -0.5000000000000003, -4.520000000000019, -0.5000000000000003, -4.780000000000022, -5.070000000000025, -0.5000000000000003, -5.170000000000027], "policy_agent_3_reward": [-0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.050000000000025, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -5.160000000000026, -0.5000000000000003, -5.330000000000028, -5.240000000000027, -0.7300000000000005, -5.180000000000026, -4.590000000000019, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -5.190000000000026, -0.8900000000000007, -5.350000000000028, -5.320000000000028, -0.5000000000000003, -0.5000000000000003, -3.7500000000000107, -0.5000000000000003, -4.8200000000000225, -5.320000000000028, -4.660000000000021], "policy_agent_4_reward": [-0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.8100000000000116, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_5_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5000000000000003, -4.870000000000023, -5.350000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.589999999999998, -5.000000000000024, -2.930000000000002, -5.290000000000028, -0.5000000000000003, -5.090000000000026, -3.8100000000000116, -0.5000000000000003, -5.050000000000025], "policy_agent_6_reward": [-5.310000000000027, -0.5000000000000003, -0.5000000000000003, -4.430000000000018, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -4.930000000000024, -5.030000000000024, -0.5000000000000003, -2.429999999999996, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.720000000000021, -5.330000000000028, -0.5000000000000003, -3.5700000000000087, -0.5000000000000003, -0.5000000000000003, -2.2799999999999945], "policy_agent_7_reward": [-0.5000000000000003, -2.349999999999995, -5.220000000000026, -2.2499999999999942, -5.040000000000025, -2.75, -5.300000000000027, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.0000000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_8_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.040000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -4.54000000000002, -0.5000000000000003, -3.0900000000000034, -0.5000000000000003, -0.5000000000000003, -5.080000000000025, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -4.61000000000002, -0.5000000000000003, -0.5000000000000003, -3.390000000000007, -0.5000000000000003], "policy_agent_9_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.790000000000011, -2.8200000000000007, -0.5000000000000003, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_10_reward": [-4.690000000000021, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.120000000000026, -4.930000000000024, -5.350000000000028, -0.5000000000000003, -2.119999999999993, -0.5000000000000003, -4.920000000000023, -2.6199999999999983, -0.5000000000000003, -3.4700000000000077, -4.560000000000019, -4.910000000000023, -4.030000000000014, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -0.5000000000000003, -4.910000000000023], "policy_agent_11_reward": [-0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.3699999999999974, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -5.330000000000028, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -5.320000000000028, -0.5000000000000003, -0.5000000000000003], "policy_agent_12_reward": [-0.5000000000000003, -5.240000000000027, -5.090000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.170000000000027, -0.5000000000000003, -0.5000000000000003, -2.139999999999993, -0.5000000000000003, -0.5000000000000003, -4.550000000000019, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.3399999999999954, -0.5000000000000003, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -5.380000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027], "policy_agent_13_reward": [-0.5000000000000003, -5.220000000000026, -5.270000000000027, -0.5000000000000003, -0.9600000000000007, -3.2600000000000056, -5.110000000000025, -5.260000000000027, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -5.280000000000027, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.210000000000027, -4.920000000000023, -0.5000000000000003, -0.5000000000000003, -5.030000000000024, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_14_reward": [-4.680000000000021, -4.600000000000021, -4.260000000000016, -4.000000000000013, -4.690000000000021, -4.530000000000019, -5.240000000000027, -4.8600000000000225, -5.150000000000026, -5.220000000000026, -3.780000000000011, -4.090000000000014, -5.000000000000024, -4.970000000000024, -4.61000000000002, -5.000000000000024, -4.160000000000014, -4.8200000000000225, -4.710000000000021, -4.420000000000018, -4.360000000000017, -3.680000000000009, -3.590000000000009, -4.400000000000017, -4.350000000000017, -4.64000000000002, -4.57000000000002, -4.290000000000017, -3.850000000000012, -3.9100000000000126], "policy_agent_15_reward": [-5.140000000000025, -4.400000000000018, -4.830000000000022, -4.520000000000019, -5.040000000000025, -3.950000000000013, -4.2300000000000155, -4.700000000000021, -3.770000000000011, -3.970000000000013, -4.950000000000024, -4.110000000000015, -4.560000000000019, -4.470000000000018, -4.120000000000014, -4.300000000000017, -3.8200000000000114, -4.240000000000015, -4.520000000000019, -4.480000000000018, -4.700000000000021, -4.030000000000014, -4.260000000000016, -4.490000000000019, -5.110000000000025, -4.130000000000015, -3.970000000000013, -4.710000000000021, -4.000000000000013, -4.960000000000024], "policy_agent_16_reward": [-4.950000000000024, -5.150000000000026, -4.950000000000024, -4.140000000000015, -5.220000000000026, -4.180000000000016, -5.280000000000027, -4.750000000000021, -5.140000000000026, -4.530000000000019, -4.410000000000018, -4.720000000000021, -4.800000000000023, -4.120000000000014, -4.63000000000002, -4.67000000000002, -5.100000000000025, -4.470000000000018, -4.700000000000021, -4.240000000000016, -4.450000000000019, -4.440000000000018, -5.250000000000027, -5.180000000000026, -4.010000000000013, -4.250000000000016, -5.010000000000025, -4.590000000000019, -4.090000000000014, -4.560000000000019], "policy_agent_17_reward": [-4.780000000000021, -4.790000000000022, -3.9800000000000133, -4.3800000000000185, -4.340000000000017, -4.750000000000021, -3.8100000000000116, -4.54000000000002, -5.190000000000026, -5.100000000000025, -4.140000000000015, -4.140000000000015, -4.260000000000016, -4.250000000000016, -5.340000000000028, -4.330000000000017, -4.950000000000024, -4.61000000000002, -5.3700000000000285, -4.490000000000019, -4.140000000000015, -4.730000000000022, -4.520000000000019, -4.64000000000002, -4.210000000000016, -3.6300000000000097, -3.9000000000000123, -4.180000000000016, -4.990000000000023, -4.4300000000000175], "policy_agent_18_reward": [-5.280000000000027, -4.150000000000015, -3.9000000000000123, -5.290000000000028, -4.800000000000022, -5.180000000000027, -3.5300000000000082, -3.780000000000011, -4.720000000000021, -4.720000000000021, -4.460000000000019, -5.290000000000028, -4.770000000000022, -4.990000000000024, -4.60000000000002, -4.550000000000019, -4.430000000000018, -3.5700000000000087, -4.990000000000024, -3.5700000000000074, -4.290000000000017, -5.150000000000026, -4.7400000000000215, -4.400000000000018, -4.170000000000015, -4.330000000000017, -3.940000000000013, -4.760000000000022, -4.090000000000014, -5.120000000000027], "policy_agent_19_reward": [-3.9800000000000133, -4.020000000000014, -5.090000000000026, -3.75000000000001, -4.370000000000019, -4.60000000000002, -5.110000000000025, -4.9400000000000235, -4.560000000000019, -4.220000000000016, -4.0700000000000145, -5.040000000000025, -3.8100000000000116, -4.5400000000000205, -3.9600000000000133, -4.130000000000014, -3.8300000000000116, -4.530000000000019, -4.170000000000015, -4.760000000000022, -4.410000000000018, -4.030000000000014, -5.290000000000028, -4.450000000000019, -5.240000000000028, -4.030000000000014, -4.63000000000002, -3.7000000000000104, -4.180000000000016, -5.340000000000028], "policy_agent_20_reward": [-3.7300000000000106, -4.9000000000000234, -4.2300000000000155, -4.240000000000016, -3.6700000000000097, -4.660000000000021, -4.500000000000019, -4.350000000000017, -4.460000000000019, -4.570000000000021, -4.550000000000019, -5.090000000000026, -4.360000000000017, -4.100000000000014, -4.100000000000015, -4.020000000000013, -4.090000000000014, -4.690000000000021, -5.050000000000025, -5.330000000000028, -4.810000000000022, -5.260000000000027, -4.920000000000023, -4.2700000000000164, -3.9900000000000135, -4.170000000000015, -5.300000000000027, -4.510000000000019, -3.870000000000012, -4.050000000000014]}, "sampler_perf": {"mean_raw_obs_processing_ms": 1.943045085749278, "mean_inference_ms": 9.188192771473751, "mean_action_processing_ms": 0.7307386925550877, "mean_env_wait_ms": 2.789114230996408, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"ObsPreprocessorConnector_ms": 0.0026453487456790984, "StateBufferConnector_ms": 0.0014766435774545821, "ViewRequirementAgentConnector_ms": 0.03303251569233243}, "num_episodes": 8, "episode_return_max": -43.9300000000001, "episode_return_min": -73.5400000000017, "episode_return_mean": -57.61166666666822, "episodes_this_iter": 8}, "num_healthy_workers": 3, "actor_manager_num_outstanding_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 336000, "num_agent_steps_trained": 336000, "num_env_steps_sampled": 16000, "num_env_steps_trained": 16000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 130.08976031372993, "num_env_steps_trained_throughput_per_sec": 130.08976031372993, "timesteps_total": 16000, "num_env_steps_sampled_lifetime": 16000, "num_agent_steps_sampled_lifetime": 336000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 336000, "timers": {"training_iteration_time_ms": 30371.375, "restore_workers_time_ms": 0.014, "training_step_time_ms": 30371.33, "sample_time_ms": 20283.078, "learn_time_ms": 10064.887, "learn_throughput": 397.421, "synch_weights_time_ms": 21.103}, "counters": {"num_env_steps_sampled": 16000, "num_env_steps_trained": 16000, "num_agent_steps_sampled": 336000, "num_agent_steps_trained": 336000}, "done": false, "training_iteration": 4, "trial_id": "cfc88_00000", "date": "2025-10-21_11-19-58", "timestamp": 1761038398, "time_this_iter_s": 30.761434316635132, "time_total_s": 121.53473401069641, "pid": 3279004, "hostname": "xuezhi-Precision-3660", "node_ip": "130.238.16.41", "config": {"exploration_config": {"type": "StochasticSampling"}, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "placement_strategy": "PACK", "num_gpus": 1, "_fake_gpus": false, "num_cpus_for_main_process": 1, "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "torch_ddp_kwargs": {}, "torch_skip_nan_gradients": false, "env": "<class 'train_utils.WarehouseMultiAgentEnv'>", "env_config": {"env_id": "tarware-extralarge-14agvs-7pickers-partialobs-chg-v1"}, "observation_space": null, "action_space": null, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "_is_atari": null, "disable_env_checking": false, "render_env": true, "action_mask_key": "action_mask", "env_runner_cls": null, "num_env_runners": 3, "create_local_env_runner": true, "num_envs_per_env_runner": 1, "gym_env_vectorize_mode": "SYNC", "num_cpus_per_env_runner": 1, "num_gpus_per_env_runner": 0, "custom_resources_per_env_runner": {}, "validate_env_runners_after_construction": true, "episodes_to_numpy": true, "max_requests_in_flight_per_env_runner": 1, "sample_timeout_s": 60.0, "_env_to_module_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "_module_to_env_connector": null, "add_default_connectors_to_module_to_env_pipeline": true, "merge_env_runner_states": "training_only", "broadcast_env_runner_states": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "compress_observations": false, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "sampler_perf_stats_ema_coef": null, "_is_online": true, "num_learners": 0, "num_gpus_per_learner": 1, "num_cpus_per_learner": "auto", "num_aggregator_actors_per_learner": 0, "max_requests_in_flight_per_aggregator_actor": 3, "local_gpu_idx": 0, "max_requests_in_flight_per_learner": 3, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "_train_batch_size_per_learner": null, "train_batch_size": 4000, "num_epochs": 10, "minibatch_size": 256, "shuffle_batch_per_epoch": true, "model": {"fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "log_std_clip_param": 20.0, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1, "_disable_preprocessor_api": false, "_disable_action_flattening": false}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "learner_config_dict": {}, "optimizer": {}, "_learner_class": null, "callbacks_on_algorithm_init": null, "callbacks_on_env_runners_recreated": null, "callbacks_on_offline_eval_runners_recreated": null, "callbacks_on_checkpoint_loaded": null, "callbacks_on_environment_created": null, "callbacks_on_episode_created": null, "callbacks_on_episode_start": null, "callbacks_on_episode_step": null, "callbacks_on_episode_end": null, "callbacks_on_evaluate_start": null, "callbacks_on_evaluate_end": null, "callbacks_on_evaluate_offline_start": null, "callbacks_on_evaluate_offline_end": null, "callbacks_on_sample_end": null, "callbacks_on_train_result": null, "explore": true, "enable_rl_module_and_learner": false, "enable_env_runner_and_connector_v2": false, "_prior_exploration_config": null, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function train.<locals>.<lambda> at 0x75979fbe5120>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "offline_data_class": null, "input_read_method": "read_parquet", "input_read_method_kwargs": {}, "input_read_schema": {}, "input_read_episodes": false, "input_read_sample_batches": false, "input_read_batch_size": null, "input_filesystem": null, "input_filesystem_kwargs": {}, "input_compress_columns": ["obs", "new_obs"], "input_spaces_jsonable": true, "materialize_data": false, "materialize_mapped_data": true, "map_batches_kwargs": {}, "iter_batches_kwargs": {}, "ignore_final_observation": false, "prelearner_class": null, "prelearner_buffer_class": null, "prelearner_buffer_kwargs": {}, "prelearner_module_synch_period": 10, "dataset_num_iters_per_learner": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "output_max_rows_per_file": null, "output_write_remaining_data": false, "output_write_method": "write_parquet", "output_write_method_kwargs": {}, "output_filesystem": null, "output_filesystem_kwargs": {}, "output_write_episodes": true, "offline_sampling": false, "evaluation_interval": 10, "evaluation_duration": 5, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 120.0, "evaluation_auto_duration_min_env_steps_per_sample": 100, "evaluation_auto_duration_max_env_steps_per_sample": 2000, "evaluation_parallel_to_training": false, "evaluation_force_reset_envs_before_iteration": true, "evaluation_config": {"explore": false}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_env_runners": 0, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 10.0, "offline_evaluation_interval": null, "num_offline_eval_runners": 0, "offline_evaluation_type": null, "offline_eval_runner_class": null, "offline_loss_for_module_fn": null, "offline_evaluation_duration": 1, "offline_evaluation_parallel_to_training": false, "offline_evaluation_timeout_s": 120.0, "num_cpus_per_offline_eval_runner": 1, "num_gpus_per_offline_eval_runner": 0, "custom_resources_per_offline_eval_runner": {}, "restart_failed_offline_eval_runners": true, "ignore_offline_eval_runner_failures": false, "max_num_offline_eval_runner_restarts": 1000, "offline_eval_runner_restore_timeout_s": 1800.0, "max_requests_in_flight_per_offline_eval_runner": 1, "validate_offline_eval_runners_after_construction": true, "offline_eval_runner_health_probe_timeout_s": 30.0, "offline_eval_rl_module_inference_only": false, "broadcast_offline_eval_runner_states": false, "offline_eval_batch_size_per_runner": 256, "dataset_num_iters_per_eval_runner": 1, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "log_gradients": false, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "restart_failed_env_runners": true, "ignore_env_runner_failures": false, "max_num_env_runner_restarts": 1000, "delay_between_env_runner_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_env_runner_failures_tolerance": 100, "env_runner_health_probe_timeout_s": 30.0, "env_runner_restore_timeout_s": 1800.0, "_model_config": {}, "_rl_module_spec": null, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "_validate_config": true, "_use_msgpack_checkpoints": false, "_torch_grad_scaler_class": null, "_torch_lr_scheduler_classes": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "_dont_auto_sync_env_runner_states": false, "env_task_fn": -1, "enable_connectors": -1, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "enable_async_evaluation": -1, "custom_async_evaluation_function": -1, "_enable_rl_module_api": -1, "auto_wrap_old_gym_envs": -1, "always_attach_evaluation_results": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "clip_param": 0.2, "vf_clip_param": 10.0, "entropy_coeff_schedule": null, "lr_schedule": null, "sgd_minibatch_size": -1, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 0.95, "input": "sampler", "policies": {"agent_0": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_1": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_2": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_3": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_4": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_5": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_6": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_7": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_8": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_9": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_10": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_11": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_12": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_13": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_14": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_15": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_16": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_17": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_18": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_19": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_20": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}]}, "callbacks": "<class 'ray.rllib.callbacks.callbacks.RLlibCallback'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch"}, "time_since_restore": 121.53473401069641, "iterations_since_restore": 4, "perf": {"cpu_util_percent": 8.888372093023255, "ram_util_percent": 28.272093023255817, "gpu_util_percent0": 0.20697674418604653, "vram_util_percent0": 0.0969098283364197}}
{"custom_metrics": {}, "episode_media": {}, "info": {"learner": {"agent_4": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.334720656275749, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03511097685768618, "policy_loss": -0.055400011129677296, "vf_loss": 0.017545339567004704, "vf_explained_var": 0.9327935721725226, "kl": 0.009145649079040186, "entropy": 5.832341703772545, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_19": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 7.262740206718445, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.0047463269540457985, "policy_loss": -0.05389838790288195, "vf_loss": 0.05422363923862576, "vf_explained_var": 0.9336194578558207, "kl": 0.01473692018327295, "entropy": 5.284564271569252, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.0698251381516455, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.028623202827293425, "policy_loss": -0.051106180902570485, "vf_loss": 0.019731172604952007, "vf_explained_var": 0.9411729853600264, "kl": 0.009172682464905213, "entropy": 5.70892736017704, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_16": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 7.586954805254936, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.0064787944633280855, "policy_loss": -0.04501551002213091, "vf_loss": 0.047798139549558985, "vf_explained_var": 0.9048696421086788, "kl": 0.01232055201306224, "entropy": 5.137464001774788, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_9": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.6391036331653597, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.034411578763683795, "policy_loss": -0.05550408638082445, "vf_loss": 0.018505797575926408, "vf_explained_var": 0.9155962254852057, "kl": 0.008622366436588891, "entropy": 5.813632014393806, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_5": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.8850133553147317, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.031992594743496736, "policy_loss": -0.053295993632673344, "vf_loss": 0.01853142698237207, "vf_explained_var": 0.9410518310964108, "kl": 0.009239909038721072, "entropy": 5.735658869147301, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_7": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.293973486125469, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.027508555234817322, "policy_loss": -0.05376403831933203, "vf_loss": 0.02306344595272094, "vf_explained_var": 0.9557563491165638, "kl": 0.010640118205057747, "entropy": 5.762755790352822, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_14": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.290681372582912, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 5.2462110761553046e-05, "policy_loss": -0.04979960373602808, "vf_loss": 0.046448787540430206, "vf_explained_var": 0.9101330872625113, "kl": 0.011344257802508414, "entropy": 5.37441845536232, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_18": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 9.03291534334421, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.006231955705152359, "policy_loss": -0.050931353284977374, "vf_loss": 0.05334206213010475, "vf_explained_var": 0.9441083949059248, "kl": 0.01273749050731563, "entropy": 5.3802505165338514, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_12": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.734047719836235, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03032596278571873, "policy_loss": -0.05392744921846315, "vf_loss": 0.02117975261353422, "vf_explained_var": 0.9252030525356532, "kl": 0.008072445517787519, "entropy": 5.857726457715034, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_11": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.572587262094021, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.022604556341684658, "policy_loss": -0.05135152234288398, "vf_loss": 0.025916734314523637, "vf_explained_var": 0.934810696542263, "kl": 0.009434106465926795, "entropy": 5.820765948295593, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_13": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.6452993407845495, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03889970050295233, "policy_loss": -0.056487780218594706, "vf_loss": 0.014953949511982501, "vf_explained_var": 0.9081567738205194, "kl": 0.00878043298878332, "entropy": 5.822018238902092, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.850695939362049, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.029046317300526425, "policy_loss": -0.04981150081148371, "vf_loss": 0.018176388170104474, "vf_explained_var": 0.9446244966238737, "kl": 0.008629318489565562, "entropy": 5.828490853309631, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_20": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.948482555150986, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.011951992900867481, "policy_loss": -0.05114375056291465, "vf_loss": 0.03587637675227597, "vf_explained_var": 0.9382223904132843, "kl": 0.0110512673216288, "entropy": 5.332267567515373, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.111078849434852, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03339975207927637, "policy_loss": -0.054232311545638366, "vf_loss": 0.018084185585030353, "vf_explained_var": 0.8769477106630802, "kl": 0.009161246557186028, "entropy": 5.76608319580555, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_10": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.122711093723774, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.0257695985987084, "policy_loss": -0.05162687941337936, "vf_loss": 0.023225660290336236, "vf_explained_var": 0.9176498547196388, "kl": 0.008772070130573902, "entropy": 5.837185573577881, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.577794441580773, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.0257456757244654, "policy_loss": -0.05382369188009761, "vf_loss": 0.024927395940176213, "vf_explained_var": 0.9300933625549078, "kl": 0.010502068100149026, "entropy": 5.742383334040642, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_15": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 8.401453004777432, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.00415894243051298, "policy_loss": -0.047395322337979454, "vf_loss": 0.04771951496950351, "vf_explained_var": 0.9279862321913243, "kl": 0.01278249588849576, "entropy": 5.367946660518646, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_17": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 8.617715454101562, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.00029762734193354844, "policy_loss": -0.05018006303725997, "vf_loss": 0.047066642314894123, "vf_explained_var": 0.9244348779320717, "kl": 0.011370159998800112, "entropy": 5.190148478746414, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_6": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.604979376494884, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03632399459311273, "policy_loss": -0.05829184523399818, "vf_loss": 0.019093151376000605, "vf_explained_var": 0.9419490184634924, "kl": 0.009582331614807283, "entropy": 5.819369640946388, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_8": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.761190912127495, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03219548014894826, "policy_loss": -0.05214392443886027, "vf_loss": 0.017359393424703738, "vf_explained_var": 0.9335091128945351, "kl": 0.00863017097531773, "entropy": 5.812810334563255, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}}, "num_env_steps_sampled": 20000, "num_env_steps_trained": 20000, "num_agent_steps_sampled": 420000, "num_agent_steps_trained": 420000}, "env_runners": {"episode_reward_max": -43.9300000000001, "episode_reward_min": -75.40000000000191, "episode_reward_mean": -57.90000000000151, "episode_len_mean": 500.0, "episode_media": {}, "episodes_timesteps_total": 19500, "policy_reward_min": {"agent_0": -5.280000000000027, "agent_1": -5.360000000000028, "agent_2": -5.330000000000028, "agent_3": -5.350000000000028, "agent_4": -5.340000000000028, "agent_5": -5.350000000000028, "agent_6": -5.330000000000028, "agent_7": -5.340000000000028, "agent_8": -5.300000000000027, "agent_9": -5.190000000000026, "agent_10": -5.350000000000028, "agent_11": -5.340000000000028, "agent_12": -5.380000000000028, "agent_13": -5.310000000000027, "agent_14": -5.240000000000027, "agent_15": -5.290000000000028, "agent_16": -5.300000000000027, "agent_17": -5.3700000000000285, "agent_18": -5.290000000000028, "agent_19": -5.340000000000028, "agent_20": -5.330000000000028}, "policy_reward_max": {"agent_0": -0.5000000000000003, "agent_1": -0.5000000000000003, "agent_2": -0.5000000000000003, "agent_3": -0.5000000000000003, "agent_4": -0.5000000000000003, "agent_5": -0.5000000000000003, "agent_6": -0.5000000000000003, "agent_7": -0.5000000000000003, "agent_8": -0.5000000000000003, "agent_9": -0.5000000000000003, "agent_10": -0.5000000000000003, "agent_11": -0.5000000000000003, "agent_12": -0.5000000000000003, "agent_13": -0.5000000000000003, "agent_14": -3.590000000000009, "agent_15": -3.770000000000011, "agent_16": -3.790000000000011, "agent_17": -3.6300000000000097, "agent_18": -3.5300000000000082, "agent_19": -3.6300000000000106, "agent_20": -3.6700000000000097}, "policy_reward_mean": {"agent_0": -1.8053846153846222, "agent_1": -1.3948717948717997, "agent_2": -2.329230769230777, "agent_3": -2.737179487179499, "agent_4": -0.9525641025641053, "agent_5": -2.4471794871794974, "agent_6": -1.9597435897435969, "agent_7": -1.926923076923083, "agent_8": -1.5597435897435952, "agent_9": -1.092820512820516, "agent_10": -2.506923076923087, "agent_11": -2.064358974358983, "agent_12": -1.656666666666673, "agent_13": -1.9300000000000073, "agent_14": -4.479743589743608, "agent_15": -4.4720512820513, "agent_16": -4.670512820512841, "agent_17": -4.521538461538481, "agent_18": -4.501282051282071, "agent_19": -4.4307692307692506, "agent_20": -4.460512820512839}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-53.290000000000305, -55.12000000000127, -61.81000000000211, -47.65000000000047, -57.68000000000135, -56.200000000001694, -52.780000000000356, -57.85000000000114, -54.040000000000376, -64.27000000000298, -49.57000000000024, -73.5400000000017, -62.680000000002885, -60.280000000003, -52.53000000000038, -58.05000000000256, -43.9300000000001, -59.7700000000019, -61.3400000000021, -52.9100000000017, -56.880000000001324, -62.430000000003005, -67.53000000000264, -58.4000000000019, -63.640000000003184, -48.380000000000244, -59.750000000001855, -54.76000000000095, -48.56000000000037, -72.73000000000226, -61.48000000000291, -54.10000000000039, -75.40000000000191, -57.9400000000012, -67.91000000000251, -57.94000000000153, -47.429999999999914, -52.13000000000102, -55.420000000001096], "episode_lengths": [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500], "policy_agent_0_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.070000000000025, -5.240000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.110000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -2.5999999999999983, -5.210000000000027, -0.5000000000000003, -5.070000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.280000000000027, -5.210000000000027, -5.020000000000024, -0.5000000000000003, -5.090000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.8200000000000007], "policy_agent_1_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.5500000000000087, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -5.360000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.920000000000023, -0.5000000000000003, -5.360000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.780000000000022, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.480000000000018], "policy_agent_2_reward": [-5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -3.500000000000008, -4.840000000000023, -2.129999999999993, -2.149999999999986, -5.130000000000026, -2.429999999999996, -0.5000000000000003, -0.5000000000000003, -4.67000000000002, -0.5000000000000003, -0.5000000000000003, -4.830000000000022, -5.060000000000025, -0.5000000000000003, -4.520000000000019, -0.5000000000000003, -4.780000000000022, -5.070000000000025, -0.5000000000000003, -5.170000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.330000000000028, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.160000000000026], "policy_agent_3_reward": [-0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.050000000000025, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -5.160000000000026, -0.5000000000000003, -5.330000000000028, -5.240000000000027, -0.7300000000000005, -5.180000000000026, -4.590000000000019, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -5.190000000000026, -0.8900000000000007, -5.350000000000028, -5.320000000000028, -0.5000000000000003, -0.5000000000000003, -3.7500000000000107, -0.5000000000000003, -4.8200000000000225, -5.320000000000028, -4.660000000000021, -5.080000000000025, -0.5000000000000003, -0.5000000000000003, -5.010000000000025, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_4_reward": [-0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.8100000000000116, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_5_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5000000000000003, -4.870000000000023, -5.350000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.589999999999998, -5.000000000000024, -2.930000000000002, -5.290000000000028, -0.5000000000000003, -5.090000000000026, -3.8100000000000116, -0.5000000000000003, -5.050000000000025, -5.300000000000027, -5.150000000000026, -3.6800000000000175, -5.110000000000025, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_6_reward": [-5.310000000000027, -0.5000000000000003, -0.5000000000000003, -4.430000000000018, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -4.930000000000024, -5.030000000000024, -0.5000000000000003, -2.429999999999996, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.720000000000021, -5.330000000000028, -0.5000000000000003, -3.5700000000000087, -0.5000000000000003, -0.5000000000000003, -2.2799999999999945, -5.310000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_7_reward": [-0.5000000000000003, -2.349999999999995, -5.220000000000026, -2.2499999999999942, -5.040000000000025, -2.75, -5.300000000000027, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.0000000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.8999999999999926, -5.110000000000025, -5.330000000000028, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.890000000000012, -0.5000000000000003], "policy_agent_8_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.040000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -4.54000000000002, -0.5000000000000003, -3.0900000000000034, -0.5000000000000003, -0.5000000000000003, -5.080000000000025, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -4.61000000000002, -0.5000000000000003, -0.5000000000000003, -3.390000000000007, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.710000000000021, -0.5000000000000003], "policy_agent_9_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.790000000000011, -2.8200000000000007, -0.5000000000000003, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5600000000000004, -0.5000000000000003, -0.5000000000000003, -4.140000000000015, -0.5000000000000003], "policy_agent_10_reward": [-4.690000000000021, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.120000000000026, -4.930000000000024, -5.350000000000028, -0.5000000000000003, -2.119999999999993, -0.5000000000000003, -4.920000000000023, -2.6199999999999983, -0.5000000000000003, -3.4700000000000077, -4.560000000000019, -4.910000000000023, -4.030000000000014, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -0.5000000000000003, -4.910000000000023, -0.9700000000000008, -0.5000000000000003, -5.050000000000025, -0.5000000000000003, -5.310000000000027, -4.850000000000023, -0.5000000000000003, -5.030000000000024, -4.970000000000024], "policy_agent_11_reward": [-0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.3699999999999974, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -5.330000000000028, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -5.320000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.180000000000026, -5.230000000000027, -5.220000000000026, -4.380000000000018, -5.290000000000028, -4.800000000000022, -0.5000000000000003, -0.5000000000000003], "policy_agent_12_reward": [-0.5000000000000003, -5.240000000000027, -5.090000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.170000000000027, -0.5000000000000003, -0.5000000000000003, -2.139999999999993, -0.5000000000000003, -0.5000000000000003, -4.550000000000019, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.3399999999999954, -0.5000000000000003, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -5.380000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -4.870000000000023, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_13_reward": [-0.5000000000000003, -5.220000000000026, -5.270000000000027, -0.5000000000000003, -0.9600000000000007, -3.2600000000000056, -5.110000000000025, -5.260000000000027, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -5.280000000000027, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.210000000000027, -4.920000000000023, -0.5000000000000003, -0.5000000000000003, -5.030000000000024, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.210000000000027, -0.5000000000000003, -0.5000000000000003, -1.2699999999999982, -0.5000000000000003, -0.7400000000000005, -0.5000000000000003], "policy_agent_14_reward": [-4.680000000000021, -4.600000000000021, -4.260000000000016, -4.000000000000013, -4.690000000000021, -4.530000000000019, -5.240000000000027, -4.8600000000000225, -5.150000000000026, -5.220000000000026, -3.780000000000011, -4.090000000000014, -5.000000000000024, -4.970000000000024, -4.61000000000002, -5.000000000000024, -4.160000000000014, -4.8200000000000225, -4.710000000000021, -4.420000000000018, -4.360000000000017, -3.680000000000009, -3.590000000000009, -4.400000000000017, -4.350000000000017, -4.64000000000002, -4.57000000000002, -4.290000000000017, -3.850000000000012, -3.9100000000000126, -4.180000000000017, -4.980000000000024, -4.200000000000015, -3.950000000000013, -4.870000000000023, -3.66000000000001, -5.130000000000026, -4.090000000000015, -5.220000000000026], "policy_agent_15_reward": [-5.140000000000025, -4.400000000000018, -4.830000000000022, -4.520000000000019, -5.040000000000025, -3.950000000000013, -4.2300000000000155, -4.700000000000021, -3.770000000000011, -3.970000000000013, -4.950000000000024, -4.110000000000015, -4.560000000000019, -4.470000000000018, -4.120000000000014, -4.300000000000017, -3.8200000000000114, -4.240000000000015, -4.520000000000019, -4.480000000000018, -4.700000000000021, -4.030000000000014, -4.260000000000016, -4.490000000000019, -5.110000000000025, -4.130000000000015, -3.970000000000013, -4.710000000000021, -4.000000000000013, -4.960000000000024, -4.0700000000000145, -4.210000000000016, -4.6200000000000205, -5.040000000000025, -4.510000000000019, -4.410000000000018, -5.200000000000027, -5.290000000000028, -4.58000000000002], "policy_agent_16_reward": [-4.950000000000024, -5.150000000000026, -4.950000000000024, -4.140000000000015, -5.220000000000026, -4.180000000000016, -5.280000000000027, -4.750000000000021, -5.140000000000026, -4.530000000000019, -4.410000000000018, -4.720000000000021, -4.800000000000023, -4.120000000000014, -4.63000000000002, -4.67000000000002, -5.100000000000025, -4.470000000000018, -4.700000000000021, -4.240000000000016, -4.450000000000019, -4.440000000000018, -5.250000000000027, -5.180000000000026, -4.010000000000013, -4.250000000000016, -5.010000000000025, -4.590000000000019, -4.090000000000014, -4.560000000000019, -3.790000000000011, -5.230000000000027, -4.080000000000014, -4.500000000000019, -5.080000000000025, -5.020000000000024, -5.050000000000025, -4.120000000000014, -5.300000000000027], "policy_agent_17_reward": [-4.780000000000021, -4.790000000000022, -3.9800000000000133, -4.3800000000000185, -4.340000000000017, -4.750000000000021, -3.8100000000000116, -4.54000000000002, -5.190000000000026, -5.100000000000025, -4.140000000000015, -4.140000000000015, -4.260000000000016, -4.250000000000016, -5.340000000000028, -4.330000000000017, -4.950000000000024, -4.61000000000002, -5.3700000000000285, -4.490000000000019, -4.140000000000015, -4.730000000000022, -4.520000000000019, -4.64000000000002, -4.210000000000016, -3.6300000000000097, -3.9000000000000123, -4.180000000000016, -4.990000000000023, -4.4300000000000175, -5.220000000000027, -3.970000000000013, -5.090000000000026, -4.2700000000000164, -4.650000000000021, -5.040000000000025, -4.200000000000015, -3.760000000000011, -5.230000000000027], "policy_agent_18_reward": [-5.280000000000027, -4.150000000000015, -3.9000000000000123, -5.290000000000028, -4.800000000000022, -5.180000000000027, -3.5300000000000082, -3.780000000000011, -4.720000000000021, -4.720000000000021, -4.460000000000019, -5.290000000000028, -4.770000000000022, -4.990000000000024, -4.60000000000002, -4.550000000000019, -4.430000000000018, -3.5700000000000087, -4.990000000000024, -3.5700000000000074, -4.290000000000017, -5.150000000000026, -4.7400000000000215, -4.400000000000018, -4.170000000000015, -4.330000000000017, -3.940000000000013, -4.760000000000022, -4.090000000000014, -5.120000000000027, -3.9000000000000123, -5.100000000000025, -5.220000000000026, -4.480000000000018, -4.0700000000000145, -4.3100000000000165, -3.880000000000013, -4.120000000000014, -4.910000000000023], "policy_agent_19_reward": [-3.9800000000000133, -4.020000000000014, -5.090000000000026, -3.75000000000001, -4.370000000000019, -4.60000000000002, -5.110000000000025, -4.9400000000000235, -4.560000000000019, -4.220000000000016, -4.0700000000000145, -5.040000000000025, -3.8100000000000116, -4.5400000000000205, -3.9600000000000133, -4.130000000000014, -3.8300000000000116, -4.530000000000019, -4.170000000000015, -4.760000000000022, -4.410000000000018, -4.030000000000014, -5.290000000000028, -4.450000000000019, -5.240000000000028, -4.030000000000014, -4.63000000000002, -3.7000000000000104, -4.180000000000016, -5.340000000000028, -4.200000000000015, -4.870000000000023, -5.270000000000027, -4.960000000000024, -4.610000000000021, -4.710000000000022, -3.790000000000011, -3.6300000000000106, -3.9800000000000133], "policy_agent_20_reward": [-3.7300000000000106, -4.9000000000000234, -4.2300000000000155, -4.240000000000016, -3.6700000000000097, -4.660000000000021, -4.500000000000019, -4.350000000000017, -4.460000000000019, -4.570000000000021, -4.550000000000019, -5.090000000000026, -4.360000000000017, -4.100000000000014, -4.100000000000015, -4.020000000000013, -4.090000000000014, -4.690000000000021, -5.050000000000025, -5.330000000000028, -4.810000000000022, -5.260000000000027, -4.920000000000023, -4.2700000000000164, -3.9900000000000135, -4.170000000000015, -5.300000000000027, -4.510000000000019, -3.870000000000012, -4.050000000000014, -3.8900000000000134, -4.800000000000022, -4.2700000000000164, -5.220000000000027, -5.320000000000028, -4.380000000000018, -4.360000000000017, -4.1100000000000145, -3.770000000000011]}, "sampler_perf": {"mean_raw_obs_processing_ms": 1.9526583850932855, "mean_inference_ms": 9.213385622246486, "mean_action_processing_ms": 0.7287989037704158, "mean_env_wait_ms": 2.7974940983164234, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"ObsPreprocessorConnector_ms": 0.002365729456565028, "StateBufferConnector_ms": 0.0014746727669777597, "ViewRequirementAgentConnector_ms": 0.03302894960247408}, "num_episodes": 9, "episode_return_max": -43.9300000000001, "episode_return_min": -75.40000000000191, "episode_return_mean": -57.90000000000151, "episodes_this_iter": 9}, "num_healthy_workers": 3, "actor_manager_num_outstanding_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 420000, "num_agent_steps_trained": 420000, "num_env_steps_sampled": 20000, "num_env_steps_trained": 20000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 134.35525834180694, "num_env_steps_trained_throughput_per_sec": 134.35525834180694, "timesteps_total": 20000, "num_env_steps_sampled_lifetime": 20000, "num_agent_steps_sampled_lifetime": 420000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 420000, "timers": {"training_iteration_time_ms": 30251.465, "restore_workers_time_ms": 0.015, "training_step_time_ms": 30251.419, "sample_time_ms": 20178.24, "learn_time_ms": 10049.584, "learn_throughput": 398.026, "synch_weights_time_ms": 21.568}, "counters": {"num_env_steps_sampled": 20000, "num_env_steps_trained": 20000, "num_agent_steps_sampled": 420000, "num_agent_steps_trained": 420000}, "done": false, "training_iteration": 5, "trial_id": "cfc88_00000", "date": "2025-10-21_11-20-28", "timestamp": 1761038428, "time_this_iter_s": 29.784170150756836, "time_total_s": 151.31890416145325, "pid": 3279004, "hostname": "xuezhi-Precision-3660", "node_ip": "130.238.16.41", "config": {"exploration_config": {"type": "StochasticSampling"}, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "placement_strategy": "PACK", "num_gpus": 1, "_fake_gpus": false, "num_cpus_for_main_process": 1, "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "torch_ddp_kwargs": {}, "torch_skip_nan_gradients": false, "env": "<class 'train_utils.WarehouseMultiAgentEnv'>", "env_config": {"env_id": "tarware-extralarge-14agvs-7pickers-partialobs-chg-v1"}, "observation_space": null, "action_space": null, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "_is_atari": null, "disable_env_checking": false, "render_env": true, "action_mask_key": "action_mask", "env_runner_cls": null, "num_env_runners": 3, "create_local_env_runner": true, "num_envs_per_env_runner": 1, "gym_env_vectorize_mode": "SYNC", "num_cpus_per_env_runner": 1, "num_gpus_per_env_runner": 0, "custom_resources_per_env_runner": {}, "validate_env_runners_after_construction": true, "episodes_to_numpy": true, "max_requests_in_flight_per_env_runner": 1, "sample_timeout_s": 60.0, "_env_to_module_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "_module_to_env_connector": null, "add_default_connectors_to_module_to_env_pipeline": true, "merge_env_runner_states": "training_only", "broadcast_env_runner_states": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "compress_observations": false, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "sampler_perf_stats_ema_coef": null, "_is_online": true, "num_learners": 0, "num_gpus_per_learner": 1, "num_cpus_per_learner": "auto", "num_aggregator_actors_per_learner": 0, "max_requests_in_flight_per_aggregator_actor": 3, "local_gpu_idx": 0, "max_requests_in_flight_per_learner": 3, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "_train_batch_size_per_learner": null, "train_batch_size": 4000, "num_epochs": 10, "minibatch_size": 256, "shuffle_batch_per_epoch": true, "model": {"fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "log_std_clip_param": 20.0, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1, "_disable_preprocessor_api": false, "_disable_action_flattening": false}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "learner_config_dict": {}, "optimizer": {}, "_learner_class": null, "callbacks_on_algorithm_init": null, "callbacks_on_env_runners_recreated": null, "callbacks_on_offline_eval_runners_recreated": null, "callbacks_on_checkpoint_loaded": null, "callbacks_on_environment_created": null, "callbacks_on_episode_created": null, "callbacks_on_episode_start": null, "callbacks_on_episode_step": null, "callbacks_on_episode_end": null, "callbacks_on_evaluate_start": null, "callbacks_on_evaluate_end": null, "callbacks_on_evaluate_offline_start": null, "callbacks_on_evaluate_offline_end": null, "callbacks_on_sample_end": null, "callbacks_on_train_result": null, "explore": true, "enable_rl_module_and_learner": false, "enable_env_runner_and_connector_v2": false, "_prior_exploration_config": null, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function train.<locals>.<lambda> at 0x75979fd644c0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "offline_data_class": null, "input_read_method": "read_parquet", "input_read_method_kwargs": {}, "input_read_schema": {}, "input_read_episodes": false, "input_read_sample_batches": false, "input_read_batch_size": null, "input_filesystem": null, "input_filesystem_kwargs": {}, "input_compress_columns": ["obs", "new_obs"], "input_spaces_jsonable": true, "materialize_data": false, "materialize_mapped_data": true, "map_batches_kwargs": {}, "iter_batches_kwargs": {}, "ignore_final_observation": false, "prelearner_class": null, "prelearner_buffer_class": null, "prelearner_buffer_kwargs": {}, "prelearner_module_synch_period": 10, "dataset_num_iters_per_learner": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "output_max_rows_per_file": null, "output_write_remaining_data": false, "output_write_method": "write_parquet", "output_write_method_kwargs": {}, "output_filesystem": null, "output_filesystem_kwargs": {}, "output_write_episodes": true, "offline_sampling": false, "evaluation_interval": 10, "evaluation_duration": 5, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 120.0, "evaluation_auto_duration_min_env_steps_per_sample": 100, "evaluation_auto_duration_max_env_steps_per_sample": 2000, "evaluation_parallel_to_training": false, "evaluation_force_reset_envs_before_iteration": true, "evaluation_config": {"explore": false}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_env_runners": 0, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 10.0, "offline_evaluation_interval": null, "num_offline_eval_runners": 0, "offline_evaluation_type": null, "offline_eval_runner_class": null, "offline_loss_for_module_fn": null, "offline_evaluation_duration": 1, "offline_evaluation_parallel_to_training": false, "offline_evaluation_timeout_s": 120.0, "num_cpus_per_offline_eval_runner": 1, "num_gpus_per_offline_eval_runner": 0, "custom_resources_per_offline_eval_runner": {}, "restart_failed_offline_eval_runners": true, "ignore_offline_eval_runner_failures": false, "max_num_offline_eval_runner_restarts": 1000, "offline_eval_runner_restore_timeout_s": 1800.0, "max_requests_in_flight_per_offline_eval_runner": 1, "validate_offline_eval_runners_after_construction": true, "offline_eval_runner_health_probe_timeout_s": 30.0, "offline_eval_rl_module_inference_only": false, "broadcast_offline_eval_runner_states": false, "offline_eval_batch_size_per_runner": 256, "dataset_num_iters_per_eval_runner": 1, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "log_gradients": false, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "restart_failed_env_runners": true, "ignore_env_runner_failures": false, "max_num_env_runner_restarts": 1000, "delay_between_env_runner_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_env_runner_failures_tolerance": 100, "env_runner_health_probe_timeout_s": 30.0, "env_runner_restore_timeout_s": 1800.0, "_model_config": {}, "_rl_module_spec": null, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "_validate_config": true, "_use_msgpack_checkpoints": false, "_torch_grad_scaler_class": null, "_torch_lr_scheduler_classes": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "_dont_auto_sync_env_runner_states": false, "env_task_fn": -1, "enable_connectors": -1, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "enable_async_evaluation": -1, "custom_async_evaluation_function": -1, "_enable_rl_module_api": -1, "auto_wrap_old_gym_envs": -1, "always_attach_evaluation_results": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "clip_param": 0.2, "vf_clip_param": 10.0, "entropy_coeff_schedule": null, "lr_schedule": null, "sgd_minibatch_size": -1, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 0.95, "input": "sampler", "policies": {"agent_0": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_1": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_2": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_3": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_4": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_5": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_6": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_7": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_8": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_9": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_10": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_11": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_12": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_13": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_14": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_15": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_16": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_17": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_18": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_19": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_20": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}]}, "callbacks": "<class 'ray.rllib.callbacks.callbacks.RLlibCallback'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch"}, "time_since_restore": 151.31890416145325, "iterations_since_restore": 5, "perf": {"cpu_util_percent": 8.748780487804876, "ram_util_percent": 27.524390243902445, "gpu_util_percent0": 0.20073170731707318, "vram_util_percent0": 0.0961013738129565}}
{"custom_metrics": {}, "episode_media": {}, "info": {"learner": {"agent_4": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.816123154759407, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.036947592180513314, "policy_loss": -0.05640200666384772, "vf_loss": 0.016533226711908357, "vf_explained_var": 0.9338246591389179, "kl": 0.00973729162366549, "entropy": 5.81858924627304, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_19": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.157169219851494, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.011984389928693417, "policy_loss": -0.05333701530325925, "vf_loss": 0.0382073035754729, "vf_explained_var": 0.8489503525197506, "kl": 0.010484405174083855, "entropy": 5.360560193657875, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.342513781785965, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.02936519103095634, "policy_loss": -0.05313907828749507, "vf_loss": 0.020858147749095224, "vf_explained_var": 0.8785913720726967, "kl": 0.009719133789233548, "entropy": 5.7391522347927095, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_16": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.974333730340004, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.006652799097355455, "policy_loss": -0.050916684643561895, "vf_loss": 0.040751679823733865, "vf_explained_var": 0.8871829513460397, "kl": 0.011707353824013977, "entropy": 5.081728079915047, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_9": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.4052833706140517, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.033682304094691064, "policy_loss": -0.05439880756894126, "vf_loss": 0.0179106200725073, "vf_explained_var": 0.9204930614680051, "kl": 0.009352944390023859, "entropy": 5.853630748391152, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_5": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.025895120203495, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.036291360638279, "policy_loss": -0.053601708670612425, "vf_loss": 0.014746995066525414, "vf_explained_var": 0.9499204155057669, "kl": 0.008544513766950115, "entropy": 5.714627885818482, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_7": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.655260710418224, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03353029349527788, "policy_loss": -0.05460962601937354, "vf_loss": 0.018156869162339717, "vf_explained_var": 0.9271282646805048, "kl": 0.009741541994621933, "entropy": 5.818973195552826, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_14": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.20392320305109, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.01117732508428162, "policy_loss": -0.05327790154260583, "vf_loss": 0.03880835215095431, "vf_explained_var": 0.8617674980312586, "kl": 0.010974079669406934, "entropy": 5.3939208656549456, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_18": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.906054040789604, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.010815595275198575, "policy_loss": -0.05276526506640948, "vf_loss": 0.03862879763473757, "vf_explained_var": 0.913093152269721, "kl": 0.011069571756698626, "entropy": 5.481313794851303, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_12": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.135285636782646, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.02764849651284749, "policy_loss": -0.05263280258513987, "vf_loss": 0.02236902626173105, "vf_explained_var": 0.9213285014033318, "kl": 0.008717598275253664, "entropy": 5.876158130168915, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_11": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.123868834972382, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.027144308350398206, "policy_loss": -0.051402652359684, "vf_loss": 0.021838704514084383, "vf_explained_var": 0.889066732302308, "kl": 0.008065462466460116, "entropy": 5.851767528057098, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_13": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.6497389167547225, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.030426665747654624, "policy_loss": -0.048882301000412555, "vf_loss": 0.0160296749236295, "vf_explained_var": 0.9395570036023855, "kl": 0.008086533799704832, "entropy": 5.859215441346168, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.1885216474533085, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03206577208911767, "policy_loss": -0.05216481983370613, "vf_loss": 0.017666667129378765, "vf_explained_var": 0.9359259672462941, "kl": 0.008107935766224539, "entropy": 5.873505017161369, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_20": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.151232071220875, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.015679872727196197, "policy_loss": -0.05397995987441391, "vf_loss": 0.03502580973436124, "vf_explained_var": 0.935293922945857, "kl": 0.010914259635787432, "entropy": 5.431407848000527, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.801121005415917, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.029801950986438897, "policy_loss": -0.05251432079821825, "vf_loss": 0.020040764575242065, "vf_explained_var": 0.9264612209051848, "kl": 0.008905351027446533, "entropy": 5.8359675139188765, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_10": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.089656671881675, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03249824281228939, "policy_loss": -0.05458973728527781, "vf_loss": 0.019675429866765626, "vf_explained_var": 0.8420846294611692, "kl": 0.008053546554190938, "entropy": 5.893489411473274, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.777277298271656, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.038308573588437866, "policy_loss": -0.05721471304714214, "vf_loss": 0.01626039952680003, "vf_explained_var": 0.9586929894983769, "kl": 0.008819134052750938, "entropy": 5.774348184466362, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_15": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.367330986261368, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": 0.00427466716937488, "policy_loss": -0.048507875435188905, "vf_loss": 0.04888611989445053, "vf_explained_var": 0.8729274947196245, "kl": 0.012988077071548943, "entropy": 5.394696721434594, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_17": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.67569383084774, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.011977090062282513, "policy_loss": -0.05281170583330095, "vf_loss": 0.03736250157817267, "vf_explained_var": 0.8687467020004987, "kl": 0.011573710266319481, "entropy": 5.183851754665374, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_6": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.5100993946194645, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03390339664547355, "policy_loss": -0.054422189397155304, "vf_loss": 0.01791202347376384, "vf_explained_var": 0.8459636747837067, "kl": 0.008689230324947627, "entropy": 5.8340441435575485, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_8": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.806490659713745, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.029475653576810146, "policy_loss": -0.04942468485678546, "vf_loss": 0.017694845856749453, "vf_explained_var": 0.9026324685662985, "kl": 0.0075139499838065725, "entropy": 5.837561309337616, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 880.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}}, "num_env_steps_sampled": 24000, "num_env_steps_trained": 24000, "num_agent_steps_sampled": 504000, "num_agent_steps_trained": 504000}, "env_runners": {"episode_reward_max": -42.70999999999981, "episode_reward_min": -75.40000000000191, "episode_reward_mean": -57.34108695652321, "episode_len_mean": 500.0, "episode_media": {}, "episodes_timesteps_total": 23000, "policy_reward_min": {"agent_0": -5.280000000000027, "agent_1": -5.380000000000028, "agent_2": -5.330000000000028, "agent_3": -5.350000000000028, "agent_4": -5.340000000000028, "agent_5": -5.350000000000028, "agent_6": -5.350000000000028, "agent_7": -5.340000000000028, "agent_8": -5.300000000000027, "agent_9": -5.330000000000028, "agent_10": -5.350000000000028, "agent_11": -5.340000000000028, "agent_12": -5.380000000000028, "agent_13": -5.310000000000027, "agent_14": -5.240000000000027, "agent_15": -5.300000000000027, "agent_16": -5.300000000000027, "agent_17": -5.3700000000000285, "agent_18": -5.290000000000028, "agent_19": -5.340000000000028, "agent_20": -5.350000000000028}, "policy_reward_max": {"agent_0": -0.5000000000000003, "agent_1": -0.5000000000000003, "agent_2": -0.5000000000000003, "agent_3": -0.5000000000000003, "agent_4": -0.5000000000000003, "agent_5": -0.5000000000000003, "agent_6": -0.5000000000000003, "agent_7": -0.1000000000000006, "agent_8": -0.5000000000000003, "agent_9": -0.5000000000000003, "agent_10": -0.5000000000000003, "agent_11": -0.5000000000000003, "agent_12": -0.5000000000000003, "agent_13": -0.5000000000000003, "agent_14": -3.590000000000009, "agent_15": -3.7300000000000106, "agent_16": -3.790000000000011, "agent_17": -3.6300000000000097, "agent_18": -3.1700000000000044, "agent_19": -3.6300000000000106, "agent_20": -3.320000000000006}, "policy_reward_mean": {"agent_0": -1.8760869565217462, "agent_1": -1.364782608695657, "agent_2": -2.23869565217392, "agent_3": -2.396739130434793, "agent_4": -1.0806521739130468, "agent_5": -2.3528260869565316, "agent_6": -1.9460869565217458, "agent_7": -1.8982608695652234, "agent_8": -1.502608695652179, "agent_9": -1.2706521739130474, "agent_10": -2.498478260869575, "agent_11": -1.826304347826094, "agent_12": -1.6832608695652231, "agent_13": -2.0165217391304426, "agent_14": -4.473913043478279, "agent_15": -4.478478260869584, "agent_16": -4.658260869565237, "agent_17": -4.5215217391304545, "agent_18": -4.408478260869583, "agent_19": -4.395000000000017, "agent_20": -4.453478260869583}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-53.290000000000305, -55.12000000000127, -61.81000000000211, -47.65000000000047, -57.68000000000135, -56.200000000001694, -52.780000000000356, -57.85000000000114, -54.040000000000376, -64.27000000000298, -49.57000000000024, -73.5400000000017, -62.680000000002885, -60.280000000003, -52.53000000000038, -58.05000000000256, -43.9300000000001, -59.7700000000019, -61.3400000000021, -52.9100000000017, -56.880000000001324, -62.430000000003005, -67.53000000000264, -58.4000000000019, -63.640000000003184, -48.380000000000244, -59.750000000001855, -54.76000000000095, -48.56000000000037, -72.73000000000226, -61.48000000000291, -54.10000000000039, -75.40000000000191, -57.9400000000012, -67.91000000000251, -57.94000000000153, -47.429999999999914, -52.13000000000102, -55.420000000001096, -57.8500000000018, -50.93000000000088, -64.32000000000227, -55.500000000001464, -42.70999999999981, -51.63000000000134, -56.65000000000114], "episode_lengths": [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500], "policy_agent_0_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.070000000000025, -5.240000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.110000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -2.5999999999999983, -5.210000000000027, -0.5000000000000003, -5.070000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.280000000000027, -5.210000000000027, -5.020000000000024, -0.5000000000000003, -5.090000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.8200000000000007, -3.770000000000011, -5.000000000000024, -5.120000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_1_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.5500000000000087, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -5.360000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.920000000000023, -0.5000000000000003, -5.360000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.780000000000022, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.480000000000018, -0.5000000000000003, -0.5000000000000003, -5.380000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_2_reward": [-5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -3.500000000000008, -4.840000000000023, -2.129999999999993, -2.149999999999986, -5.130000000000026, -2.429999999999996, -0.5000000000000003, -0.5000000000000003, -4.67000000000002, -0.5000000000000003, -0.5000000000000003, -4.830000000000022, -5.060000000000025, -0.5000000000000003, -4.520000000000019, -0.5000000000000003, -4.780000000000022, -5.070000000000025, -0.5000000000000003, -5.170000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.330000000000028, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.160000000000026, -0.5000000000000003, -2.5499999999999976, -0.5000000000000003, -5.010000000000025, -0.5000000000000003, -2.579999999999998, -0.5000000000000003], "policy_agent_3_reward": [-0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.050000000000025, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -5.160000000000026, -0.5000000000000003, -5.330000000000028, -5.240000000000027, -0.7300000000000005, -5.180000000000026, -4.590000000000019, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -5.190000000000026, -0.8900000000000007, -5.350000000000028, -5.320000000000028, -0.5000000000000003, -0.5000000000000003, -3.7500000000000107, -0.5000000000000003, -4.8200000000000225, -5.320000000000028, -4.660000000000021, -5.080000000000025, -0.5000000000000003, -0.5000000000000003, -5.010000000000025, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_4_reward": [-0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.8100000000000116, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.110000000000025, -4.950000000000024], "policy_agent_5_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5000000000000003, -4.870000000000023, -5.350000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.589999999999998, -5.000000000000024, -2.930000000000002, -5.290000000000028, -0.5000000000000003, -5.090000000000026, -3.8100000000000116, -0.5000000000000003, -5.050000000000025, -5.300000000000027, -5.150000000000026, -3.6800000000000175, -5.110000000000025, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -5.000000000000024, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_6_reward": [-5.310000000000027, -0.5000000000000003, -0.5000000000000003, -4.430000000000018, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -4.930000000000024, -5.030000000000024, -0.5000000000000003, -2.429999999999996, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.720000000000021, -5.330000000000028, -0.5000000000000003, -3.5700000000000087, -0.5000000000000003, -0.5000000000000003, -2.2799999999999945, -5.310000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.240000000000027, -0.5000000000000003, -0.5000000000000003, -5.350000000000028], "policy_agent_7_reward": [-0.5000000000000003, -2.349999999999995, -5.220000000000026, -2.2499999999999942, -5.040000000000025, -2.75, -5.300000000000027, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.0000000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.8999999999999926, -5.110000000000025, -5.330000000000028, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.890000000000012, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -0.1000000000000006, -4.890000000000023], "policy_agent_8_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.040000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -4.54000000000002, -0.5000000000000003, -3.0900000000000034, -0.5000000000000003, -0.5000000000000003, -5.080000000000025, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -4.61000000000002, -0.5000000000000003, -0.5000000000000003, -3.390000000000007, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.710000000000021, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_9_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.790000000000011, -2.8200000000000007, -0.5000000000000003, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5600000000000004, -0.5000000000000003, -0.5000000000000003, -4.140000000000015, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.330000000000028, -3.3800000000000066, -0.5000000000000003, -5.120000000000026, -0.5000000000000003], "policy_agent_10_reward": [-4.690000000000021, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.120000000000026, -4.930000000000024, -5.350000000000028, -0.5000000000000003, -2.119999999999993, -0.5000000000000003, -4.920000000000023, -2.6199999999999983, -0.5000000000000003, -3.4700000000000077, -4.560000000000019, -4.910000000000023, -4.030000000000014, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -0.5000000000000003, -4.910000000000023, -0.9700000000000008, -0.5000000000000003, -5.050000000000025, -0.5000000000000003, -5.310000000000027, -4.850000000000023, -0.5000000000000003, -5.030000000000024, -4.970000000000024, -5.130000000000026, -4.9000000000000234, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.130000000000026, -0.5000000000000003], "policy_agent_11_reward": [-0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.3699999999999974, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -5.330000000000028, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -5.320000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.180000000000026, -5.230000000000027, -5.220000000000026, -4.380000000000018, -5.290000000000028, -4.800000000000022, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_12_reward": [-0.5000000000000003, -5.240000000000027, -5.090000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.170000000000027, -0.5000000000000003, -0.5000000000000003, -2.139999999999993, -0.5000000000000003, -0.5000000000000003, -4.550000000000019, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.3399999999999954, -0.5000000000000003, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -5.380000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -4.870000000000023, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.270000000000027, -3.970000000000013, -0.5000000000000003, -1.5799999999999954, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_13_reward": [-0.5000000000000003, -5.220000000000026, -5.270000000000027, -0.5000000000000003, -0.9600000000000007, -3.2600000000000056, -5.110000000000025, -5.260000000000027, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -5.280000000000027, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.210000000000027, -4.920000000000023, -0.5000000000000003, -0.5000000000000003, -5.030000000000024, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.210000000000027, -0.5000000000000003, -0.5000000000000003, -1.2699999999999982, -0.5000000000000003, -0.7400000000000005, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.180000000000026], "policy_agent_14_reward": [-4.680000000000021, -4.600000000000021, -4.260000000000016, -4.000000000000013, -4.690000000000021, -4.530000000000019, -5.240000000000027, -4.8600000000000225, -5.150000000000026, -5.220000000000026, -3.780000000000011, -4.090000000000014, -5.000000000000024, -4.970000000000024, -4.61000000000002, -5.000000000000024, -4.160000000000014, -4.8200000000000225, -4.710000000000021, -4.420000000000018, -4.360000000000017, -3.680000000000009, -3.590000000000009, -4.400000000000017, -4.350000000000017, -4.64000000000002, -4.57000000000002, -4.290000000000017, -3.850000000000012, -3.9100000000000126, -4.180000000000017, -4.980000000000024, -4.200000000000015, -3.950000000000013, -4.870000000000023, -3.66000000000001, -5.130000000000026, -4.090000000000015, -5.220000000000026, -4.2300000000000155, -3.920000000000013, -5.060000000000025, -4.740000000000022, -4.8200000000000225, -4.2300000000000155, -4.090000000000014], "policy_agent_15_reward": [-5.140000000000025, -4.400000000000018, -4.830000000000022, -4.520000000000019, -5.040000000000025, -3.950000000000013, -4.2300000000000155, -4.700000000000021, -3.770000000000011, -3.970000000000013, -4.950000000000024, -4.110000000000015, -4.560000000000019, -4.470000000000018, -4.120000000000014, -4.300000000000017, -3.8200000000000114, -4.240000000000015, -4.520000000000019, -4.480000000000018, -4.700000000000021, -4.030000000000014, -4.260000000000016, -4.490000000000019, -5.110000000000025, -4.130000000000015, -3.970000000000013, -4.710000000000021, -4.000000000000013, -4.960000000000024, -4.0700000000000145, -4.210000000000016, -4.6200000000000205, -5.040000000000025, -4.510000000000019, -4.410000000000018, -5.200000000000027, -5.290000000000028, -4.58000000000002, -3.7300000000000106, -4.730000000000022, -4.2300000000000155, -5.300000000000027, -4.52000000000002, -4.730000000000023, -4.360000000000017], "policy_agent_16_reward": [-4.950000000000024, -5.150000000000026, -4.950000000000024, -4.140000000000015, -5.220000000000026, -4.180000000000016, -5.280000000000027, -4.750000000000021, -5.140000000000026, -4.530000000000019, -4.410000000000018, -4.720000000000021, -4.800000000000023, -4.120000000000014, -4.63000000000002, -4.67000000000002, -5.100000000000025, -4.470000000000018, -4.700000000000021, -4.240000000000016, -4.450000000000019, -4.440000000000018, -5.250000000000027, -5.180000000000026, -4.010000000000013, -4.250000000000016, -5.010000000000025, -4.590000000000019, -4.090000000000014, -4.560000000000019, -3.790000000000011, -5.230000000000027, -4.080000000000014, -4.500000000000019, -5.080000000000025, -5.020000000000024, -5.050000000000025, -4.120000000000014, -5.300000000000027, -4.470000000000018, -4.510000000000018, -5.000000000000024, -4.790000000000022, -4.280000000000016, -4.3100000000000165, -4.770000000000023], "policy_agent_17_reward": [-4.780000000000021, -4.790000000000022, -3.9800000000000133, -4.3800000000000185, -4.340000000000017, -4.750000000000021, -3.8100000000000116, -4.54000000000002, -5.190000000000026, -5.100000000000025, -4.140000000000015, -4.140000000000015, -4.260000000000016, -4.250000000000016, -5.340000000000028, -4.330000000000017, -4.950000000000024, -4.61000000000002, -5.3700000000000285, -4.490000000000019, -4.140000000000015, -4.730000000000022, -4.520000000000019, -4.64000000000002, -4.210000000000016, -3.6300000000000097, -3.9000000000000123, -4.180000000000016, -4.990000000000023, -4.4300000000000175, -5.220000000000027, -3.970000000000013, -5.090000000000026, -4.2700000000000164, -4.650000000000021, -5.040000000000025, -4.200000000000015, -3.760000000000011, -5.230000000000027, -4.660000000000021, -3.760000000000011, -5.310000000000027, -4.320000000000017, -4.960000000000024, -3.670000000000011, -4.970000000000025], "policy_agent_18_reward": [-5.280000000000027, -4.150000000000015, -3.9000000000000123, -5.290000000000028, -4.800000000000022, -5.180000000000027, -3.5300000000000082, -3.780000000000011, -4.720000000000021, -4.720000000000021, -4.460000000000019, -5.290000000000028, -4.770000000000022, -4.990000000000024, -4.60000000000002, -4.550000000000019, -4.430000000000018, -3.5700000000000087, -4.990000000000024, -3.5700000000000074, -4.290000000000017, -5.150000000000026, -4.7400000000000215, -4.400000000000018, -4.170000000000015, -4.330000000000017, -3.940000000000013, -4.760000000000022, -4.090000000000014, -5.120000000000027, -3.9000000000000123, -5.100000000000025, -5.220000000000026, -4.480000000000018, -4.0700000000000145, -4.3100000000000165, -3.880000000000013, -4.120000000000014, -4.910000000000023, -4.240000000000016, -3.1700000000000044, -3.9300000000000126, -3.5600000000000085, -3.9000000000000123, -3.780000000000011, -4.660000000000021], "policy_agent_19_reward": [-3.9800000000000133, -4.020000000000014, -5.090000000000026, -3.75000000000001, -4.370000000000019, -4.60000000000002, -5.110000000000025, -4.9400000000000235, -4.560000000000019, -4.220000000000016, -4.0700000000000145, -5.040000000000025, -3.8100000000000116, -4.5400000000000205, -3.9600000000000133, -4.130000000000014, -3.8300000000000116, -4.530000000000019, -4.170000000000015, -4.760000000000022, -4.410000000000018, -4.030000000000014, -5.290000000000028, -4.450000000000019, -5.240000000000028, -4.030000000000014, -4.63000000000002, -3.7000000000000104, -4.180000000000016, -5.340000000000028, -4.200000000000015, -4.870000000000023, -5.270000000000027, -4.960000000000024, -4.610000000000021, -4.710000000000022, -3.790000000000011, -3.6300000000000106, -3.9800000000000133, -3.950000000000013, -4.64000000000002, -4.64000000000002, -3.820000000000012, -3.8000000000000114, -4.180000000000016, -4.340000000000017], "policy_agent_20_reward": [-3.7300000000000106, -4.9000000000000234, -4.2300000000000155, -4.240000000000016, -3.6700000000000097, -4.660000000000021, -4.500000000000019, -4.350000000000017, -4.460000000000019, -4.570000000000021, -4.550000000000019, -5.090000000000026, -4.360000000000017, -4.100000000000014, -4.100000000000015, -4.020000000000013, -4.090000000000014, -4.690000000000021, -5.050000000000025, -5.330000000000028, -4.810000000000022, -5.260000000000027, -4.920000000000023, -4.2700000000000164, -3.9900000000000135, -4.170000000000015, -5.300000000000027, -4.510000000000019, -3.870000000000012, -4.050000000000014, -3.8900000000000134, -4.800000000000022, -4.2700000000000164, -5.220000000000027, -5.320000000000028, -4.380000000000018, -4.360000000000017, -4.1100000000000145, -3.770000000000011, -3.320000000000006, -4.780000000000022, -5.350000000000028, -4.260000000000016, -4.910000000000023, -4.1900000000000155, -4.090000000000014]}, "sampler_perf": {"mean_raw_obs_processing_ms": 1.9569042566856176, "mean_inference_ms": 9.224713436902247, "mean_action_processing_ms": 0.7276407794088273, "mean_env_wait_ms": 2.800346607496096, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"ObsPreprocessorConnector_ms": 0.0022241047450474332, "StateBufferConnector_ms": 0.0014704206715459409, "ViewRequirementAgentConnector_ms": 0.03300658417537839}, "num_episodes": 7, "episode_return_max": -42.70999999999981, "episode_return_min": -75.40000000000191, "episode_return_mean": -57.34108695652321, "episodes_this_iter": 7}, "num_healthy_workers": 3, "actor_manager_num_outstanding_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 504000, "num_agent_steps_trained": 504000, "num_env_steps_sampled": 24000, "num_env_steps_trained": 24000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 134.20932864337618, "num_env_steps_trained_throughput_per_sec": 134.20932864337618, "timesteps_total": 24000, "num_env_steps_sampled_lifetime": 24000, "num_agent_steps_sampled_lifetime": 504000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 504000, "timers": {"training_iteration_time_ms": 30176.92, "restore_workers_time_ms": 0.014, "training_step_time_ms": 30176.874, "sample_time_ms": 20131.786, "learn_time_ms": 10022.085, "learn_throughput": 399.119, "synch_weights_time_ms": 20.751}, "counters": {"num_env_steps_sampled": 24000, "num_env_steps_trained": 24000, "num_agent_steps_sampled": 504000, "num_agent_steps_trained": 504000}, "done": false, "training_iteration": 6, "trial_id": "cfc88_00000", "date": "2025-10-21_11-20-58", "timestamp": 1761038458, "time_this_iter_s": 29.816407442092896, "time_total_s": 181.13531160354614, "pid": 3279004, "hostname": "xuezhi-Precision-3660", "node_ip": "130.238.16.41", "config": {"exploration_config": {"type": "StochasticSampling"}, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "placement_strategy": "PACK", "num_gpus": 1, "_fake_gpus": false, "num_cpus_for_main_process": 1, "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "torch_ddp_kwargs": {}, "torch_skip_nan_gradients": false, "env": "<class 'train_utils.WarehouseMultiAgentEnv'>", "env_config": {"env_id": "tarware-extralarge-14agvs-7pickers-partialobs-chg-v1"}, "observation_space": null, "action_space": null, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "_is_atari": null, "disable_env_checking": false, "render_env": true, "action_mask_key": "action_mask", "env_runner_cls": null, "num_env_runners": 3, "create_local_env_runner": true, "num_envs_per_env_runner": 1, "gym_env_vectorize_mode": "SYNC", "num_cpus_per_env_runner": 1, "num_gpus_per_env_runner": 0, "custom_resources_per_env_runner": {}, "validate_env_runners_after_construction": true, "episodes_to_numpy": true, "max_requests_in_flight_per_env_runner": 1, "sample_timeout_s": 60.0, "_env_to_module_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "_module_to_env_connector": null, "add_default_connectors_to_module_to_env_pipeline": true, "merge_env_runner_states": "training_only", "broadcast_env_runner_states": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "compress_observations": false, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "sampler_perf_stats_ema_coef": null, "_is_online": true, "num_learners": 0, "num_gpus_per_learner": 1, "num_cpus_per_learner": "auto", "num_aggregator_actors_per_learner": 0, "max_requests_in_flight_per_aggregator_actor": 3, "local_gpu_idx": 0, "max_requests_in_flight_per_learner": 3, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "_train_batch_size_per_learner": null, "train_batch_size": 4000, "num_epochs": 10, "minibatch_size": 256, "shuffle_batch_per_epoch": true, "model": {"fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "log_std_clip_param": 20.0, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1, "_disable_preprocessor_api": false, "_disable_action_flattening": false}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "learner_config_dict": {}, "optimizer": {}, "_learner_class": null, "callbacks_on_algorithm_init": null, "callbacks_on_env_runners_recreated": null, "callbacks_on_offline_eval_runners_recreated": null, "callbacks_on_checkpoint_loaded": null, "callbacks_on_environment_created": null, "callbacks_on_episode_created": null, "callbacks_on_episode_start": null, "callbacks_on_episode_step": null, "callbacks_on_episode_end": null, "callbacks_on_evaluate_start": null, "callbacks_on_evaluate_end": null, "callbacks_on_evaluate_offline_start": null, "callbacks_on_evaluate_offline_end": null, "callbacks_on_sample_end": null, "callbacks_on_train_result": null, "explore": true, "enable_rl_module_and_learner": false, "enable_env_runner_and_connector_v2": false, "_prior_exploration_config": null, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function train.<locals>.<lambda> at 0x75979fbe5630>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "offline_data_class": null, "input_read_method": "read_parquet", "input_read_method_kwargs": {}, "input_read_schema": {}, "input_read_episodes": false, "input_read_sample_batches": false, "input_read_batch_size": null, "input_filesystem": null, "input_filesystem_kwargs": {}, "input_compress_columns": ["obs", "new_obs"], "input_spaces_jsonable": true, "materialize_data": false, "materialize_mapped_data": true, "map_batches_kwargs": {}, "iter_batches_kwargs": {}, "ignore_final_observation": false, "prelearner_class": null, "prelearner_buffer_class": null, "prelearner_buffer_kwargs": {}, "prelearner_module_synch_period": 10, "dataset_num_iters_per_learner": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "output_max_rows_per_file": null, "output_write_remaining_data": false, "output_write_method": "write_parquet", "output_write_method_kwargs": {}, "output_filesystem": null, "output_filesystem_kwargs": {}, "output_write_episodes": true, "offline_sampling": false, "evaluation_interval": 10, "evaluation_duration": 5, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 120.0, "evaluation_auto_duration_min_env_steps_per_sample": 100, "evaluation_auto_duration_max_env_steps_per_sample": 2000, "evaluation_parallel_to_training": false, "evaluation_force_reset_envs_before_iteration": true, "evaluation_config": {"explore": false}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_env_runners": 0, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 10.0, "offline_evaluation_interval": null, "num_offline_eval_runners": 0, "offline_evaluation_type": null, "offline_eval_runner_class": null, "offline_loss_for_module_fn": null, "offline_evaluation_duration": 1, "offline_evaluation_parallel_to_training": false, "offline_evaluation_timeout_s": 120.0, "num_cpus_per_offline_eval_runner": 1, "num_gpus_per_offline_eval_runner": 0, "custom_resources_per_offline_eval_runner": {}, "restart_failed_offline_eval_runners": true, "ignore_offline_eval_runner_failures": false, "max_num_offline_eval_runner_restarts": 1000, "offline_eval_runner_restore_timeout_s": 1800.0, "max_requests_in_flight_per_offline_eval_runner": 1, "validate_offline_eval_runners_after_construction": true, "offline_eval_runner_health_probe_timeout_s": 30.0, "offline_eval_rl_module_inference_only": false, "broadcast_offline_eval_runner_states": false, "offline_eval_batch_size_per_runner": 256, "dataset_num_iters_per_eval_runner": 1, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "log_gradients": false, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "restart_failed_env_runners": true, "ignore_env_runner_failures": false, "max_num_env_runner_restarts": 1000, "delay_between_env_runner_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_env_runner_failures_tolerance": 100, "env_runner_health_probe_timeout_s": 30.0, "env_runner_restore_timeout_s": 1800.0, "_model_config": {}, "_rl_module_spec": null, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "_validate_config": true, "_use_msgpack_checkpoints": false, "_torch_grad_scaler_class": null, "_torch_lr_scheduler_classes": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "_dont_auto_sync_env_runner_states": false, "env_task_fn": -1, "enable_connectors": -1, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "enable_async_evaluation": -1, "custom_async_evaluation_function": -1, "_enable_rl_module_api": -1, "auto_wrap_old_gym_envs": -1, "always_attach_evaluation_results": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "clip_param": 0.2, "vf_clip_param": 10.0, "entropy_coeff_schedule": null, "lr_schedule": null, "sgd_minibatch_size": -1, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 0.95, "input": "sampler", "policies": {"agent_0": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_1": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_2": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_3": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_4": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_5": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_6": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_7": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_8": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_9": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_10": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_11": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_12": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_13": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_14": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_15": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_16": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_17": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_18": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_19": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_20": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}]}, "callbacks": "<class 'ray.rllib.callbacks.callbacks.RLlibCallback'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch"}, "time_since_restore": 181.13531160354614, "iterations_since_restore": 6, "perf": {"cpu_util_percent": 8.860975609756096, "ram_util_percent": 27.463414634146346, "gpu_util_percent0": 0.21048780487804875, "vram_util_percent0": 0.09639210266064555}}
{"custom_metrics": {}, "episode_media": {}, "info": {"learner": {"agent_4": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.894450509548187, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03476134444354102, "policy_loss": -0.05389369966724189, "vf_loss": 0.01636459506698884, "vf_explained_var": 0.8687057722359895, "kl": 0.00922586409753876, "entropy": 5.812503555417061, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_19": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.975583915412426, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.014189992463798262, "policy_loss": -0.05215093782753684, "vf_loss": 0.03463750195223838, "vf_explained_var": 0.8616186954081059, "kl": 0.011078145076045542, "entropy": 5.356221321225166, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.270707470178604, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03225911665795138, "policy_loss": -0.049664796813158316, "vf_loss": 0.01455973061383702, "vf_explained_var": 0.8840881887823343, "kl": 0.0094865004703373, "entropy": 5.638458985090256, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_16": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 7.551849487423897, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.0035967511736089363, "policy_loss": -0.04807658062200062, "vf_loss": 0.040901996928732844, "vf_explained_var": 0.9612156867980957, "kl": 0.011926108844636473, "entropy": 5.108847883343697, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_9": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.1819927275180815, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.037368422771396584, "policy_loss": -0.05480067789176246, "vf_loss": 0.014731990636209957, "vf_explained_var": 0.9146540973335504, "kl": 0.009000884626573225, "entropy": 5.837162593007088, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_5": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.1473264083266255, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.035920958455244546, "policy_loss": -0.05275731667934451, "vf_loss": 0.014053777558729053, "vf_explained_var": 0.88127126917243, "kl": 0.009275271198730601, "entropy": 5.71482155919075, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_7": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.162483349442482, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03782302029721905, "policy_loss": -0.05423649584117811, "vf_loss": 0.013793245912529528, "vf_explained_var": 0.9081599589437246, "kl": 0.008734099112371042, "entropy": 5.7725765109062195, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_14": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.731833422183991, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.02114613987650955, "policy_loss": -0.04999823598191142, "vf_loss": 0.025921310670673846, "vf_explained_var": 0.8918871745467186, "kl": 0.009769278794703073, "entropy": 5.409965464472771, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_18": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 7.4036392867565155, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.010861328868486453, "policy_loss": -0.049531528598163274, "vf_loss": 0.035372334177372976, "vf_explained_var": 0.9269998777657747, "kl": 0.010992887774868565, "entropy": 5.422397583723068, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_12": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.6355735912919043, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.033900025078764884, "policy_loss": -0.051471042807679626, "vf_loss": 0.015121771191479639, "vf_explained_var": 0.9512994568794966, "kl": 0.008164158671159383, "entropy": 5.832524791359901, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_11": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.787819464504719, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.028704083208867814, "policy_loss": -0.051558715937426315, "vf_loss": 0.020188090804731475, "vf_explained_var": 0.8905659865587949, "kl": 0.008888469323564635, "entropy": 5.792205265164375, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_13": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.7862399518489838, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03882527368405135, "policy_loss": -0.0555972966743866, "vf_loss": 0.013948741959757172, "vf_explained_var": 0.879136748984456, "kl": 0.009410938350328432, "entropy": 5.770944210886955, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.733140301704407, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03351432805211516, "policy_loss": -0.05140192012768239, "vf_loss": 0.015177767138811759, "vf_explained_var": 0.9205551531165839, "kl": 0.009032747243505754, "entropy": 5.820621392130851, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_20": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.762016071379184, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.012418179881933611, "policy_loss": -0.04777166485291673, "vf_loss": 0.03218177600065246, "vf_explained_var": 0.9434992857277393, "kl": 0.010572365387280337, "entropy": 5.320539829134941, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.9212553665041923, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03291238387246267, "policy_loss": -0.05167695375275798, "vf_loss": 0.01605457786063198, "vf_explained_var": 0.8444697793573142, "kl": 0.009033308627365056, "entropy": 5.819334217905999, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_10": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.8766982063651083, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.038902439114826846, "policy_loss": -0.05592073603183963, "vf_loss": 0.014657587176770903, "vf_explained_var": 0.9115712247788906, "kl": 0.00786903137724972, "entropy": 5.849792686104775, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.628292535245419, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.032458020524063616, "policy_loss": -0.0517065418564016, "vf_loss": 0.016337248677155004, "vf_explained_var": 0.8976349532604218, "kl": 0.009704242935778285, "entropy": 5.736710339784622, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_15": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.228632570803166, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.020733396829746197, "policy_loss": -0.053102351317647845, "vf_loss": 0.029131166485603897, "vf_explained_var": 0.8478197079151869, "kl": 0.010792624910126758, "entropy": 5.426778554916382, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_17": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 7.33563942015171, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.011034796499006915, "policy_loss": -0.048364955673605436, "vf_loss": 0.03415883957059122, "vf_explained_var": 0.8507565155625343, "kl": 0.010571065854334571, "entropy": 5.240207061171532, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_6": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.543966571986675, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03422489129297901, "policy_loss": -0.05405929483385989, "vf_loss": 0.016980286457692274, "vf_explained_var": 0.9069493930786848, "kl": 0.00951372370467014, "entropy": 5.760202223062516, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_8": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.342729923129082, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.037160383203627134, "policy_loss": -0.052698089287150654, "vf_loss": 0.013160168487229385, "vf_explained_var": 0.8765937477350235, "kl": 0.007925125464088422, "entropy": 5.856857439875602, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1040.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}}, "num_env_steps_sampled": 28000, "num_env_steps_trained": 28000, "num_agent_steps_sampled": 588000, "num_agent_steps_trained": 588000}, "env_runners": {"episode_reward_max": -42.70999999999981, "episode_reward_min": -75.40000000000191, "episode_reward_mean": -57.26944444444592, "episode_len_mean": 500.0, "episode_media": {}, "episodes_timesteps_total": 27000, "policy_reward_min": {"agent_0": -5.310000000000027, "agent_1": -5.380000000000028, "agent_2": -5.330000000000028, "agent_3": -5.350000000000028, "agent_4": -5.340000000000028, "agent_5": -5.350000000000028, "agent_6": -5.350000000000028, "agent_7": -5.340000000000028, "agent_8": -5.300000000000027, "agent_9": -5.330000000000028, "agent_10": -5.350000000000028, "agent_11": -5.340000000000028, "agent_12": -5.380000000000028, "agent_13": -5.330000000000028, "agent_14": -5.240000000000027, "agent_15": -5.300000000000027, "agent_16": -5.300000000000027, "agent_17": -5.3700000000000285, "agent_18": -5.380000000000028, "agent_19": -5.340000000000028, "agent_20": -5.350000000000028}, "policy_reward_max": {"agent_0": -0.5000000000000003, "agent_1": -0.5000000000000003, "agent_2": -0.5000000000000003, "agent_3": -0.5000000000000003, "agent_4": -0.5000000000000003, "agent_5": -0.5000000000000003, "agent_6": -0.5000000000000003, "agent_7": -0.1000000000000006, "agent_8": -0.5000000000000003, "agent_9": -0.5000000000000003, "agent_10": -0.5000000000000003, "agent_11": -0.5000000000000003, "agent_12": -0.5000000000000003, "agent_13": -0.5000000000000003, "agent_14": -3.590000000000009, "agent_15": -3.7300000000000106, "agent_16": -3.6400000000000095, "agent_17": -3.6300000000000097, "agent_18": -3.1700000000000044, "agent_19": -3.6300000000000106, "agent_20": -3.320000000000006}, "policy_reward_mean": {"agent_0": -1.8101851851851916, "agent_1": -1.39981481481482, "agent_2": -2.240555555555563, "agent_3": -2.378518518518529, "agent_4": -1.2592592592592637, "agent_5": -2.3359259259259355, "agent_6": -1.908888888888896, "agent_7": -1.8644444444444506, "agent_8": -1.5283333333333387, "agent_9": -1.3259259259259304, "agent_10": -2.299074074074083, "agent_11": -1.886481481481489, "agent_12": -1.5942592592592646, "agent_13": -1.9651851851851931, "agent_14": -4.5314814814815, "agent_15": -4.4888888888889085, "agent_16": -4.584259259259279, "agent_17": -4.5268518518518714, "agent_18": -4.433518518518537, "agent_19": -4.415185185185204, "agent_20": -4.492407407407427}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-53.290000000000305, -55.12000000000127, -61.81000000000211, -47.65000000000047, -57.68000000000135, -56.200000000001694, -52.780000000000356, -57.85000000000114, -54.040000000000376, -64.27000000000298, -49.57000000000024, -73.5400000000017, -62.680000000002885, -60.280000000003, -52.53000000000038, -58.05000000000256, -43.9300000000001, -59.7700000000019, -61.3400000000021, -52.9100000000017, -56.880000000001324, -62.430000000003005, -67.53000000000264, -58.4000000000019, -63.640000000003184, -48.380000000000244, -59.750000000001855, -54.76000000000095, -48.56000000000037, -72.73000000000226, -61.48000000000291, -54.10000000000039, -75.40000000000191, -57.9400000000012, -67.91000000000251, -57.94000000000153, -47.429999999999914, -52.13000000000102, -55.420000000001096, -57.8500000000018, -50.93000000000088, -64.32000000000227, -55.500000000001464, -42.70999999999981, -51.63000000000134, -56.65000000000114, -65.42000000000365, -51.810000000000436, -65.04000000000308, -62.730000000002164, -45.47999999999959, -51.710000000000335, -53.17000000000035, -59.50000000000128], "episode_lengths": [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500], "policy_agent_0_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.070000000000025, -5.240000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.110000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -2.5999999999999983, -5.210000000000027, -0.5000000000000003, -5.070000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.280000000000027, -5.210000000000027, -5.020000000000024, -0.5000000000000003, -5.090000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.8200000000000007, -3.770000000000011, -5.000000000000024, -5.120000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.140000000000004, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.310000000000027, -0.5000000000000003], "policy_agent_1_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.5500000000000087, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -5.360000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.920000000000023, -0.5000000000000003, -5.360000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.780000000000022, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.480000000000018, -0.5000000000000003, -0.5000000000000003, -5.380000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.590000000000019, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_2_reward": [-5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -3.500000000000008, -4.840000000000023, -2.129999999999993, -2.149999999999986, -5.130000000000026, -2.429999999999996, -0.5000000000000003, -0.5000000000000003, -4.67000000000002, -0.5000000000000003, -0.5000000000000003, -4.830000000000022, -5.060000000000025, -0.5000000000000003, -4.520000000000019, -0.5000000000000003, -4.780000000000022, -5.070000000000025, -0.5000000000000003, -5.170000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.330000000000028, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.160000000000026, -0.5000000000000003, -2.5499999999999976, -0.5000000000000003, -5.010000000000025, -0.5000000000000003, -2.579999999999998, -0.5000000000000003, -1.7599999999999938, -0.5900000000000004, -4.0700000000000145, -5.040000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.050000000000025], "policy_agent_3_reward": [-0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.050000000000025, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -5.160000000000026, -0.5000000000000003, -5.330000000000028, -5.240000000000027, -0.7300000000000005, -5.180000000000026, -4.590000000000019, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -5.190000000000026, -0.8900000000000007, -5.350000000000028, -5.320000000000028, -0.5000000000000003, -0.5000000000000003, -3.7500000000000107, -0.5000000000000003, -4.8200000000000225, -5.320000000000028, -4.660000000000021, -5.080000000000025, -0.5000000000000003, -0.5000000000000003, -5.010000000000025, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.330000000000028, -5.060000000000025, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003], "policy_agent_4_reward": [-0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.8100000000000116, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.110000000000025, -4.950000000000024, -5.270000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5000000000000003, -5.330000000000028], "policy_agent_5_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5000000000000003, -4.870000000000023, -5.350000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.589999999999998, -5.000000000000024, -2.930000000000002, -5.290000000000028, -0.5000000000000003, -5.090000000000026, -3.8100000000000116, -0.5000000000000003, -5.050000000000025, -5.300000000000027, -5.150000000000026, -3.6800000000000175, -5.110000000000025, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -5.000000000000024, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -4.980000000000024, -0.5000000000000003, -0.5000000000000003], "policy_agent_6_reward": [-5.310000000000027, -0.5000000000000003, -0.5000000000000003, -4.430000000000018, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -4.930000000000024, -5.030000000000024, -0.5000000000000003, -2.429999999999996, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.720000000000021, -5.330000000000028, -0.5000000000000003, -3.5700000000000087, -0.5000000000000003, -0.5000000000000003, -2.2799999999999945, -5.310000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.240000000000027, -0.5000000000000003, -0.5000000000000003, -5.350000000000028, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.310000000000027], "policy_agent_7_reward": [-0.5000000000000003, -2.349999999999995, -5.220000000000026, -2.2499999999999942, -5.040000000000025, -2.75, -5.300000000000027, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.0000000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.8999999999999926, -5.110000000000025, -5.330000000000028, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.890000000000012, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -0.1000000000000006, -4.890000000000023, -5.200000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.160000000000026, -0.5000000000000003, -0.5000000000000003], "policy_agent_8_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.040000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -4.54000000000002, -0.5000000000000003, -3.0900000000000034, -0.5000000000000003, -0.5000000000000003, -5.080000000000025, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -4.61000000000002, -0.5000000000000003, -0.5000000000000003, -3.390000000000007, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.710000000000021, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.260000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003], "policy_agent_9_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.790000000000011, -2.8200000000000007, -0.5000000000000003, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5600000000000004, -0.5000000000000003, -0.5000000000000003, -4.140000000000015, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.330000000000028, -3.3800000000000066, -0.5000000000000003, -5.120000000000026, -0.5000000000000003, -4.890000000000023, -0.5000000000000003, -0.5000000000000003, -5.260000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_10_reward": [-4.690000000000021, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.120000000000026, -4.930000000000024, -5.350000000000028, -0.5000000000000003, -2.119999999999993, -0.5000000000000003, -4.920000000000023, -2.6199999999999983, -0.5000000000000003, -3.4700000000000077, -4.560000000000019, -4.910000000000023, -4.030000000000014, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -0.5000000000000003, -4.910000000000023, -0.9700000000000008, -0.5000000000000003, -5.050000000000025, -0.5000000000000003, -5.310000000000027, -4.850000000000023, -0.5000000000000003, -5.030000000000024, -4.970000000000024, -5.130000000000026, -4.9000000000000234, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.130000000000026, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -1.1199999999999997, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_11_reward": [-0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.3699999999999974, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -5.330000000000028, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -5.320000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.180000000000026, -5.230000000000027, -5.220000000000026, -4.380000000000018, -5.290000000000028, -4.800000000000022, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.5499999999999956, -0.5000000000000003, -4.170000000000015, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -5.200000000000027, -0.5000000000000003], "policy_agent_12_reward": [-0.5000000000000003, -5.240000000000027, -5.090000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.170000000000027, -0.5000000000000003, -0.5000000000000003, -2.139999999999993, -0.5000000000000003, -0.5000000000000003, -4.550000000000019, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.3399999999999954, -0.5000000000000003, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -5.380000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -4.870000000000023, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.270000000000027, -3.970000000000013, -0.5000000000000003, -1.5799999999999954, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.160000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_13_reward": [-0.5000000000000003, -5.220000000000026, -5.270000000000027, -0.5000000000000003, -0.9600000000000007, -3.2600000000000056, -5.110000000000025, -5.260000000000027, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -5.280000000000027, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.210000000000027, -4.920000000000023, -0.5000000000000003, -0.5000000000000003, -5.030000000000024, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.210000000000027, -0.5000000000000003, -0.5000000000000003, -1.2699999999999982, -0.5000000000000003, -0.7400000000000005, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.180000000000026, -0.5000000000000003, -5.330000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.030000000000024], "policy_agent_14_reward": [-4.680000000000021, -4.600000000000021, -4.260000000000016, -4.000000000000013, -4.690000000000021, -4.530000000000019, -5.240000000000027, -4.8600000000000225, -5.150000000000026, -5.220000000000026, -3.780000000000011, -4.090000000000014, -5.000000000000024, -4.970000000000024, -4.61000000000002, -5.000000000000024, -4.160000000000014, -4.8200000000000225, -4.710000000000021, -4.420000000000018, -4.360000000000017, -3.680000000000009, -3.590000000000009, -4.400000000000017, -4.350000000000017, -4.64000000000002, -4.57000000000002, -4.290000000000017, -3.850000000000012, -3.9100000000000126, -4.180000000000017, -4.980000000000024, -4.200000000000015, -3.950000000000013, -4.870000000000023, -3.66000000000001, -5.130000000000026, -4.090000000000015, -5.220000000000026, -4.2300000000000155, -3.920000000000013, -5.060000000000025, -4.740000000000022, -4.8200000000000225, -4.2300000000000155, -4.090000000000014, -4.480000000000018, -5.100000000000025, -4.800000000000023, -5.080000000000025, -5.130000000000026, -5.060000000000025, -4.380000000000018, -4.870000000000023], "policy_agent_15_reward": [-5.140000000000025, -4.400000000000018, -4.830000000000022, -4.520000000000019, -5.040000000000025, -3.950000000000013, -4.2300000000000155, -4.700000000000021, -3.770000000000011, -3.970000000000013, -4.950000000000024, -4.110000000000015, -4.560000000000019, -4.470000000000018, -4.120000000000014, -4.300000000000017, -3.8200000000000114, -4.240000000000015, -4.520000000000019, -4.480000000000018, -4.700000000000021, -4.030000000000014, -4.260000000000016, -4.490000000000019, -5.110000000000025, -4.130000000000015, -3.970000000000013, -4.710000000000021, -4.000000000000013, -4.960000000000024, -4.0700000000000145, -4.210000000000016, -4.6200000000000205, -5.040000000000025, -4.510000000000019, -4.410000000000018, -5.200000000000027, -5.290000000000028, -4.58000000000002, -3.7300000000000106, -4.730000000000022, -4.2300000000000155, -5.300000000000027, -4.52000000000002, -4.730000000000023, -4.360000000000017, -4.760000000000022, -4.130000000000015, -4.080000000000014, -4.650000000000021, -4.690000000000021, -3.8800000000000123, -5.240000000000027, -4.960000000000024], "policy_agent_16_reward": [-4.950000000000024, -5.150000000000026, -4.950000000000024, -4.140000000000015, -5.220000000000026, -4.180000000000016, -5.280000000000027, -4.750000000000021, -5.140000000000026, -4.530000000000019, -4.410000000000018, -4.720000000000021, -4.800000000000023, -4.120000000000014, -4.63000000000002, -4.67000000000002, -5.100000000000025, -4.470000000000018, -4.700000000000021, -4.240000000000016, -4.450000000000019, -4.440000000000018, -5.250000000000027, -5.180000000000026, -4.010000000000013, -4.250000000000016, -5.010000000000025, -4.590000000000019, -4.090000000000014, -4.560000000000019, -3.790000000000011, -5.230000000000027, -4.080000000000014, -4.500000000000019, -5.080000000000025, -5.020000000000024, -5.050000000000025, -4.120000000000014, -5.300000000000027, -4.470000000000018, -4.510000000000018, -5.000000000000024, -4.790000000000022, -4.280000000000016, -4.3100000000000165, -4.770000000000023, -4.1100000000000145, -3.920000000000013, -4.170000000000015, -4.960000000000024, -4.510000000000019, -3.6400000000000095, -3.7300000000000106, -4.2300000000000155], "policy_agent_17_reward": [-4.780000000000021, -4.790000000000022, -3.9800000000000133, -4.3800000000000185, -4.340000000000017, -4.750000000000021, -3.8100000000000116, -4.54000000000002, -5.190000000000026, -5.100000000000025, -4.140000000000015, -4.140000000000015, -4.260000000000016, -4.250000000000016, -5.340000000000028, -4.330000000000017, -4.950000000000024, -4.61000000000002, -5.3700000000000285, -4.490000000000019, -4.140000000000015, -4.730000000000022, -4.520000000000019, -4.64000000000002, -4.210000000000016, -3.6300000000000097, -3.9000000000000123, -4.180000000000016, -4.990000000000023, -4.4300000000000175, -5.220000000000027, -3.970000000000013, -5.090000000000026, -4.2700000000000164, -4.650000000000021, -5.040000000000025, -4.200000000000015, -3.760000000000011, -5.230000000000027, -4.660000000000021, -3.760000000000011, -5.310000000000027, -4.320000000000017, -4.960000000000024, -3.670000000000011, -4.970000000000025, -4.7700000000000236, -4.410000000000018, -5.340000000000028, -4.060000000000014, -4.560000000000019, -4.660000000000021, -4.100000000000015, -4.560000000000019], "policy_agent_18_reward": [-5.280000000000027, -4.150000000000015, -3.9000000000000123, -5.290000000000028, -4.800000000000022, -5.180000000000027, -3.5300000000000082, -3.780000000000011, -4.720000000000021, -4.720000000000021, -4.460000000000019, -5.290000000000028, -4.770000000000022, -4.990000000000024, -4.60000000000002, -4.550000000000019, -4.430000000000018, -3.5700000000000087, -4.990000000000024, -3.5700000000000074, -4.290000000000017, -5.150000000000026, -4.7400000000000215, -4.400000000000018, -4.170000000000015, -4.330000000000017, -3.940000000000013, -4.760000000000022, -4.090000000000014, -5.120000000000027, -3.9000000000000123, -5.100000000000025, -5.220000000000026, -4.480000000000018, -4.0700000000000145, -4.3100000000000165, -3.880000000000013, -4.120000000000014, -4.910000000000023, -4.240000000000016, -3.1700000000000044, -3.9300000000000126, -3.5600000000000085, -3.9000000000000123, -3.780000000000011, -4.660000000000021, -3.68000000000001, -3.850000000000012, -4.170000000000015, -5.060000000000025, -5.380000000000028, -4.060000000000014, -5.060000000000025, -5.360000000000028], "policy_agent_19_reward": [-3.9800000000000133, -4.020000000000014, -5.090000000000026, -3.75000000000001, -4.370000000000019, -4.60000000000002, -5.110000000000025, -4.9400000000000235, -4.560000000000019, -4.220000000000016, -4.0700000000000145, -5.040000000000025, -3.8100000000000116, -4.5400000000000205, -3.9600000000000133, -4.130000000000014, -3.8300000000000116, -4.530000000000019, -4.170000000000015, -4.760000000000022, -4.410000000000018, -4.030000000000014, -5.290000000000028, -4.450000000000019, -5.240000000000028, -4.030000000000014, -4.63000000000002, -3.7000000000000104, -4.180000000000016, -5.340000000000028, -4.200000000000015, -4.870000000000023, -5.270000000000027, -4.960000000000024, -4.610000000000021, -4.710000000000022, -3.790000000000011, -3.6300000000000106, -3.9800000000000133, -3.950000000000013, -4.64000000000002, -4.64000000000002, -3.820000000000012, -3.8000000000000114, -4.180000000000016, -4.340000000000017, -4.370000000000019, -4.480000000000018, -4.040000000000013, -3.970000000000013, -5.020000000000024, -5.210000000000027, -4.650000000000021, -4.510000000000019], "policy_agent_20_reward": [-3.7300000000000106, -4.9000000000000234, -4.2300000000000155, -4.240000000000016, -3.6700000000000097, -4.660000000000021, -4.500000000000019, -4.350000000000017, -4.460000000000019, -4.570000000000021, -4.550000000000019, -5.090000000000026, -4.360000000000017, -4.100000000000014, -4.100000000000015, -4.020000000000013, -4.090000000000014, -4.690000000000021, -5.050000000000025, -5.330000000000028, -4.810000000000022, -5.260000000000027, -4.920000000000023, -4.2700000000000164, -3.9900000000000135, -4.170000000000015, -5.300000000000027, -4.510000000000019, -3.870000000000012, -4.050000000000014, -3.8900000000000134, -4.800000000000022, -4.2700000000000164, -5.220000000000027, -5.320000000000028, -4.380000000000018, -4.360000000000017, -4.1100000000000145, -3.770000000000011, -3.320000000000006, -4.780000000000022, -5.350000000000028, -4.260000000000016, -4.910000000000023, -4.1900000000000155, -4.090000000000014, -4.090000000000014, -4.690000000000022, -5.120000000000026, -4.930000000000024, -4.500000000000019, -4.260000000000016, -4.850000000000023, -5.290000000000028]}, "sampler_perf": {"mean_raw_obs_processing_ms": 1.9618895834229546, "mean_inference_ms": 9.233842421916004, "mean_action_processing_ms": 0.7261569513868023, "mean_env_wait_ms": 2.8036005051567403, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"ObsPreprocessorConnector_ms": 0.0021154287630918795, "StateBufferConnector_ms": 0.0014726657295563443, "ViewRequirementAgentConnector_ms": 0.03310092963028627}, "num_episodes": 8, "episode_return_max": -42.70999999999981, "episode_return_min": -75.40000000000191, "episode_return_mean": -57.26944444444592, "episodes_this_iter": 8}, "num_healthy_workers": 3, "actor_manager_num_outstanding_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 588000, "num_agent_steps_trained": 588000, "num_env_steps_sampled": 28000, "num_env_steps_trained": 28000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 132.9673298238029, "num_env_steps_trained_throughput_per_sec": 132.9673298238029, "timesteps_total": 28000, "num_env_steps_sampled_lifetime": 28000, "num_agent_steps_sampled_lifetime": 588000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 588000, "timers": {"training_iteration_time_ms": 30163.444, "restore_workers_time_ms": 0.015, "training_step_time_ms": 30163.397, "sample_time_ms": 20139.678, "learn_time_ms": 10000.918, "learn_throughput": 399.963, "synch_weights_time_ms": 20.262}, "counters": {"num_env_steps_sampled": 28000, "num_env_steps_trained": 28000, "num_agent_steps_sampled": 588000, "num_agent_steps_trained": 588000}, "done": false, "training_iteration": 7, "trial_id": "cfc88_00000", "date": "2025-10-21_11-21-28", "timestamp": 1761038488, "time_this_iter_s": 30.09853982925415, "time_total_s": 211.2338514328003, "pid": 3279004, "hostname": "xuezhi-Precision-3660", "node_ip": "130.238.16.41", "config": {"exploration_config": {"type": "StochasticSampling"}, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "placement_strategy": "PACK", "num_gpus": 1, "_fake_gpus": false, "num_cpus_for_main_process": 1, "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "torch_ddp_kwargs": {}, "torch_skip_nan_gradients": false, "env": "<class 'train_utils.WarehouseMultiAgentEnv'>", "env_config": {"env_id": "tarware-extralarge-14agvs-7pickers-partialobs-chg-v1"}, "observation_space": null, "action_space": null, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "_is_atari": null, "disable_env_checking": false, "render_env": true, "action_mask_key": "action_mask", "env_runner_cls": null, "num_env_runners": 3, "create_local_env_runner": true, "num_envs_per_env_runner": 1, "gym_env_vectorize_mode": "SYNC", "num_cpus_per_env_runner": 1, "num_gpus_per_env_runner": 0, "custom_resources_per_env_runner": {}, "validate_env_runners_after_construction": true, "episodes_to_numpy": true, "max_requests_in_flight_per_env_runner": 1, "sample_timeout_s": 60.0, "_env_to_module_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "_module_to_env_connector": null, "add_default_connectors_to_module_to_env_pipeline": true, "merge_env_runner_states": "training_only", "broadcast_env_runner_states": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "compress_observations": false, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "sampler_perf_stats_ema_coef": null, "_is_online": true, "num_learners": 0, "num_gpus_per_learner": 1, "num_cpus_per_learner": "auto", "num_aggregator_actors_per_learner": 0, "max_requests_in_flight_per_aggregator_actor": 3, "local_gpu_idx": 0, "max_requests_in_flight_per_learner": 3, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "_train_batch_size_per_learner": null, "train_batch_size": 4000, "num_epochs": 10, "minibatch_size": 256, "shuffle_batch_per_epoch": true, "model": {"fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "log_std_clip_param": 20.0, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1, "_disable_preprocessor_api": false, "_disable_action_flattening": false}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "learner_config_dict": {}, "optimizer": {}, "_learner_class": null, "callbacks_on_algorithm_init": null, "callbacks_on_env_runners_recreated": null, "callbacks_on_offline_eval_runners_recreated": null, "callbacks_on_checkpoint_loaded": null, "callbacks_on_environment_created": null, "callbacks_on_episode_created": null, "callbacks_on_episode_start": null, "callbacks_on_episode_step": null, "callbacks_on_episode_end": null, "callbacks_on_evaluate_start": null, "callbacks_on_evaluate_end": null, "callbacks_on_evaluate_offline_start": null, "callbacks_on_evaluate_offline_end": null, "callbacks_on_sample_end": null, "callbacks_on_train_result": null, "explore": true, "enable_rl_module_and_learner": false, "enable_env_runner_and_connector_v2": false, "_prior_exploration_config": null, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function train.<locals>.<lambda> at 0x75979fd64940>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "offline_data_class": null, "input_read_method": "read_parquet", "input_read_method_kwargs": {}, "input_read_schema": {}, "input_read_episodes": false, "input_read_sample_batches": false, "input_read_batch_size": null, "input_filesystem": null, "input_filesystem_kwargs": {}, "input_compress_columns": ["obs", "new_obs"], "input_spaces_jsonable": true, "materialize_data": false, "materialize_mapped_data": true, "map_batches_kwargs": {}, "iter_batches_kwargs": {}, "ignore_final_observation": false, "prelearner_class": null, "prelearner_buffer_class": null, "prelearner_buffer_kwargs": {}, "prelearner_module_synch_period": 10, "dataset_num_iters_per_learner": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "output_max_rows_per_file": null, "output_write_remaining_data": false, "output_write_method": "write_parquet", "output_write_method_kwargs": {}, "output_filesystem": null, "output_filesystem_kwargs": {}, "output_write_episodes": true, "offline_sampling": false, "evaluation_interval": 10, "evaluation_duration": 5, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 120.0, "evaluation_auto_duration_min_env_steps_per_sample": 100, "evaluation_auto_duration_max_env_steps_per_sample": 2000, "evaluation_parallel_to_training": false, "evaluation_force_reset_envs_before_iteration": true, "evaluation_config": {"explore": false}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_env_runners": 0, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 10.0, "offline_evaluation_interval": null, "num_offline_eval_runners": 0, "offline_evaluation_type": null, "offline_eval_runner_class": null, "offline_loss_for_module_fn": null, "offline_evaluation_duration": 1, "offline_evaluation_parallel_to_training": false, "offline_evaluation_timeout_s": 120.0, "num_cpus_per_offline_eval_runner": 1, "num_gpus_per_offline_eval_runner": 0, "custom_resources_per_offline_eval_runner": {}, "restart_failed_offline_eval_runners": true, "ignore_offline_eval_runner_failures": false, "max_num_offline_eval_runner_restarts": 1000, "offline_eval_runner_restore_timeout_s": 1800.0, "max_requests_in_flight_per_offline_eval_runner": 1, "validate_offline_eval_runners_after_construction": true, "offline_eval_runner_health_probe_timeout_s": 30.0, "offline_eval_rl_module_inference_only": false, "broadcast_offline_eval_runner_states": false, "offline_eval_batch_size_per_runner": 256, "dataset_num_iters_per_eval_runner": 1, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "log_gradients": false, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "restart_failed_env_runners": true, "ignore_env_runner_failures": false, "max_num_env_runner_restarts": 1000, "delay_between_env_runner_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_env_runner_failures_tolerance": 100, "env_runner_health_probe_timeout_s": 30.0, "env_runner_restore_timeout_s": 1800.0, "_model_config": {}, "_rl_module_spec": null, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "_validate_config": true, "_use_msgpack_checkpoints": false, "_torch_grad_scaler_class": null, "_torch_lr_scheduler_classes": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "_dont_auto_sync_env_runner_states": false, "env_task_fn": -1, "enable_connectors": -1, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "enable_async_evaluation": -1, "custom_async_evaluation_function": -1, "_enable_rl_module_api": -1, "auto_wrap_old_gym_envs": -1, "always_attach_evaluation_results": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "clip_param": 0.2, "vf_clip_param": 10.0, "entropy_coeff_schedule": null, "lr_schedule": null, "sgd_minibatch_size": -1, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 0.95, "input": "sampler", "policies": {"agent_0": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_1": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_2": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_3": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_4": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_5": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_6": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_7": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_8": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_9": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_10": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_11": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_12": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_13": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_14": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_15": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_16": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_17": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_18": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_19": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_20": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}]}, "callbacks": "<class 'ray.rllib.callbacks.callbacks.RLlibCallback'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch"}, "time_since_restore": 211.2338514328003, "iterations_since_restore": 7, "perf": {"cpu_util_percent": 8.938095238095238, "ram_util_percent": 27.388095238095243, "gpu_util_percent0": 0.20500000000000002, "vram_util_percent0": 0.09623025565868752}}
{"custom_metrics": {}, "episode_media": {}, "info": {"learner": {"agent_4": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.27435477077961, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03919571615406312, "policy_loss": -0.05689721288981673, "vf_loss": 0.014722753199748695, "vf_explained_var": 0.8669820241630077, "kl": 0.009929144036643644, "entropy": 5.7946094840765, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_19": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.654473097622395, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.01829388996848138, "policy_loss": -0.05705977639881894, "vf_loss": 0.03513068134197965, "vf_explained_var": 0.8814147207885981, "kl": 0.012117353643884398, "entropy": 5.325803378224373, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.515736643970013, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03793888926738873, "policy_loss": -0.0548939366126433, "vf_loss": 0.01411656561831478, "vf_explained_var": 0.8884301900863647, "kl": 0.009461605751494063, "entropy": 5.675506186485291, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_16": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.57918601334095, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.021074323751963674, "policy_loss": -0.052435102424351496, "vf_loss": 0.02799900193931535, "vf_explained_var": 0.9284684631973505, "kl": 0.01120592268823411, "entropy": 4.967221072316169, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_9": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.7023138493299483, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03874238077187329, "policy_loss": -0.05558601157390512, "vf_loss": 0.014168791667907498, "vf_explained_var": 0.9103568252176046, "kl": 0.00891613341966604, "entropy": 5.841108760237693, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_5": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.675074252486229, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03783859912218759, "policy_loss": -0.05652661588974297, "vf_loss": 0.015597756329225377, "vf_explained_var": 0.9130995783954858, "kl": 0.010300866780147508, "entropy": 5.719193252921104, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_7": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.625849723815918, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.040190104070643426, "policy_loss": -0.058324402512516825, "vf_loss": 0.015317205374594777, "vf_explained_var": 0.9142678182572126, "kl": 0.00939030706496013, "entropy": 5.773246321082115, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_14": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.846854381263256, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.027528344353777358, "policy_loss": -0.058688712696312, "vf_loss": 0.02801763403695077, "vf_explained_var": 0.884218380227685, "kl": 0.010475777661349639, "entropy": 5.357658204436302, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_18": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.31404365003109, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.012932868865027558, "policy_loss": -0.05362083104300837, "vf_loss": 0.03724202266312204, "vf_explained_var": 0.8813325192779302, "kl": 0.01148646541687478, "entropy": 5.334138494729996, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_12": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.00693814009428, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03315515533176949, "policy_loss": -0.0535340238653589, "vf_loss": 0.017692832680768333, "vf_explained_var": 0.9362706527113914, "kl": 0.008953453585845806, "entropy": 5.859077689051628, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_11": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.638269133865833, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03115186410723254, "policy_loss": -0.054295233538141474, "vf_loss": 0.020209557918133215, "vf_explained_var": 0.9029571861028671, "kl": 0.009779369945704275, "entropy": 5.7416515946388245, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_13": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.7290082171559336, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.040882586962834463, "policy_loss": -0.05751680470857536, "vf_loss": 0.013717095518950373, "vf_explained_var": 0.9289140179753304, "kl": 0.00972374049104019, "entropy": 5.778705689311027, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.032215884327888, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03654330282151932, "policy_loss": -0.05443782145739533, "vf_loss": 0.015378162276465445, "vf_explained_var": 0.9041794277727604, "kl": 0.00838785645591843, "entropy": 5.804823938012123, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_20": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 7.15870848596096, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.015152843453688548, "policy_loss": -0.050455603099544534, "vf_loss": 0.031815541168907654, "vf_explained_var": 0.8620479710400104, "kl": 0.011624062742529962, "entropy": 5.332396367192269, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.9179382964968683, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03302600759634515, "policy_loss": -0.05225304429186508, "vf_loss": 0.016750839474843814, "vf_explained_var": 0.8953320283442736, "kl": 0.008253992405073084, "entropy": 5.815165174007416, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_10": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.8071965858340264, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.037076794341555794, "policy_loss": -0.05503963542287238, "vf_loss": 0.01539252239454072, "vf_explained_var": 0.8739301301538944, "kl": 0.008567728373100925, "entropy": 5.817234811186791, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.266221144795418, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.040216615203826224, "policy_loss": -0.057345081036328335, "vf_loss": 0.014071227124077268, "vf_explained_var": 0.8637577179819346, "kl": 0.010190794804280113, "entropy": 5.722878226637841, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_15": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.168573325872421, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.01311155092989793, "policy_loss": -0.051842302390286935, "vf_loss": 0.03504350777366198, "vf_explained_var": 0.8882319793105126, "kl": 0.01229081323729624, "entropy": 5.389135232567787, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_17": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.81778549104929, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.020756771085143556, "policy_loss": -0.053438212290711815, "vf_loss": 0.029419742239406334, "vf_explained_var": 0.9131636191159487, "kl": 0.01087232949127102, "entropy": 5.1456157773733135, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_6": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.578842411935329, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.04344482652086299, "policy_loss": -0.060397295508300884, "vf_loss": 0.01394077845616266, "vf_explained_var": 0.9456783141940832, "kl": 0.01003896178037047, "entropy": 5.731501612067222, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_8": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.4842693120241166, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03813019516019267, "policy_loss": -0.0540518690351746, "vf_loss": 0.0134361551143229, "vf_explained_var": 0.8921211443841457, "kl": 0.00828506000219369, "entropy": 5.826079502701759, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}}, "num_env_steps_sampled": 32000, "num_env_steps_trained": 32000, "num_agent_steps_sampled": 672000, "num_agent_steps_trained": 672000}, "env_runners": {"episode_reward_max": -42.70999999999981, "episode_reward_min": -75.40000000000191, "episode_reward_mean": -57.36015873016024, "episode_len_mean": 500.0, "episode_media": {}, "episodes_timesteps_total": 31500, "policy_reward_min": {"agent_0": -5.3700000000000285, "agent_1": -5.380000000000028, "agent_2": -5.330000000000028, "agent_3": -5.350000000000028, "agent_4": -5.340000000000028, "agent_5": -5.350000000000028, "agent_6": -5.360000000000028, "agent_7": -5.340000000000028, "agent_8": -5.300000000000027, "agent_9": -5.330000000000028, "agent_10": -5.350000000000028, "agent_11": -5.340000000000028, "agent_12": -5.380000000000028, "agent_13": -5.330000000000028, "agent_14": -5.320000000000028, "agent_15": -5.300000000000027, "agent_16": -5.300000000000027, "agent_17": -5.3700000000000285, "agent_18": -5.380000000000028, "agent_19": -5.340000000000028, "agent_20": -5.350000000000028}, "policy_reward_max": {"agent_0": -0.5000000000000003, "agent_1": -0.5000000000000003, "agent_2": -0.5000000000000003, "agent_3": -0.5000000000000003, "agent_4": -0.5000000000000003, "agent_5": -0.5000000000000003, "agent_6": -0.5000000000000003, "agent_7": -0.1000000000000006, "agent_8": -0.5000000000000003, "agent_9": -0.5000000000000003, "agent_10": -0.5000000000000003, "agent_11": -0.5000000000000003, "agent_12": -0.5000000000000003, "agent_13": -0.5000000000000003, "agent_14": -3.590000000000009, "agent_15": -3.7300000000000106, "agent_16": -3.6400000000000095, "agent_17": -3.400000000000006, "agent_18": -3.1700000000000044, "agent_19": -3.4100000000000072, "agent_20": -3.320000000000006}, "policy_reward_mean": {"agent_0": -2.001269841269849, "agent_1": -1.5695238095238153, "agent_2": -2.4466666666666756, "agent_3": -2.452539682539693, "agent_4": -1.25587301587302, "agent_5": -2.1504761904761995, "agent_6": -2.0087301587301667, "agent_7": -1.889206349206356, "agent_8": -1.3814285714285761, "agent_9": -1.2657142857142891, "agent_10": -2.0420634920635, "agent_11": -1.981269841269849, "agent_12": -1.5117460317460367, "agent_13": -2.007460317460325, "agent_14": -4.518253968253989, "agent_15": -4.47158730158732, "agent_16": -4.583809523809542, "agent_17": -4.50079365079367, "agent_18": -4.454444444444463, "agent_19": -4.396507936507955, "agent_20": -4.470793650793669}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-53.290000000000305, -55.12000000000127, -61.81000000000211, -47.65000000000047, -57.68000000000135, -56.200000000001694, -52.780000000000356, -57.85000000000114, -54.040000000000376, -64.27000000000298, -49.57000000000024, -73.5400000000017, -62.680000000002885, -60.280000000003, -52.53000000000038, -58.05000000000256, -43.9300000000001, -59.7700000000019, -61.3400000000021, -52.9100000000017, -56.880000000001324, -62.430000000003005, -67.53000000000264, -58.4000000000019, -63.640000000003184, -48.380000000000244, -59.750000000001855, -54.76000000000095, -48.56000000000037, -72.73000000000226, -61.48000000000291, -54.10000000000039, -75.40000000000191, -57.9400000000012, -67.91000000000251, -57.94000000000153, -47.429999999999914, -52.13000000000102, -55.420000000001096, -57.8500000000018, -50.93000000000088, -64.32000000000227, -55.500000000001464, -42.70999999999981, -51.63000000000134, -56.65000000000114, -65.42000000000365, -51.810000000000436, -65.04000000000308, -62.730000000002164, -45.47999999999959, -51.710000000000335, -53.17000000000035, -59.50000000000128, -59.81000000000211, -61.23000000000282, -67.91000000000287, -44.25999999999966, -50.24000000000036, -55.640000000001045, -60.08000000000257, -61.84000000000216, -60.130000000002866], "episode_lengths": [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500], "policy_agent_0_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.070000000000025, -5.240000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.110000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -2.5999999999999983, -5.210000000000027, -0.5000000000000003, -5.070000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.280000000000027, -5.210000000000027, -5.020000000000024, -0.5000000000000003, -5.090000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.8200000000000007, -3.770000000000011, -5.000000000000024, -5.120000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.140000000000004, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -5.3700000000000285, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.230000000000027, -0.5000000000000003, -5.240000000000027, -5.270000000000027, -5.220000000000026], "policy_agent_1_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.5500000000000087, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -5.360000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.920000000000023, -0.5000000000000003, -5.360000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.780000000000022, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.480000000000018, -0.5000000000000003, -0.5000000000000003, -5.380000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.590000000000019, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5100000000000003, -0.5000000000000003, -5.320000000000028, -0.5000000000000003, -5.130000000000026, -0.5000000000000003, -5.010000000000025, -0.5000000000000003, -5.320000000000028], "policy_agent_2_reward": [-5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -3.500000000000008, -4.840000000000023, -2.129999999999993, -2.149999999999986, -5.130000000000026, -2.429999999999996, -0.5000000000000003, -0.5000000000000003, -4.67000000000002, -0.5000000000000003, -0.5000000000000003, -4.830000000000022, -5.060000000000025, -0.5000000000000003, -4.520000000000019, -0.5000000000000003, -4.780000000000022, -5.070000000000025, -0.5000000000000003, -5.170000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.330000000000028, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.160000000000026, -0.5000000000000003, -2.5499999999999976, -0.5000000000000003, -5.010000000000025, -0.5000000000000003, -2.579999999999998, -0.5000000000000003, -1.7599999999999938, -0.5900000000000004, -4.0700000000000145, -5.040000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.050000000000025, -5.220000000000026, -4.990000000000024, -5.260000000000027, -0.5000000000000003, -1.4699999999999964, -5.270000000000027, -5.320000000000028, -4.6200000000000205, -0.5000000000000003], "policy_agent_3_reward": [-0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.050000000000025, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -5.160000000000026, -0.5000000000000003, -5.330000000000028, -5.240000000000027, -0.7300000000000005, -5.180000000000026, -4.590000000000019, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -5.190000000000026, -0.8900000000000007, -5.350000000000028, -5.320000000000028, -0.5000000000000003, -0.5000000000000003, -3.7500000000000107, -0.5000000000000003, -4.8200000000000225, -5.320000000000028, -4.660000000000021, -5.080000000000025, -0.5000000000000003, -0.5000000000000003, -5.010000000000025, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.330000000000028, -5.060000000000025, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.230000000000027, -5.060000000000025, -4.400000000000018, -0.5000000000000003, -4.200000000000015, -0.5000000000000003, -0.5000000000000003, -5.180000000000026], "policy_agent_4_reward": [-0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.8100000000000116, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.110000000000025, -4.950000000000024, -5.270000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5000000000000003, -5.330000000000028, -0.5000000000000003, -2.3699999999999886, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003], "policy_agent_5_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5000000000000003, -4.870000000000023, -5.350000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.589999999999998, -5.000000000000024, -2.930000000000002, -5.290000000000028, -0.5000000000000003, -5.090000000000026, -3.8100000000000116, -0.5000000000000003, -5.050000000000025, -5.300000000000027, -5.150000000000026, -3.6800000000000175, -5.110000000000025, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -5.000000000000024, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -4.980000000000024, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003], "policy_agent_6_reward": [-5.310000000000027, -0.5000000000000003, -0.5000000000000003, -4.430000000000018, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -4.930000000000024, -5.030000000000024, -0.5000000000000003, -2.429999999999996, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.720000000000021, -5.330000000000028, -0.5000000000000003, -3.5700000000000087, -0.5000000000000003, -0.5000000000000003, -2.2799999999999945, -5.310000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.240000000000027, -0.5000000000000003, -0.5000000000000003, -5.350000000000028, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.310000000000027, -5.250000000000027, -5.360000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.030000000000024, -0.5000000000000003, -5.330000000000028, -0.5000000000000003], "policy_agent_7_reward": [-0.5000000000000003, -2.349999999999995, -5.220000000000026, -2.2499999999999942, -5.040000000000025, -2.75, -5.300000000000027, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.0000000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.8999999999999926, -5.110000000000025, -5.330000000000028, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.890000000000012, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -0.1000000000000006, -4.890000000000023, -5.200000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.160000000000026, -0.5000000000000003, -0.5000000000000003, -5.180000000000026, -4.960000000000024, -5.200000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_8_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.040000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -4.54000000000002, -0.5000000000000003, -3.0900000000000034, -0.5000000000000003, -0.5000000000000003, -5.080000000000025, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -4.61000000000002, -0.5000000000000003, -0.5000000000000003, -3.390000000000007, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.710000000000021, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.260000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_9_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.790000000000011, -2.8200000000000007, -0.5000000000000003, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5600000000000004, -0.5000000000000003, -0.5000000000000003, -4.140000000000015, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.330000000000028, -3.3800000000000066, -0.5000000000000003, -5.120000000000026, -0.5000000000000003, -4.890000000000023, -0.5000000000000003, -0.5000000000000003, -5.260000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.9199999999999924, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.7199999999999993], "policy_agent_10_reward": [-4.690000000000021, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.120000000000026, -4.930000000000024, -5.350000000000028, -0.5000000000000003, -2.119999999999993, -0.5000000000000003, -4.920000000000023, -2.6199999999999983, -0.5000000000000003, -3.4700000000000077, -4.560000000000019, -4.910000000000023, -4.030000000000014, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -0.5000000000000003, -4.910000000000023, -0.9700000000000008, -0.5000000000000003, -5.050000000000025, -0.5000000000000003, -5.310000000000027, -4.850000000000023, -0.5000000000000003, -5.030000000000024, -4.970000000000024, -5.130000000000026, -4.9000000000000234, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.130000000000026, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -1.1199999999999997, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_11_reward": [-0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.3699999999999974, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -5.330000000000028, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -5.320000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.180000000000026, -5.230000000000027, -5.220000000000026, -4.380000000000018, -5.290000000000028, -4.800000000000022, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.5499999999999956, -0.5000000000000003, -4.170000000000015, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -5.200000000000027, -0.5000000000000003, -0.5000000000000003, -5.070000000000025, -5.240000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.810000000000022, -5.330000000000028], "policy_agent_12_reward": [-0.5000000000000003, -5.240000000000027, -5.090000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.170000000000027, -0.5000000000000003, -0.5000000000000003, -2.139999999999993, -0.5000000000000003, -0.5000000000000003, -4.550000000000019, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.3399999999999954, -0.5000000000000003, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -5.380000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -4.870000000000023, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.270000000000027, -3.970000000000013, -0.5000000000000003, -1.5799999999999954, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.160000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_13_reward": [-0.5000000000000003, -5.220000000000026, -5.270000000000027, -0.5000000000000003, -0.9600000000000007, -3.2600000000000056, -5.110000000000025, -5.260000000000027, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -5.280000000000027, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.210000000000027, -4.920000000000023, -0.5000000000000003, -0.5000000000000003, -5.030000000000024, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.210000000000027, -0.5000000000000003, -0.5000000000000003, -1.2699999999999982, -0.5000000000000003, -0.7400000000000005, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.180000000000026, -0.5000000000000003, -5.330000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.030000000000024, -4.310000000000027, -0.5000000000000003, -5.030000000000024, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.9799999999999918, -4.920000000000023, -2.1099999999999928], "policy_agent_14_reward": [-4.680000000000021, -4.600000000000021, -4.260000000000016, -4.000000000000013, -4.690000000000021, -4.530000000000019, -5.240000000000027, -4.8600000000000225, -5.150000000000026, -5.220000000000026, -3.780000000000011, -4.090000000000014, -5.000000000000024, -4.970000000000024, -4.61000000000002, -5.000000000000024, -4.160000000000014, -4.8200000000000225, -4.710000000000021, -4.420000000000018, -4.360000000000017, -3.680000000000009, -3.590000000000009, -4.400000000000017, -4.350000000000017, -4.64000000000002, -4.57000000000002, -4.290000000000017, -3.850000000000012, -3.9100000000000126, -4.180000000000017, -4.980000000000024, -4.200000000000015, -3.950000000000013, -4.870000000000023, -3.66000000000001, -5.130000000000026, -4.090000000000015, -5.220000000000026, -4.2300000000000155, -3.920000000000013, -5.060000000000025, -4.740000000000022, -4.8200000000000225, -4.2300000000000155, -4.090000000000014, -4.480000000000018, -5.100000000000025, -4.800000000000023, -5.080000000000025, -5.130000000000026, -5.060000000000025, -4.380000000000018, -4.870000000000023, -4.1100000000000145, -4.140000000000015, -4.550000000000019, -4.510000000000019, -5.320000000000028, -4.650000000000021, -3.590000000000009, -5.150000000000026, -3.9300000000000126], "policy_agent_15_reward": [-5.140000000000025, -4.400000000000018, -4.830000000000022, -4.520000000000019, -5.040000000000025, -3.950000000000013, -4.2300000000000155, -4.700000000000021, -3.770000000000011, -3.970000000000013, -4.950000000000024, -4.110000000000015, -4.560000000000019, -4.470000000000018, -4.120000000000014, -4.300000000000017, -3.8200000000000114, -4.240000000000015, -4.520000000000019, -4.480000000000018, -4.700000000000021, -4.030000000000014, -4.260000000000016, -4.490000000000019, -5.110000000000025, -4.130000000000015, -3.970000000000013, -4.710000000000021, -4.000000000000013, -4.960000000000024, -4.0700000000000145, -4.210000000000016, -4.6200000000000205, -5.040000000000025, -4.510000000000019, -4.410000000000018, -5.200000000000027, -5.290000000000028, -4.58000000000002, -3.7300000000000106, -4.730000000000022, -4.2300000000000155, -5.300000000000027, -4.52000000000002, -4.730000000000023, -4.360000000000017, -4.760000000000022, -4.130000000000015, -4.080000000000014, -4.650000000000021, -4.690000000000021, -3.8800000000000123, -5.240000000000027, -4.960000000000024, -3.790000000000012, -3.9800000000000133, -4.55000000000002, -5.170000000000027, -5.190000000000026, -3.9600000000000133, -4.220000000000016, -4.370000000000017, -4.080000000000014], "policy_agent_16_reward": [-4.950000000000024, -5.150000000000026, -4.950000000000024, -4.140000000000015, -5.220000000000026, -4.180000000000016, -5.280000000000027, -4.750000000000021, -5.140000000000026, -4.530000000000019, -4.410000000000018, -4.720000000000021, -4.800000000000023, -4.120000000000014, -4.63000000000002, -4.67000000000002, -5.100000000000025, -4.470000000000018, -4.700000000000021, -4.240000000000016, -4.450000000000019, -4.440000000000018, -5.250000000000027, -5.180000000000026, -4.010000000000013, -4.250000000000016, -5.010000000000025, -4.590000000000019, -4.090000000000014, -4.560000000000019, -3.790000000000011, -5.230000000000027, -4.080000000000014, -4.500000000000019, -5.080000000000025, -5.020000000000024, -5.050000000000025, -4.120000000000014, -5.300000000000027, -4.470000000000018, -4.510000000000018, -5.000000000000024, -4.790000000000022, -4.280000000000016, -4.3100000000000165, -4.770000000000023, -4.1100000000000145, -3.920000000000013, -4.170000000000015, -4.960000000000024, -4.510000000000019, -3.6400000000000095, -3.7300000000000106, -4.2300000000000155, -5.040000000000025, -4.120000000000014, -5.000000000000024, -4.840000000000023, -4.970000000000024, -3.8200000000000114, -4.290000000000017, -4.340000000000017, -4.810000000000024], "policy_agent_17_reward": [-4.780000000000021, -4.790000000000022, -3.9800000000000133, -4.3800000000000185, -4.340000000000017, -4.750000000000021, -3.8100000000000116, -4.54000000000002, -5.190000000000026, -5.100000000000025, -4.140000000000015, -4.140000000000015, -4.260000000000016, -4.250000000000016, -5.340000000000028, -4.330000000000017, -4.950000000000024, -4.61000000000002, -5.3700000000000285, -4.490000000000019, -4.140000000000015, -4.730000000000022, -4.520000000000019, -4.64000000000002, -4.210000000000016, -3.6300000000000097, -3.9000000000000123, -4.180000000000016, -4.990000000000023, -4.4300000000000175, -5.220000000000027, -3.970000000000013, -5.090000000000026, -4.2700000000000164, -4.650000000000021, -5.040000000000025, -4.200000000000015, -3.760000000000011, -5.230000000000027, -4.660000000000021, -3.760000000000011, -5.310000000000027, -4.320000000000017, -4.960000000000024, -3.670000000000011, -4.970000000000025, -4.7700000000000236, -4.410000000000018, -5.340000000000028, -4.060000000000014, -4.560000000000019, -4.660000000000021, -4.100000000000015, -4.560000000000019, -4.220000000000017, -5.060000000000025, -4.57000000000002, -4.770000000000022, -3.950000000000013, -4.400000000000018, -3.400000000000006, -4.3100000000000165, -4.420000000000017], "policy_agent_18_reward": [-5.280000000000027, -4.150000000000015, -3.9000000000000123, -5.290000000000028, -4.800000000000022, -5.180000000000027, -3.5300000000000082, -3.780000000000011, -4.720000000000021, -4.720000000000021, -4.460000000000019, -5.290000000000028, -4.770000000000022, -4.990000000000024, -4.60000000000002, -4.550000000000019, -4.430000000000018, -3.5700000000000087, -4.990000000000024, -3.5700000000000074, -4.290000000000017, -5.150000000000026, -4.7400000000000215, -4.400000000000018, -4.170000000000015, -4.330000000000017, -3.940000000000013, -4.760000000000022, -4.090000000000014, -5.120000000000027, -3.9000000000000123, -5.100000000000025, -5.220000000000026, -4.480000000000018, -4.0700000000000145, -4.3100000000000165, -3.880000000000013, -4.120000000000014, -4.910000000000023, -4.240000000000016, -3.1700000000000044, -3.9300000000000126, -3.5600000000000085, -3.9000000000000123, -3.780000000000011, -4.660000000000021, -3.68000000000001, -3.850000000000012, -4.170000000000015, -5.060000000000025, -5.380000000000028, -4.060000000000014, -5.060000000000025, -5.360000000000028, -4.330000000000017, -3.920000000000013, -4.090000000000014, -5.320000000000028, -5.030000000000026, -4.840000000000023, -4.490000000000019, -4.870000000000023, -4.330000000000017], "policy_agent_19_reward": [-3.9800000000000133, -4.020000000000014, -5.090000000000026, -3.75000000000001, -4.370000000000019, -4.60000000000002, -5.110000000000025, -4.9400000000000235, -4.560000000000019, -4.220000000000016, -4.0700000000000145, -5.040000000000025, -3.8100000000000116, -4.5400000000000205, -3.9600000000000133, -4.130000000000014, -3.8300000000000116, -4.530000000000019, -4.170000000000015, -4.760000000000022, -4.410000000000018, -4.030000000000014, -5.290000000000028, -4.450000000000019, -5.240000000000028, -4.030000000000014, -4.63000000000002, -3.7000000000000104, -4.180000000000016, -5.340000000000028, -4.200000000000015, -4.870000000000023, -5.270000000000027, -4.960000000000024, -4.610000000000021, -4.710000000000022, -3.790000000000011, -3.6300000000000106, -3.9800000000000133, -3.950000000000013, -4.64000000000002, -4.64000000000002, -3.820000000000012, -3.8000000000000114, -4.180000000000016, -4.340000000000017, -4.370000000000019, -4.480000000000018, -4.040000000000013, -3.970000000000013, -5.020000000000024, -5.210000000000027, -4.650000000000021, -4.510000000000019, -4.430000000000018, -3.4100000000000072, -5.030000000000024, -4.290000000000017, -4.350000000000017, -4.410000000000018, -3.420000000000007, -4.760000000000022, -4.460000000000019], "policy_agent_20_reward": [-3.7300000000000106, -4.9000000000000234, -4.2300000000000155, -4.240000000000016, -3.6700000000000097, -4.660000000000021, -4.500000000000019, -4.350000000000017, -4.460000000000019, -4.570000000000021, -4.550000000000019, -5.090000000000026, -4.360000000000017, -4.100000000000014, -4.100000000000015, -4.020000000000013, -4.090000000000014, -4.690000000000021, -5.050000000000025, -5.330000000000028, -4.810000000000022, -5.260000000000027, -4.920000000000023, -4.2700000000000164, -3.9900000000000135, -4.170000000000015, -5.300000000000027, -4.510000000000019, -3.870000000000012, -4.050000000000014, -3.8900000000000134, -4.800000000000022, -4.2700000000000164, -5.220000000000027, -5.320000000000028, -4.380000000000018, -4.360000000000017, -4.1100000000000145, -3.770000000000011, -3.320000000000006, -4.780000000000022, -5.350000000000028, -4.260000000000016, -4.910000000000023, -4.1900000000000155, -4.090000000000014, -4.090000000000014, -4.690000000000022, -5.120000000000026, -4.930000000000024, -4.500000000000019, -4.260000000000016, -4.850000000000023, -5.290000000000028, -4.050000000000014, -4.6200000000000205, -3.590000000000009, -4.460000000000019, -4.100000000000015, -4.910000000000023, -4.530000000000018, -4.590000000000019, -4.220000000000016]}, "sampler_perf": {"mean_raw_obs_processing_ms": 1.9666723905120171, "mean_inference_ms": 9.246059106723264, "mean_action_processing_ms": 0.7266736906612771, "mean_env_wait_ms": 2.8071236679146367, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"ObsPreprocessorConnector_ms": 0.002029963902064732, "StateBufferConnector_ms": 0.00147659131639038, "ViewRequirementAgentConnector_ms": 0.03326155431146254}, "num_episodes": 9, "episode_return_max": -42.70999999999981, "episode_return_min": -75.40000000000191, "episode_return_mean": -57.36015873016024, "episodes_this_iter": 9}, "num_healthy_workers": 3, "actor_manager_num_outstanding_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 672000, "num_agent_steps_trained": 672000, "num_env_steps_sampled": 32000, "num_env_steps_trained": 32000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 129.58135080744353, "num_env_steps_trained_throughput_per_sec": 129.58135080744353, "timesteps_total": 32000, "num_env_steps_sampled_lifetime": 32000, "num_agent_steps_sampled_lifetime": 672000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 672000, "timers": {"training_iteration_time_ms": 30251.594, "restore_workers_time_ms": 0.015, "training_step_time_ms": 30251.548, "sample_time_ms": 20220.918, "learn_time_ms": 10008.169, "learn_throughput": 399.673, "synch_weights_time_ms": 19.839}, "counters": {"num_env_steps_sampled": 32000, "num_env_steps_trained": 32000, "num_agent_steps_sampled": 672000, "num_agent_steps_trained": 672000}, "done": false, "training_iteration": 8, "trial_id": "cfc88_00000", "date": "2025-10-21_11-21-59", "timestamp": 1761038519, "time_this_iter_s": 30.880501985549927, "time_total_s": 242.11435341835022, "pid": 3279004, "hostname": "xuezhi-Precision-3660", "node_ip": "130.238.16.41", "config": {"exploration_config": {"type": "StochasticSampling"}, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "placement_strategy": "PACK", "num_gpus": 1, "_fake_gpus": false, "num_cpus_for_main_process": 1, "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "torch_ddp_kwargs": {}, "torch_skip_nan_gradients": false, "env": "<class 'train_utils.WarehouseMultiAgentEnv'>", "env_config": {"env_id": "tarware-extralarge-14agvs-7pickers-partialobs-chg-v1"}, "observation_space": null, "action_space": null, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "_is_atari": null, "disable_env_checking": false, "render_env": true, "action_mask_key": "action_mask", "env_runner_cls": null, "num_env_runners": 3, "create_local_env_runner": true, "num_envs_per_env_runner": 1, "gym_env_vectorize_mode": "SYNC", "num_cpus_per_env_runner": 1, "num_gpus_per_env_runner": 0, "custom_resources_per_env_runner": {}, "validate_env_runners_after_construction": true, "episodes_to_numpy": true, "max_requests_in_flight_per_env_runner": 1, "sample_timeout_s": 60.0, "_env_to_module_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "_module_to_env_connector": null, "add_default_connectors_to_module_to_env_pipeline": true, "merge_env_runner_states": "training_only", "broadcast_env_runner_states": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "compress_observations": false, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "sampler_perf_stats_ema_coef": null, "_is_online": true, "num_learners": 0, "num_gpus_per_learner": 1, "num_cpus_per_learner": "auto", "num_aggregator_actors_per_learner": 0, "max_requests_in_flight_per_aggregator_actor": 3, "local_gpu_idx": 0, "max_requests_in_flight_per_learner": 3, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "_train_batch_size_per_learner": null, "train_batch_size": 4000, "num_epochs": 10, "minibatch_size": 256, "shuffle_batch_per_epoch": true, "model": {"fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "log_std_clip_param": 20.0, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1, "_disable_preprocessor_api": false, "_disable_action_flattening": false}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "learner_config_dict": {}, "optimizer": {}, "_learner_class": null, "callbacks_on_algorithm_init": null, "callbacks_on_env_runners_recreated": null, "callbacks_on_offline_eval_runners_recreated": null, "callbacks_on_checkpoint_loaded": null, "callbacks_on_environment_created": null, "callbacks_on_episode_created": null, "callbacks_on_episode_start": null, "callbacks_on_episode_step": null, "callbacks_on_episode_end": null, "callbacks_on_evaluate_start": null, "callbacks_on_evaluate_end": null, "callbacks_on_evaluate_offline_start": null, "callbacks_on_evaluate_offline_end": null, "callbacks_on_sample_end": null, "callbacks_on_train_result": null, "explore": true, "enable_rl_module_and_learner": false, "enable_env_runner_and_connector_v2": false, "_prior_exploration_config": null, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function train.<locals>.<lambda> at 0x75979fbe6290>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "offline_data_class": null, "input_read_method": "read_parquet", "input_read_method_kwargs": {}, "input_read_schema": {}, "input_read_episodes": false, "input_read_sample_batches": false, "input_read_batch_size": null, "input_filesystem": null, "input_filesystem_kwargs": {}, "input_compress_columns": ["obs", "new_obs"], "input_spaces_jsonable": true, "materialize_data": false, "materialize_mapped_data": true, "map_batches_kwargs": {}, "iter_batches_kwargs": {}, "ignore_final_observation": false, "prelearner_class": null, "prelearner_buffer_class": null, "prelearner_buffer_kwargs": {}, "prelearner_module_synch_period": 10, "dataset_num_iters_per_learner": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "output_max_rows_per_file": null, "output_write_remaining_data": false, "output_write_method": "write_parquet", "output_write_method_kwargs": {}, "output_filesystem": null, "output_filesystem_kwargs": {}, "output_write_episodes": true, "offline_sampling": false, "evaluation_interval": 10, "evaluation_duration": 5, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 120.0, "evaluation_auto_duration_min_env_steps_per_sample": 100, "evaluation_auto_duration_max_env_steps_per_sample": 2000, "evaluation_parallel_to_training": false, "evaluation_force_reset_envs_before_iteration": true, "evaluation_config": {"explore": false}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_env_runners": 0, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 10.0, "offline_evaluation_interval": null, "num_offline_eval_runners": 0, "offline_evaluation_type": null, "offline_eval_runner_class": null, "offline_loss_for_module_fn": null, "offline_evaluation_duration": 1, "offline_evaluation_parallel_to_training": false, "offline_evaluation_timeout_s": 120.0, "num_cpus_per_offline_eval_runner": 1, "num_gpus_per_offline_eval_runner": 0, "custom_resources_per_offline_eval_runner": {}, "restart_failed_offline_eval_runners": true, "ignore_offline_eval_runner_failures": false, "max_num_offline_eval_runner_restarts": 1000, "offline_eval_runner_restore_timeout_s": 1800.0, "max_requests_in_flight_per_offline_eval_runner": 1, "validate_offline_eval_runners_after_construction": true, "offline_eval_runner_health_probe_timeout_s": 30.0, "offline_eval_rl_module_inference_only": false, "broadcast_offline_eval_runner_states": false, "offline_eval_batch_size_per_runner": 256, "dataset_num_iters_per_eval_runner": 1, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "log_gradients": false, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "restart_failed_env_runners": true, "ignore_env_runner_failures": false, "max_num_env_runner_restarts": 1000, "delay_between_env_runner_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_env_runner_failures_tolerance": 100, "env_runner_health_probe_timeout_s": 30.0, "env_runner_restore_timeout_s": 1800.0, "_model_config": {}, "_rl_module_spec": null, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "_validate_config": true, "_use_msgpack_checkpoints": false, "_torch_grad_scaler_class": null, "_torch_lr_scheduler_classes": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "_dont_auto_sync_env_runner_states": false, "env_task_fn": -1, "enable_connectors": -1, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "enable_async_evaluation": -1, "custom_async_evaluation_function": -1, "_enable_rl_module_api": -1, "auto_wrap_old_gym_envs": -1, "always_attach_evaluation_results": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "clip_param": 0.2, "vf_clip_param": 10.0, "entropy_coeff_schedule": null, "lr_schedule": null, "sgd_minibatch_size": -1, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 0.95, "input": "sampler", "policies": {"agent_0": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_1": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_2": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_3": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_4": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_5": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_6": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_7": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_8": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_9": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_10": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_11": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_12": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_13": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_14": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_15": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_16": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_17": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_18": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_19": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_20": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}]}, "callbacks": "<class 'ray.rllib.callbacks.callbacks.RLlibCallback'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch"}, "time_since_restore": 242.11435341835022, "iterations_since_restore": 8, "perf": {"cpu_util_percent": 8.878571428571428, "ram_util_percent": 27.202380952380953, "gpu_util_percent0": 0.21976190476190477, "vram_util_percent0": 0.09623258194337822}}
{"custom_metrics": {}, "episode_media": {}, "info": {"learner": {"agent_4": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.7596640050411225, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03931791623399476, "policy_loss": -0.05781016726687085, "vf_loss": 0.015595271278289146, "vf_explained_var": 0.8346296578645707, "kl": 0.00965659799030511, "entropy": 5.809634938836098, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_19": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.262499691545964, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.017604488068900536, "policy_loss": -0.05717677850916516, "vf_loss": 0.03596381391980685, "vf_explained_var": 0.8972849544137717, "kl": 0.012028248073833703, "entropy": 5.3494768053293225, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.281002387404442, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.04254018992432975, "policy_loss": -0.059387051331577824, "vf_loss": 0.014032613794552163, "vf_explained_var": 0.8422004286199808, "kl": 0.009380828065341098, "entropy": 5.6849428594112394, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_16": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.2157009780406955, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.018587250921700615, "policy_loss": -0.05344710660574492, "vf_loss": 0.03165154035086744, "vf_explained_var": 0.9257023960351944, "kl": 0.010694382882788464, "entropy": 5.009989598393441, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_9": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.6236964508891107, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.0454752660356462, "policy_loss": -0.06170114770429791, "vf_loss": 0.013488411967409774, "vf_explained_var": 0.9212047666311264, "kl": 0.00912490053714976, "entropy": 5.8403242319822315, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_5": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.6387097626924514, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.04114356617355952, "policy_loss": -0.05784360751567874, "vf_loss": 0.013825340114999562, "vf_explained_var": 0.8805976942181587, "kl": 0.009582340955911362, "entropy": 5.754261630773544, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_7": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.569281074404716, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03431037073751213, "policy_loss": -0.05340950512618292, "vf_loss": 0.016453567499411292, "vf_explained_var": 0.9047003880143165, "kl": 0.008818560551799238, "entropy": 5.803174066543579, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_14": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.068068359792233, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.020161841440130956, "policy_loss": -0.05416593865375034, "vf_loss": 0.030551570130046456, "vf_explained_var": 0.9075256776064634, "kl": 0.011508425522692748, "entropy": 5.40127058327198, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_18": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.52001324892044, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.013050186775217298, "policy_loss": -0.053943454893305896, "vf_loss": 0.037451037898426874, "vf_explained_var": 0.9484574712812901, "kl": 0.011474096617206659, "entropy": 5.456767213344574, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_12": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.504026027023792, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03622928209515521, "policy_loss": -0.05477572621020954, "vf_loss": 0.01587030768569093, "vf_explained_var": 0.9292636506259442, "kl": 0.00892045184338417, "entropy": 5.8861082881689075, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_11": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.369680120050907, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.033829337582938025, "policy_loss": -0.055776616536604706, "vf_loss": 0.019053627640823835, "vf_explained_var": 0.8634607840329409, "kl": 0.009645506379561242, "entropy": 5.767223709821701, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_13": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.5259917333722113, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.042806497107085305, "policy_loss": -0.05717265967978165, "vf_loss": 0.011740622232900932, "vf_explained_var": 0.7826364617794752, "kl": 0.008751800627880602, "entropy": 5.832958161830902, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_0": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.6928601443767546, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03991610531084007, "policy_loss": -0.05699441050528549, "vf_loss": 0.01443655290058814, "vf_explained_var": 0.870412727445364, "kl": 0.008805843590120844, "entropy": 5.830640435218811, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_20": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.133200287818909, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.017888236079306806, "policy_loss": -0.05171495240065269, "vf_loss": 0.0306697771709878, "vf_explained_var": 0.9172077052295208, "kl": 0.010523130490427968, "entropy": 5.45480055809021, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.5413038089871405, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.041993948178424036, "policy_loss": -0.05856493927858537, "vf_loss": 0.013824590219883248, "vf_explained_var": 0.9087004601955414, "kl": 0.009154666050312699, "entropy": 5.775721019506454, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_10": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.559478782117367, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.038513172997045333, "policy_loss": -0.0575572047237074, "vf_loss": 0.016227496913052163, "vf_explained_var": 0.9110210780054331, "kl": 0.009388444746384183, "entropy": 5.852959379553795, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_3": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.843729092180729, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.04205703207262559, "policy_loss": -0.05917877672472969, "vf_loss": 0.014210066138184629, "vf_explained_var": 0.891862016171217, "kl": 0.009705593099777676, "entropy": 5.767122390866279, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_15": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.57664357572794, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.014660818468837534, "policy_loss": -0.053803438268369065, "vf_loss": 0.0356072056107223, "vf_explained_var": 0.9114962596446275, "kl": 0.011784710763511355, "entropy": 5.444631555676461, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_17": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 6.465611965954304, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.023309387027984484, "policy_loss": -0.05344181514810771, "vf_loss": 0.027008022245718168, "vf_explained_var": 0.8752101972699166, "kl": 0.010414687313264637, "entropy": 5.205356472730637, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_6": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.165240585803986, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.03611991463694721, "policy_loss": -0.055225759657332674, "vf_loss": 0.016294301283778623, "vf_explained_var": 0.9092819903045892, "kl": 0.009371812654085817, "entropy": 5.775402614474297, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}, "agent_8": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.935825291275978, "cur_kl_coeff": 0.3, "cur_lr": 5.000000000000001e-05, "total_loss": -0.034681288078718356, "policy_loss": -0.05187696903885808, "vf_loss": 0.014729894557967782, "vf_explained_var": 0.9060894247144461, "kl": 0.008219288599697894, "entropy": 5.838049608469009, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 250.0, "num_grad_updates_lifetime": 1360.5, "diff_num_grad_updates_vs_sampler_policy": 79.5}}, "num_env_steps_sampled": 36000, "num_env_steps_trained": 36000, "num_agent_steps_sampled": 756000, "num_agent_steps_trained": 756000}, "env_runners": {"episode_reward_max": -42.70999999999981, "episode_reward_min": -75.40000000000191, "episode_reward_mean": -57.26157142857291, "episode_len_mean": 500.0, "episode_media": {}, "episodes_timesteps_total": 35000, "policy_reward_min": {"agent_0": -5.3700000000000285, "agent_1": -5.380000000000028, "agent_2": -5.330000000000028, "agent_3": -5.350000000000028, "agent_4": -5.340000000000028, "agent_5": -5.350000000000028, "agent_6": -5.360000000000028, "agent_7": -5.340000000000028, "agent_8": -5.300000000000027, "agent_9": -5.330000000000028, "agent_10": -5.350000000000028, "agent_11": -5.3700000000000285, "agent_12": -5.380000000000028, "agent_13": -5.330000000000028, "agent_14": -5.320000000000028, "agent_15": -5.300000000000027, "agent_16": -5.330000000000028, "agent_17": -5.3700000000000285, "agent_18": -5.380000000000028, "agent_19": -5.340000000000028, "agent_20": -5.350000000000028}, "policy_reward_max": {"agent_0": -0.5000000000000003, "agent_1": -0.5000000000000003, "agent_2": -0.5000000000000003, "agent_3": -0.5000000000000003, "agent_4": -0.5000000000000003, "agent_5": -0.5000000000000003, "agent_6": -0.5000000000000003, "agent_7": -0.1000000000000006, "agent_8": -0.5000000000000003, "agent_9": -0.5000000000000003, "agent_10": -0.5000000000000003, "agent_11": -0.5000000000000003, "agent_12": -0.5000000000000003, "agent_13": -0.5000000000000003, "agent_14": -3.590000000000009, "agent_15": -3.7300000000000106, "agent_16": -3.6400000000000095, "agent_17": -3.400000000000006, "agent_18": -3.1700000000000044, "agent_19": -3.4100000000000072, "agent_20": -3.320000000000006}, "policy_reward_mean": {"agent_0": -1.9835714285714363, "agent_1": -1.5314285714285771, "agent_2": -2.3181428571428655, "agent_3": -2.437857142857153, "agent_4": -1.4322857142857195, "agent_5": -2.1374285714285794, "agent_6": -1.9814285714285793, "agent_7": -1.7502857142857204, "agent_8": -1.4237142857142902, "agent_9": -1.362142857142861, "agent_10": -1.948714285714293, "agent_11": -1.9027142857142934, "agent_12": -1.5547142857142908, "agent_13": -2.0521428571428655, "agent_14": -4.531142857142878, "agent_15": -4.490571428571448, "agent_16": -4.598428571428593, "agent_17": -4.477428571428591, "agent_18": -4.456571428571447, "agent_19": -4.423285714285733, "agent_20": -4.467571428571445}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-53.290000000000305, -55.12000000000127, -61.81000000000211, -47.65000000000047, -57.68000000000135, -56.200000000001694, -52.780000000000356, -57.85000000000114, -54.040000000000376, -64.27000000000298, -49.57000000000024, -73.5400000000017, -62.680000000002885, -60.280000000003, -52.53000000000038, -58.05000000000256, -43.9300000000001, -59.7700000000019, -61.3400000000021, -52.9100000000017, -56.880000000001324, -62.430000000003005, -67.53000000000264, -58.4000000000019, -63.640000000003184, -48.380000000000244, -59.750000000001855, -54.76000000000095, -48.56000000000037, -72.73000000000226, -61.48000000000291, -54.10000000000039, -75.40000000000191, -57.9400000000012, -67.91000000000251, -57.94000000000153, -47.429999999999914, -52.13000000000102, -55.420000000001096, -57.8500000000018, -50.93000000000088, -64.32000000000227, -55.500000000001464, -42.70999999999981, -51.63000000000134, -56.65000000000114, -65.42000000000365, -51.810000000000436, -65.04000000000308, -62.730000000002164, -45.47999999999959, -51.710000000000335, -53.17000000000035, -59.50000000000128, -59.81000000000211, -61.23000000000282, -67.91000000000287, -44.25999999999966, -50.24000000000036, -55.640000000001045, -60.08000000000257, -61.84000000000216, -60.130000000002866, -48.87000000000027, -60.04000000000195, -70.80000000000253, -55.11000000000172, -50.0799999999998, -48.94000000000056, -60.78000000000201], "episode_lengths": [500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500], "policy_agent_0_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.070000000000025, -5.240000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.110000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -2.5999999999999983, -5.210000000000027, -0.5000000000000003, -5.070000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.280000000000027, -5.210000000000027, -5.020000000000024, -0.5000000000000003, -5.090000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.8200000000000007, -3.770000000000011, -5.000000000000024, -5.120000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.140000000000004, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -5.3700000000000285, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.230000000000027, -0.5000000000000003, -5.240000000000027, -5.270000000000027, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -5.200000000000027, -0.5000000000000003, -5.070000000000025, -0.5000000000000003, -0.5000000000000003], "policy_agent_1_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.5500000000000087, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -5.360000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.920000000000023, -0.5000000000000003, -5.360000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.780000000000022, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.480000000000018, -0.5000000000000003, -0.5000000000000003, -5.380000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.590000000000019, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5100000000000003, -0.5000000000000003, -5.320000000000028, -0.5000000000000003, -5.130000000000026, -0.5000000000000003, -5.010000000000025, -0.5000000000000003, -5.320000000000028, -0.5000000000000003, -5.320000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_2_reward": [-5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -3.500000000000008, -4.840000000000023, -2.129999999999993, -2.149999999999986, -5.130000000000026, -2.429999999999996, -0.5000000000000003, -0.5000000000000003, -4.67000000000002, -0.5000000000000003, -0.5000000000000003, -4.830000000000022, -5.060000000000025, -0.5000000000000003, -4.520000000000019, -0.5000000000000003, -4.780000000000022, -5.070000000000025, -0.5000000000000003, -5.170000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.330000000000028, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.160000000000026, -0.5000000000000003, -2.5499999999999976, -0.5000000000000003, -5.010000000000025, -0.5000000000000003, -2.579999999999998, -0.5000000000000003, -1.7599999999999938, -0.5900000000000004, -4.0700000000000145, -5.040000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.050000000000025, -5.220000000000026, -4.990000000000024, -5.260000000000027, -0.5000000000000003, -1.4699999999999964, -5.270000000000027, -5.320000000000028, -4.6200000000000205, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.130000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_3_reward": [-0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.050000000000025, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -5.160000000000026, -0.5000000000000003, -5.330000000000028, -5.240000000000027, -0.7300000000000005, -5.180000000000026, -4.590000000000019, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -5.190000000000026, -0.8900000000000007, -5.350000000000028, -5.320000000000028, -0.5000000000000003, -0.5000000000000003, -3.7500000000000107, -0.5000000000000003, -4.8200000000000225, -5.320000000000028, -4.660000000000021, -5.080000000000025, -0.5000000000000003, -0.5000000000000003, -5.010000000000025, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.330000000000028, -5.060000000000025, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.230000000000027, -5.060000000000025, -4.400000000000018, -0.5000000000000003, -4.200000000000015, -0.5000000000000003, -0.5000000000000003, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -4.920000000000023, -0.9500000000000007, -0.5000000000000003, -5.140000000000025, -3.6300000000000097], "policy_agent_4_reward": [-0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.8100000000000116, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.110000000000025, -4.950000000000024, -5.270000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5000000000000003, -5.330000000000028, -0.5000000000000003, -2.3699999999999886, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.57000000000002, -4.990000000000024, -4.8600000000000225, -0.5000000000000003, -0.5000000000000003, -5.220000000000026], "policy_agent_5_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5000000000000003, -4.870000000000023, -5.350000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.589999999999998, -5.000000000000024, -2.930000000000002, -5.290000000000028, -0.5000000000000003, -5.090000000000026, -3.8100000000000116, -0.5000000000000003, -5.050000000000025, -5.300000000000027, -5.150000000000026, -3.6800000000000175, -5.110000000000025, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -5.000000000000024, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -4.980000000000024, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.310000000000027, -2.039999999999992, -4.790000000000022], "policy_agent_6_reward": [-5.310000000000027, -0.5000000000000003, -0.5000000000000003, -4.430000000000018, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -4.930000000000024, -5.030000000000024, -0.5000000000000003, -2.429999999999996, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.720000000000021, -5.330000000000028, -0.5000000000000003, -3.5700000000000087, -0.5000000000000003, -0.5000000000000003, -2.2799999999999945, -5.310000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.240000000000027, -0.5000000000000003, -0.5000000000000003, -5.350000000000028, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.310000000000027, -5.250000000000027, -5.360000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.030000000000024, -0.5000000000000003, -5.330000000000028, -0.5000000000000003, -0.5000000000000003, -5.270000000000027, -4.380000000000018, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_7_reward": [-0.5000000000000003, -2.349999999999995, -5.220000000000026, -2.2499999999999942, -5.040000000000025, -2.75, -5.300000000000027, -0.5000000000000003, -5.270000000000027, -0.5000000000000003, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.0000000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.8999999999999926, -5.110000000000025, -5.330000000000028, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.890000000000012, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -0.1000000000000006, -4.890000000000023, -5.200000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.160000000000026, -0.5000000000000003, -0.5000000000000003, -5.180000000000026, -4.960000000000024, -5.200000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_8_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.040000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -4.54000000000002, -0.5000000000000003, -3.0900000000000034, -0.5000000000000003, -0.5000000000000003, -5.080000000000025, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -4.61000000000002, -0.5000000000000003, -0.5000000000000003, -3.390000000000007, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.710000000000021, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.260000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.190000000000005, -0.5000000000000003, -5.270000000000027, -2.1699999999999933, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_9_reward": [-0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -3.790000000000011, -2.8200000000000007, -0.5000000000000003, -5.180000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.190000000000026, -0.5000000000000003, -0.5600000000000004, -0.5000000000000003, -0.5000000000000003, -4.140000000000015, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.330000000000028, -3.3800000000000066, -0.5000000000000003, -5.120000000000026, -0.5000000000000003, -4.890000000000023, -0.5000000000000003, -0.5000000000000003, -5.260000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.9199999999999924, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.7199999999999993, -3.050000000000003, -0.5000000000000003, -5.260000000000027, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_10_reward": [-4.690000000000021, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.120000000000026, -4.930000000000024, -5.350000000000028, -0.5000000000000003, -2.119999999999993, -0.5000000000000003, -4.920000000000023, -2.6199999999999983, -0.5000000000000003, -3.4700000000000077, -4.560000000000019, -4.910000000000023, -4.030000000000014, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -0.5000000000000003, -4.910000000000023, -0.9700000000000008, -0.5000000000000003, -5.050000000000025, -0.5000000000000003, -5.310000000000027, -4.850000000000023, -0.5000000000000003, -5.030000000000024, -4.970000000000024, -5.130000000000026, -4.9000000000000234, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.130000000000026, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -1.1199999999999997, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.760000000000022, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003], "policy_agent_11_reward": [-0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.100000000000025, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.3699999999999974, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -5.330000000000028, -0.5000000000000003, -5.340000000000028, -0.5000000000000003, -5.320000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.180000000000026, -5.230000000000027, -5.220000000000026, -4.380000000000018, -5.290000000000028, -4.800000000000022, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.5499999999999956, -0.5000000000000003, -4.170000000000015, -4.9400000000000235, -0.5000000000000003, -0.5000000000000003, -5.200000000000027, -0.5000000000000003, -0.5000000000000003, -5.070000000000025, -5.240000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -4.810000000000022, -5.330000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.3700000000000285], "policy_agent_12_reward": [-0.5000000000000003, -5.240000000000027, -5.090000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.170000000000027, -0.5000000000000003, -0.5000000000000003, -2.139999999999993, -0.5000000000000003, -0.5000000000000003, -4.550000000000019, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -2.3399999999999954, -0.5000000000000003, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -5.380000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.300000000000027, -0.5000000000000003, -0.5000000000000003, -4.870000000000023, -0.5000000000000003, -5.250000000000027, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.270000000000027, -3.970000000000013, -0.5000000000000003, -1.5799999999999954, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.160000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.280000000000027, -0.5000000000000003, -0.5000000000000003, -1.07, -0.5000000000000003, -0.5000000000000003, -5.240000000000027], "policy_agent_13_reward": [-0.5000000000000003, -5.220000000000026, -5.270000000000027, -0.5000000000000003, -0.9600000000000007, -3.2600000000000056, -5.110000000000025, -5.260000000000027, -0.5000000000000003, -5.310000000000027, -0.5000000000000003, -5.280000000000027, -5.220000000000026, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.210000000000027, -4.920000000000023, -0.5000000000000003, -0.5000000000000003, -5.030000000000024, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.210000000000027, -0.5000000000000003, -0.5000000000000003, -1.2699999999999982, -0.5000000000000003, -0.7400000000000005, -0.5000000000000003, -5.290000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.020000000000024, -0.5000000000000003, -5.180000000000026, -0.5000000000000003, -5.330000000000028, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -5.030000000000024, -4.310000000000027, -0.5000000000000003, -5.030000000000024, -0.5000000000000003, -0.5000000000000003, -0.5000000000000003, -1.9799999999999918, -4.920000000000023, -2.1099999999999928, -0.5000000000000003, -4.930000000000024, -0.5000000000000003, -5.150000000000026, -0.5000000000000003, -5.100000000000025, -0.5000000000000003], "policy_agent_14_reward": [-4.680000000000021, -4.600000000000021, -4.260000000000016, -4.000000000000013, -4.690000000000021, -4.530000000000019, -5.240000000000027, -4.8600000000000225, -5.150000000000026, -5.220000000000026, -3.780000000000011, -4.090000000000014, -5.000000000000024, -4.970000000000024, -4.61000000000002, -5.000000000000024, -4.160000000000014, -4.8200000000000225, -4.710000000000021, -4.420000000000018, -4.360000000000017, -3.680000000000009, -3.590000000000009, -4.400000000000017, -4.350000000000017, -4.64000000000002, -4.57000000000002, -4.290000000000017, -3.850000000000012, -3.9100000000000126, -4.180000000000017, -4.980000000000024, -4.200000000000015, -3.950000000000013, -4.870000000000023, -3.66000000000001, -5.130000000000026, -4.090000000000015, -5.220000000000026, -4.2300000000000155, -3.920000000000013, -5.060000000000025, -4.740000000000022, -4.8200000000000225, -4.2300000000000155, -4.090000000000014, -4.480000000000018, -5.100000000000025, -4.800000000000023, -5.080000000000025, -5.130000000000026, -5.060000000000025, -4.380000000000018, -4.870000000000023, -4.1100000000000145, -4.140000000000015, -4.550000000000019, -4.510000000000019, -5.320000000000028, -4.650000000000021, -3.590000000000009, -5.150000000000026, -3.9300000000000126, -4.3900000000000166, -4.020000000000014, -5.020000000000024, -4.780000000000022, -5.010000000000025, -4.800000000000022, -4.510000000000019], "policy_agent_15_reward": [-5.140000000000025, -4.400000000000018, -4.830000000000022, -4.520000000000019, -5.040000000000025, -3.950000000000013, -4.2300000000000155, -4.700000000000021, -3.770000000000011, -3.970000000000013, -4.950000000000024, -4.110000000000015, -4.560000000000019, -4.470000000000018, -4.120000000000014, -4.300000000000017, -3.8200000000000114, -4.240000000000015, -4.520000000000019, -4.480000000000018, -4.700000000000021, -4.030000000000014, -4.260000000000016, -4.490000000000019, -5.110000000000025, -4.130000000000015, -3.970000000000013, -4.710000000000021, -4.000000000000013, -4.960000000000024, -4.0700000000000145, -4.210000000000016, -4.6200000000000205, -5.040000000000025, -4.510000000000019, -4.410000000000018, -5.200000000000027, -5.290000000000028, -4.58000000000002, -3.7300000000000106, -4.730000000000022, -4.2300000000000155, -5.300000000000027, -4.52000000000002, -4.730000000000023, -4.360000000000017, -4.760000000000022, -4.130000000000015, -4.080000000000014, -4.650000000000021, -4.690000000000021, -3.8800000000000123, -5.240000000000027, -4.960000000000024, -3.790000000000012, -3.9800000000000133, -4.55000000000002, -5.170000000000027, -5.190000000000026, -3.9600000000000133, -4.220000000000016, -4.370000000000017, -4.080000000000014, -4.560000000000019, -4.050000000000014, -4.54000000000002, -4.9400000000000235, -5.240000000000027, -4.400000000000016, -4.9000000000000234], "policy_agent_16_reward": [-4.950000000000024, -5.150000000000026, -4.950000000000024, -4.140000000000015, -5.220000000000026, -4.180000000000016, -5.280000000000027, -4.750000000000021, -5.140000000000026, -4.530000000000019, -4.410000000000018, -4.720000000000021, -4.800000000000023, -4.120000000000014, -4.63000000000002, -4.67000000000002, -5.100000000000025, -4.470000000000018, -4.700000000000021, -4.240000000000016, -4.450000000000019, -4.440000000000018, -5.250000000000027, -5.180000000000026, -4.010000000000013, -4.250000000000016, -5.010000000000025, -4.590000000000019, -4.090000000000014, -4.560000000000019, -3.790000000000011, -5.230000000000027, -4.080000000000014, -4.500000000000019, -5.080000000000025, -5.020000000000024, -5.050000000000025, -4.120000000000014, -5.300000000000027, -4.470000000000018, -4.510000000000018, -5.000000000000024, -4.790000000000022, -4.280000000000016, -4.3100000000000165, -4.770000000000023, -4.1100000000000145, -3.920000000000013, -4.170000000000015, -4.960000000000024, -4.510000000000019, -3.6400000000000095, -3.7300000000000106, -4.2300000000000155, -5.040000000000025, -4.120000000000014, -5.000000000000024, -4.840000000000023, -4.970000000000024, -3.8200000000000114, -4.290000000000017, -4.340000000000017, -4.810000000000024, -4.060000000000014, -5.330000000000028, -4.720000000000021, -3.960000000000014, -5.320000000000028, -4.790000000000022, -4.930000000000024], "policy_agent_17_reward": [-4.780000000000021, -4.790000000000022, -3.9800000000000133, -4.3800000000000185, -4.340000000000017, -4.750000000000021, -3.8100000000000116, -4.54000000000002, -5.190000000000026, -5.100000000000025, -4.140000000000015, -4.140000000000015, -4.260000000000016, -4.250000000000016, -5.340000000000028, -4.330000000000017, -4.950000000000024, -4.61000000000002, -5.3700000000000285, -4.490000000000019, -4.140000000000015, -4.730000000000022, -4.520000000000019, -4.64000000000002, -4.210000000000016, -3.6300000000000097, -3.9000000000000123, -4.180000000000016, -4.990000000000023, -4.4300000000000175, -5.220000000000027, -3.970000000000013, -5.090000000000026, -4.2700000000000164, -4.650000000000021, -5.040000000000025, -4.200000000000015, -3.760000000000011, -5.230000000000027, -4.660000000000021, -3.760000000000011, -5.310000000000027, -4.320000000000017, -4.960000000000024, -3.670000000000011, -4.970000000000025, -4.7700000000000236, -4.410000000000018, -5.340000000000028, -4.060000000000014, -4.560000000000019, -4.660000000000021, -4.100000000000015, -4.560000000000019, -4.220000000000017, -5.060000000000025, -4.57000000000002, -4.770000000000022, -3.950000000000013, -4.400000000000018, -3.400000000000006, -4.3100000000000165, -4.420000000000017, -4.730000000000022, -3.8800000000000123, -4.390000000000017, -3.7700000000000102, -4.210000000000016, -4.810000000000022, -4.080000000000014], "policy_agent_18_reward": [-5.280000000000027, -4.150000000000015, -3.9000000000000123, -5.290000000000028, -4.800000000000022, -5.180000000000027, -3.5300000000000082, -3.780000000000011, -4.720000000000021, -4.720000000000021, -4.460000000000019, -5.290000000000028, -4.770000000000022, -4.990000000000024, -4.60000000000002, -4.550000000000019, -4.430000000000018, -3.5700000000000087, -4.990000000000024, -3.5700000000000074, -4.290000000000017, -5.150000000000026, -4.7400000000000215, -4.400000000000018, -4.170000000000015, -4.330000000000017, -3.940000000000013, -4.760000000000022, -4.090000000000014, -5.120000000000027, -3.9000000000000123, -5.100000000000025, -5.220000000000026, -4.480000000000018, -4.0700000000000145, -4.3100000000000165, -3.880000000000013, -4.120000000000014, -4.910000000000023, -4.240000000000016, -3.1700000000000044, -3.9300000000000126, -3.5600000000000085, -3.9000000000000123, -3.780000000000011, -4.660000000000021, -3.68000000000001, -3.850000000000012, -4.170000000000015, -5.060000000000025, -5.380000000000028, -4.060000000000014, -5.060000000000025, -5.360000000000028, -4.330000000000017, -3.920000000000013, -4.090000000000014, -5.320000000000028, -5.030000000000026, -4.840000000000023, -4.490000000000019, -4.870000000000023, -4.330000000000017, -5.080000000000025, -4.020000000000014, -4.300000000000017, -4.57000000000002, -5.280000000000027, -3.870000000000012, -4.210000000000016], "policy_agent_19_reward": [-3.9800000000000133, -4.020000000000014, -5.090000000000026, -3.75000000000001, -4.370000000000019, -4.60000000000002, -5.110000000000025, -4.9400000000000235, -4.560000000000019, -4.220000000000016, -4.0700000000000145, -5.040000000000025, -3.8100000000000116, -4.5400000000000205, -3.9600000000000133, -4.130000000000014, -3.8300000000000116, -4.530000000000019, -4.170000000000015, -4.760000000000022, -4.410000000000018, -4.030000000000014, -5.290000000000028, -4.450000000000019, -5.240000000000028, -4.030000000000014, -4.63000000000002, -3.7000000000000104, -4.180000000000016, -5.340000000000028, -4.200000000000015, -4.870000000000023, -5.270000000000027, -4.960000000000024, -4.610000000000021, -4.710000000000022, -3.790000000000011, -3.6300000000000106, -3.9800000000000133, -3.950000000000013, -4.64000000000002, -4.64000000000002, -3.820000000000012, -3.8000000000000114, -4.180000000000016, -4.340000000000017, -4.370000000000019, -4.480000000000018, -4.040000000000013, -3.970000000000013, -5.020000000000024, -5.210000000000027, -4.650000000000021, -4.510000000000019, -4.430000000000018, -3.4100000000000072, -5.030000000000024, -4.290000000000017, -4.350000000000017, -4.410000000000018, -3.420000000000007, -4.760000000000022, -4.460000000000019, -5.080000000000025, -4.770000000000023, -4.570000000000021, -5.020000000000025, -3.9000000000000123, -4.420000000000018, -4.890000000000023], "policy_agent_20_reward": [-3.7300000000000106, -4.9000000000000234, -4.2300000000000155, -4.240000000000016, -3.6700000000000097, -4.660000000000021, -4.500000000000019, -4.350000000000017, -4.460000000000019, -4.570000000000021, -4.550000000000019, -5.090000000000026, -4.360000000000017, -4.100000000000014, -4.100000000000015, -4.020000000000013, -4.090000000000014, -4.690000000000021, -5.050000000000025, -5.330000000000028, -4.810000000000022, -5.260000000000027, -4.920000000000023, -4.2700000000000164, -3.9900000000000135, -4.170000000000015, -5.300000000000027, -4.510000000000019, -3.870000000000012, -4.050000000000014, -3.8900000000000134, -4.800000000000022, -4.2700000000000164, -5.220000000000027, -5.320000000000028, -4.380000000000018, -4.360000000000017, -4.1100000000000145, -3.770000000000011, -3.320000000000006, -4.780000000000022, -5.350000000000028, -4.260000000000016, -4.910000000000023, -4.1900000000000155, -4.090000000000014, -4.090000000000014, -4.690000000000022, -5.120000000000026, -4.930000000000024, -4.500000000000019, -4.260000000000016, -4.850000000000023, -5.290000000000028, -4.050000000000014, -4.6200000000000205, -3.590000000000009, -4.460000000000019, -4.100000000000015, -4.910000000000023, -4.530000000000018, -4.590000000000019, -4.220000000000016, -3.950000000000012, -4.6200000000000205, -4.61000000000002, -4.57000000000002, -4.7400000000000215, -4.0700000000000145, -4.510000000000019]}, "sampler_perf": {"mean_raw_obs_processing_ms": 1.9697847023802577, "mean_inference_ms": 9.255205851388503, "mean_action_processing_ms": 0.7266842811105234, "mean_env_wait_ms": 2.809802904413022, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"ObsPreprocessorConnector_ms": 0.0019761818606837266, "StateBufferConnector_ms": 0.001476151602608817, "ViewRequirementAgentConnector_ms": 0.03333135527007434}, "num_episodes": 7, "episode_return_max": -42.70999999999981, "episode_return_min": -75.40000000000191, "episode_return_mean": -57.26157142857291, "episodes_this_iter": 7}, "num_healthy_workers": 3, "actor_manager_num_outstanding_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 756000, "num_agent_steps_trained": 756000, "num_env_steps_sampled": 36000, "num_env_steps_trained": 36000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 131.1736103579235, "num_env_steps_trained_throughput_per_sec": 131.1736103579235, "timesteps_total": 36000, "num_env_steps_sampled_lifetime": 36000, "num_agent_steps_sampled_lifetime": 756000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 756000, "timers": {"training_iteration_time_ms": 30278.522, "restore_workers_time_ms": 0.015, "training_step_time_ms": 30278.476, "sample_time_ms": 20252.197, "learn_time_ms": 10003.994, "learn_throughput": 399.84, "synch_weights_time_ms": 19.468}, "counters": {"num_env_steps_sampled": 36000, "num_env_steps_trained": 36000, "num_agent_steps_sampled": 756000, "num_agent_steps_trained": 756000}, "done": false, "training_iteration": 9, "trial_id": "cfc88_00000", "date": "2025-10-21_11-22-29", "timestamp": 1761038549, "time_this_iter_s": 30.504745960235596, "time_total_s": 272.6190993785858, "pid": 3279004, "hostname": "xuezhi-Precision-3660", "node_ip": "130.238.16.41", "config": {"exploration_config": {"type": "StochasticSampling"}, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "placement_strategy": "PACK", "num_gpus": 1, "_fake_gpus": false, "num_cpus_for_main_process": 1, "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "torch_ddp_kwargs": {}, "torch_skip_nan_gradients": false, "env": "<class 'train_utils.WarehouseMultiAgentEnv'>", "env_config": {"env_id": "tarware-extralarge-14agvs-7pickers-partialobs-chg-v1"}, "observation_space": null, "action_space": null, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "_is_atari": null, "disable_env_checking": false, "render_env": true, "action_mask_key": "action_mask", "env_runner_cls": null, "num_env_runners": 3, "create_local_env_runner": true, "num_envs_per_env_runner": 1, "gym_env_vectorize_mode": "SYNC", "num_cpus_per_env_runner": 1, "num_gpus_per_env_runner": 0, "custom_resources_per_env_runner": {}, "validate_env_runners_after_construction": true, "episodes_to_numpy": true, "max_requests_in_flight_per_env_runner": 1, "sample_timeout_s": 60.0, "_env_to_module_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "_module_to_env_connector": null, "add_default_connectors_to_module_to_env_pipeline": true, "merge_env_runner_states": "training_only", "broadcast_env_runner_states": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "compress_observations": false, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "sampler_perf_stats_ema_coef": null, "_is_online": true, "num_learners": 0, "num_gpus_per_learner": 1, "num_cpus_per_learner": "auto", "num_aggregator_actors_per_learner": 0, "max_requests_in_flight_per_aggregator_actor": 3, "local_gpu_idx": 0, "max_requests_in_flight_per_learner": 3, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "_train_batch_size_per_learner": null, "train_batch_size": 4000, "num_epochs": 10, "minibatch_size": 256, "shuffle_batch_per_epoch": true, "model": {"fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "log_std_clip_param": 20.0, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": null, "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1, "_disable_preprocessor_api": false, "_disable_action_flattening": false}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "learner_config_dict": {}, "optimizer": {}, "_learner_class": null, "callbacks_on_algorithm_init": null, "callbacks_on_env_runners_recreated": null, "callbacks_on_offline_eval_runners_recreated": null, "callbacks_on_checkpoint_loaded": null, "callbacks_on_environment_created": null, "callbacks_on_episode_created": null, "callbacks_on_episode_start": null, "callbacks_on_episode_step": null, "callbacks_on_episode_end": null, "callbacks_on_evaluate_start": null, "callbacks_on_evaluate_end": null, "callbacks_on_evaluate_offline_start": null, "callbacks_on_evaluate_offline_end": null, "callbacks_on_sample_end": null, "callbacks_on_train_result": null, "explore": true, "enable_rl_module_and_learner": false, "enable_env_runner_and_connector_v2": false, "_prior_exploration_config": null, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function train.<locals>.<lambda> at 0x75979fbe6200>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "offline_data_class": null, "input_read_method": "read_parquet", "input_read_method_kwargs": {}, "input_read_schema": {}, "input_read_episodes": false, "input_read_sample_batches": false, "input_read_batch_size": null, "input_filesystem": null, "input_filesystem_kwargs": {}, "input_compress_columns": ["obs", "new_obs"], "input_spaces_jsonable": true, "materialize_data": false, "materialize_mapped_data": true, "map_batches_kwargs": {}, "iter_batches_kwargs": {}, "ignore_final_observation": false, "prelearner_class": null, "prelearner_buffer_class": null, "prelearner_buffer_kwargs": {}, "prelearner_module_synch_period": 10, "dataset_num_iters_per_learner": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "output_max_rows_per_file": null, "output_write_remaining_data": false, "output_write_method": "write_parquet", "output_write_method_kwargs": {}, "output_filesystem": null, "output_filesystem_kwargs": {}, "output_write_episodes": true, "offline_sampling": false, "evaluation_interval": 10, "evaluation_duration": 5, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 120.0, "evaluation_auto_duration_min_env_steps_per_sample": 100, "evaluation_auto_duration_max_env_steps_per_sample": 2000, "evaluation_parallel_to_training": false, "evaluation_force_reset_envs_before_iteration": true, "evaluation_config": {"explore": false}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_env_runners": 0, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 10.0, "offline_evaluation_interval": null, "num_offline_eval_runners": 0, "offline_evaluation_type": null, "offline_eval_runner_class": null, "offline_loss_for_module_fn": null, "offline_evaluation_duration": 1, "offline_evaluation_parallel_to_training": false, "offline_evaluation_timeout_s": 120.0, "num_cpus_per_offline_eval_runner": 1, "num_gpus_per_offline_eval_runner": 0, "custom_resources_per_offline_eval_runner": {}, "restart_failed_offline_eval_runners": true, "ignore_offline_eval_runner_failures": false, "max_num_offline_eval_runner_restarts": 1000, "offline_eval_runner_restore_timeout_s": 1800.0, "max_requests_in_flight_per_offline_eval_runner": 1, "validate_offline_eval_runners_after_construction": true, "offline_eval_runner_health_probe_timeout_s": 30.0, "offline_eval_rl_module_inference_only": false, "broadcast_offline_eval_runner_states": false, "offline_eval_batch_size_per_runner": 256, "dataset_num_iters_per_eval_runner": 1, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "log_gradients": false, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "restart_failed_env_runners": true, "ignore_env_runner_failures": false, "max_num_env_runner_restarts": 1000, "delay_between_env_runner_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_env_runner_failures_tolerance": 100, "env_runner_health_probe_timeout_s": 30.0, "env_runner_restore_timeout_s": 1800.0, "_model_config": {}, "_rl_module_spec": null, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "_validate_config": true, "_use_msgpack_checkpoints": false, "_torch_grad_scaler_class": null, "_torch_lr_scheduler_classes": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "_dont_auto_sync_env_runner_states": false, "env_task_fn": -1, "enable_connectors": -1, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "enable_async_evaluation": -1, "custom_async_evaluation_function": -1, "_enable_rl_module_api": -1, "auto_wrap_old_gym_envs": -1, "always_attach_evaluation_results": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "clip_param": 0.2, "vf_clip_param": 10.0, "entropy_coeff_schedule": null, "lr_schedule": null, "sgd_minibatch_size": -1, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 0.95, "input": "sampler", "policies": {"agent_0": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_1": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_2": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_3": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_4": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_5": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_6": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_7": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_8": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_9": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_10": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_11": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_12": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_13": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_14": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_15": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_16": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_17": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_18": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_19": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}], "agent_20": [null, null, null, {"model": {"custom_model": "warehouse_marl_model"}}]}, "callbacks": "<class 'ray.rllib.callbacks.callbacks.RLlibCallback'>", "create_env_on_driver": false, "custom_eval_function": null, "framework": "torch"}, "time_since_restore": 272.6190993785858, "iterations_since_restore": 9, "perf": {"cpu_util_percent": 8.811904761904762, "ram_util_percent": 27.116666666666667, "gpu_util_percent0": 0.20809523809523814, "vram_util_percent0": 0.09617558796845557}}
